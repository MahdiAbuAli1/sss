# 1_knowledge_pipeline/main_builder.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆØ§Ù„Ø¢Ù„ÙŠØ©)

import os
import argparse
from dotenv import load_dotenv
from typing import List
from langchain_core.documents import Document

load_dotenv()
from loaders import load_documents
from cleaners import clean_documents
from splitters import split_documents
from vector_store_manager import add_to_vector_store

EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
if not EMBEDDING_MODEL_NAME:
    print("[!] Ø®Ø·Ø£: 'EMBEDDING_MODEL_NAME' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ .env.")
    exit()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CLIENT_DOCS_BASE_DIR = os.path.abspath(os.path.join(BASE_DIR, "../4_client_docs/"))
OUTPUTS_BASE_DIR = os.path.join(BASE_DIR, "_processing_outputs/")

def save_docs_to_file(docs: List[Document], filepath: str, message: str):
    # (Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)
    print(message)
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"--- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {len(docs)} ---\n\n")
            for i, doc in enumerate(docs):
                f.write(f"--- Chunk {i+1} ---\nMetadata: {doc.metadata}\n---\n{doc.page_content}\n\n")
        print(f"[+] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙÙŠ: '{filepath}'")
    except IOError as e:
        print(f"[!] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù '{filepath}': {e}")

def process_tenant(tenant_id: str):
    """
    ÙŠÙ†Ø³Ù‚ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¹Ù…ÙŠÙ„ ÙˆØ§Ø­Ø¯.
    """
    print("-" * 70)
    print(f"[>>] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
    print("-" * 70)

    source_directory = os.path.join(CLIENT_DOCS_BASE_DIR, tenant_id)
    if not os.path.isdir(source_directory):
        print(f"[!] Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'")
        return

    tenant_output_dir = os.path.join(OUTPUTS_BASE_DIR, tenant_id)

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ø³Ù… Ø§Ù„ÙƒÙŠØ§Ù† ---
    raw_docs, entity_name = load_documents(source_directory)
    if not raw_docs:
        print(f"[!] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        return
    save_docs_to_file(raw_docs, os.path.join(tenant_output_dir, "1_raw_content.txt"), 
                      "[*] Ø¬Ø§Ø±Ù Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…...")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ---
    cleaned_docs = clean_documents(raw_docs)
    save_docs_to_file(cleaned_docs, os.path.join(tenant_output_dir, "2_cleaned_content.txt"), 
                      "[*] Ø¬Ø§Ø±Ù Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ø¸ÙŠÙ...")
    
    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ ---
    chunks = split_documents(cleaned_docs)
    
    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© ---
    print(f"\n[+] Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù€ {len(chunks)} Ù‚Ø·Ø¹Ø©...")
    for chunk in chunks:
        chunk.metadata["tenant_id"] = tenant_id
        if entity_name:
            chunk.metadata["entity_name"] = entity_name
    print(f"[*] Ø§ÙƒØªÙ…Ù„ Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©.")
        
    save_docs_to_file(chunks, os.path.join(tenant_output_dir, "3_final_chunks.txt"), 
                      "[*] Ø¬Ø§Ø±Ù Ø­ÙØ¸ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ---
    print("\n[+] Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ø·Ø¹ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©...")
    add_to_vector_store(chunks, embedding_model_name=EMBEDDING_MODEL_NAME)

    print(f"\n[<<] Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

def main():
    # (Ø¯Ø§Ù„Ø© main ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)
    parser = argparse.ArgumentParser(description="Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¢Ù„ÙŠ.")
    parser.add_argument("--tenant", type=str, required=False, help="Ù‡ÙˆÙŠØ© Ø¹Ù…ÙŠÙ„ Ù…Ø¹ÙŠÙ† Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡.")
    args = parser.parse_args()
    
    if args.tenant:
        process_tenant(args.tenant)
    else:
        print("[*] Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡...")
        try:
            if not os.path.exists(CLIENT_DOCS_BASE_DIR):
                 print(f"[!] Ø®Ø·Ø£: Ø§Ù„Ø¯Ù„ÙŠÙ„ '{CLIENT_DOCS_BASE_DIR}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
                 return
            tenant_ids = [name for name in os.listdir(CLIENT_DOCS_BASE_DIR) if os.path.isdir(os.path.join(CLIENT_DOCS_BASE_DIR, name))]
            if not tenant_ids:
                print("[!] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¹Ù…Ù„Ø§Ø¡ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
                return
            print(f"[*] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(tenant_ids)} Ø¹Ù…ÙŠÙ„: {', '.join(tenant_ids)}")
            for tenant_id in tenant_ids:
                process_tenant(tenant_id)
            print("\n" + "="*70 + "\nğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰\n" + "="*70)
        except Exception as e:
            print(f"[!] Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

if __name__ == "__main__":
    main()
