# # #C:\Users\mahdi\support_service_platform\1_knowledge_pipeline\comprehensive_lab.py
# # # --- Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: 7 Ø·Ø±Ù‚ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ ---

# # import asyncio
# # import os
# # import time
# # from typing import List, Dict, Set
# # from dotenv import load_dotenv

# # # --- 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ---
# # from langchain_core.documents import Document
# # from langchain_community.embeddings import OllamaEmbeddings
# # from langchain_community.vectorstores import FAISS
# # from langchain.retrievers import BM25Retriever, EnsembleRetriever
# # from langchain.storage import InMemoryStore
# # from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
# # from langchain.text_splitter import RecursiveCharacterTextSplitter
# # from flashrank import Ranker, RerankRequest

# # # --- 2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
# # load_dotenv()
# # EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# # OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# # PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# # UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# # TOP_K = 7

# # # --- 3. Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù„Ø§ ØªØºÙŠÙŠØ±) ---
# # def print_results(docs: List[Document], title: str, duration: float, scores: List[float] = None):
# #     print("\n" + "="*80)
# #     print(f"ğŸ”¬ Ù†ØªØ§Ø¦Ø¬ Ø·Ø±ÙŠÙ‚Ø©: {title}")
# #     print(f"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {duration:.4f} Ø«Ø§Ù†ÙŠØ©")
# #     print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(docs)}")
# #     print("="*80)
# #     if not docs:
# #         print("   -> Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.")
# #         return
# #     for i, doc in enumerate(docs):
# #         source = doc.metadata.get('source', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ').split('\\')[-1]
# #         content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:110]
# #         score_info = f"[Ø§Ù„Ø¯Ø±Ø¬Ø©: {scores[i]:.4f}]" if scores and i < len(scores) else ""
# #         print(f"   {i+1}. {score_info} [Ø§Ù„Ù…ØµØ¯Ø±: {source}] -> \"{content_preview}...\"")
# #     print("-" * 80)

# # # --- 4. Ø§Ù„Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
# # async def run_final_showdown_lab(question: str, tenant_id: str, embeddings: OllamaEmbeddings, vector_store: FAISS, reranker: Ranker, all_tenant_docs: Dict[str, List[Document]]):
# #     print("\n" + "#"*30 + f" Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„: '{tenant_id}' | Ø§Ù„Ø³Ø¤Ø§Ù„: '{question}' " + "#"*30)
    
# #     tenant_docs = all_tenant_docs.get(tenant_id)
# #     if not tenant_docs:
# #         print(f"âŒ Ø®Ø·Ø£: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")
# #         return

# #     # --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª ---
# #     faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
# #     bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=TOP_K)
# #     ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
# #     store = InMemoryStore()
# #     parent_document_retriever = ParentDocumentRetriever(vectorstore=vector_store, docstore=store, child_splitter=RecursiveCharacterTextSplitter(chunk_size=400))
# #     parent_document_retriever.add_documents(tenant_docs, ids=None)

# #     # --- ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ---
    
# #     # 1. BM25 (Keywords)
# #     start_time = time.time(); bm25_docs = await bm25_retriever.ainvoke(question); duration = time.time() - start_time
# #     print_results(bm25_docs, "1. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (BM25)", duration)

# #     # 2. Vector Search
# #     start_time = time.time(); vector_docs = await faiss_retriever.ainvoke(question); duration = time.time() - start_time
# #     print_results(vector_docs, "2. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰ (Vector Search)", duration)

# #     # 3. Hybrid
# #     start_time = time.time(); hybrid_docs = await ensemble_retriever.ainvoke(question); duration = time.time() - start_time
# #     print_results(hybrid_docs, "3. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid)", duration)

# #     # 4. Hybrid + Reranker
# #     if hybrid_docs:
# #         start_time = time.time()
# #         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(hybrid_docs)]
# #         reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
# #         duration = time.time() - start_time
# #         original_docs_map = {i: doc for i, doc in enumerate(hybrid_docs)}
# #         final_docs = [original_docs_map[res["id"]] for res in reranked_results]
# #         final_scores = [res["score"] for res in reranked_results]
# #         print_results(final_docs, "4. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† + Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Hybrid + Reranker)", duration, scores=final_scores)

# #     # 5. Parent Document
# #     start_time = time.time(); parent_docs = await asyncio.to_thread(parent_document_retriever.invoke, question); duration = time.time() - start_time
# #     print_results(parent_docs, "5. Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© (Parent Document)", duration)

# #     # 6. Parent + Reranker
# #     if parent_docs:
# #         start_time = time.time()
# #         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(parent_docs)]
# #         reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
# #         duration = time.time() - start_time
# #         original_docs_map = {i: doc for i, doc in enumerate(parent_docs)}
# #         super_hybrid_docs = [original_docs_map[res["id"]] for res in reranked_results]
# #         super_hybrid_scores = [res["score"] for res in reranked_results]
# #         print_results(super_hybrid_docs, "6. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„ÙØ§Ø¦Ù‚ (Parent + Reranker)", duration, scores=super_hybrid_scores)

# #     # 7. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker)
# #     # Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
# #     combined_initial_docs = hybrid_docs + parent_docs
# #     # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
# #     unique_docs_map = {doc.page_content: doc for doc in reversed(combined_initial_docs)}
# #     unique_docs = list(unique_docs_map.values())[::-1]
    
# #     if unique_docs:
# #         start_time = time.time()
# #         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
# #         reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
# #         duration = time.time() - start_time
# #         original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
# #         ultimate_docs = [original_docs_map[res["id"]] for res in reranked_results]
# #         ultimate_scores = [res["score"] for res in reranked_results]
# #         print_results(ultimate_docs, "7. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker)", duration, scores=ultimate_scores)


# # # --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ†ÙÙŠØ° ---
# # async def main():
# #     print("--- ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸš€ ---")
# #     try:
# #         embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
# #         vector_store = FAISS.load_local(UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
# #         reranker = Ranker()
        
# #         all_docs = list(vector_store.docstore._dict.values())
# #         all_tenant_docs = {}
# #         for doc in all_docs:
# #             tenant_id = doc.metadata.get("tenant_id")
# #             if tenant_id:
# #                 if tenant_id not in all_tenant_docs:
# #                     all_tenant_docs[tenant_id] = []
# #                 all_tenant_docs[tenant_id].append(doc)
# #         print("--- âœ… Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¬Ø§Ù‡Ø²Ø©. ---")
# #     except Exception as e:
# #         print(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø©: {e}")
# #         return

# #     # --- ØªØ¹Ø±ÙŠÙ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ù†ÙØ³ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©) ---
# #     test_cases = [
# #         {
# #             "tenant_id": "sys",
# #             "question": "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø¹Ø¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ÙŠØ¯Ø§Ù†ÙŠØ©ØŸ"
# #         },
# #         {
# #             "tenant_id": "un",
# #             "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡ ÙˆÙ‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯ØŸ"
# #         },
# #         {
# #             "tenant_id": "school_beta",
# #             "question": "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù‚ØµÙ‰ (Max Pooling) ÙˆØ§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø· (Average Pooling)."
# #         },
# #         {
# #             "tenant_id": "university_alpha",
# #             "question": "ÙƒÙŠÙ ÙŠØ³Ø§Ù‡Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¹Ø§Ø¦Ø¯ Ù…Ø§Ù„ÙŠ Ù„Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙˆÙ…Ø§ Ù‡ÙŠ Ø­Ø¯ÙˆØ¯Ù‡ØŸ"
# #         }
# #     ]

# #     # --- ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
# #     for case in test_cases:
# #         await run_final_showdown_lab(
# #             question=case["question"],
# #             tenant_id=case["tenant_id"],
# #             embeddings=embeddings,
# #             vector_store=vector_store,
# #             reranker=reranker,
# #             all_tenant_docs=all_tenant_docs
# #         )

# # if __name__ == "__main__":
# #     asyncio.run(main())
# # --- Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ø§Ù„Ø¥ØµØ¯Ø§Ø± 2.0 ---

# import asyncio
# import os
# import time
# from typing import List, Dict
# from dotenv import load_dotenv

# # --- 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ---
# from langchain_core.documents import Document
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.retrievers import BM25Retriever
# from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
# from langchain.storage import InMemoryStore
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# # **Ø§Ù„ØªØ­Ø³ÙŠÙ†: Ø§Ø³ØªØ®Ø¯Ø§Ù… CrossEncoder Ù…Ù† sentence-transformers Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨**
# from sentence_transformers.cross_encoder import CrossEncoder

# # --- 2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
# load_dotenv()
# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# TOP_K = 7
# # **Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªØ­Ø¯ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ Reranker Ù‚ÙˆÙŠ**
# RERANK_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'

# # --- 3. Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù…ÙØ­Ø³Ù‘Ù†Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„) ---
# def print_results(docs: List[Document], title: str, duration: float, scores: List[float] = None):
#     print("\n" + "="*80)
#     print(f"ğŸ”¬ Ù†ØªØ§Ø¦Ø¬ Ø·Ø±ÙŠÙ‚Ø©: {title}")
#     print(f"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ + Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨: {duration:.4f} Ø«Ø§Ù†ÙŠØ©")
#     print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(docs)}")
#     print("="*80)
#     if not docs:
#         print("   -> Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.")
#         return
#     for i, doc in enumerate(docs):
#         source = doc.metadata.get('source', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ').split('\\')[-1]
#         content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:110]
#         score_info = f"[Ø§Ù„Ø¯Ø±Ø¬Ø©: {scores[i]:.4f}]" if scores and i < len(scores) else ""
#         print(f"   {i+1}. {score_info} [Ø§Ù„Ù…ØµØ¯Ø±: {source}] -> \"{content_preview}...\"")
#     print("-" * 80)

# # --- 4. Ø§Ù„Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ---
# async def run_final_showdown_lab(
#     question: str,
#     tenant_id: str,
#     vector_store: FAISS,
#     reranker: CrossEncoder,
#     retrievers_cache: Dict[str, Dict]
# ):
#     print("\n" + "#"*30 + f" Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„: '{tenant_id}' | Ø§Ù„Ø³Ø¤Ø§Ù„: '{question}' " + "#"*30)

#     # --- Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ù‡ÙŠØ£Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© ---
#     tenant_retrievers = retrievers_cache.get(tenant_id)
#     if not tenant_retrievers:
#         print(f"âŒ Ø®Ø·Ø£: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ù‡ÙŠØ£Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")
#         return

#     faiss_retriever = tenant_retrievers['faiss']
#     bm25_retriever = tenant_retrievers['bm25']
#     ensemble_retriever = tenant_retrievers['ensemble']
#     parent_document_retriever = tenant_retrievers['parent']

#     # --- ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ---

#     # 1. BM25 (Keywords)
#     start_time = time.time(); bm25_docs = await bm25_retriever.ainvoke(question); duration = time.time() - start_time
#     print_results(bm25_docs, "1. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (BM25)", duration)

#     # 2. Vector Search
#     start_time = time.time(); vector_docs = await faiss_retriever.ainvoke(question); duration = time.time() - start_time
#     print_results(vector_docs, "2. Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰ (Vector Search)", duration)

#     # 3. Hybrid (70% Vector, 30% BM25)
#     start_time = time.time(); hybrid_docs = await ensemble_retriever.ainvoke(question); duration = time.time() - start_time
#     print_results(hybrid_docs, "3. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid - 70/30)", duration)

#     # 4. Hybrid + Reranker (Ù…Ø­Ø³Ù‘Ù†)
#     if hybrid_docs:
#         start_time = time.time()
#         # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø²ÙˆØ§Ø¬ Ù…Ù† [Ø§Ù„Ø³Ø¤Ø§Ù„, Ø§Ù„Ù…Ø­ØªÙˆÙ‰] Ù„Ù€ CrossEncoder
#         passages_for_reranking = [[question, doc.page_content] for doc in hybrid_docs]
#         # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª
#         reranked_scores = reranker.predict(passages_for_reranking)
#         duration = time.time() - start_time
#         # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹ Ø¯Ø±Ø¬Ø§ØªÙ‡Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØªØ±ØªÙŠØ¨Ù‡Ø§
#         reranked_hybrid_docs = sorted(zip(hybrid_docs, reranked_scores), key=lambda x: x[1], reverse=True)
#         final_docs = [doc for doc, score in reranked_hybrid_docs]
#         final_scores = [score for doc, score in reranked_hybrid_docs]
#         print_results(final_docs, "4. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† + Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Hybrid + Reranker)", duration, scores=final_scores)

#     # 5. Parent Document
#     start_time = time.time(); parent_docs = await asyncio.to_thread(parent_document_retriever.invoke, question); duration = time.time() - start_time
#     print_results(parent_docs, "5. Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© (Parent Document)", duration)

#     # 6. Parent + Reranker (Ù…Ø­Ø³Ù‘Ù†)
#     if parent_docs:
#         start_time = time.time()
#         passages_for_reranking = [[question, doc.page_content] for doc in parent_docs]
#         reranked_scores = reranker.predict(passages_for_reranking)
#         duration = time.time() - start_time
#         reranked_parent_docs = sorted(zip(parent_docs, reranked_scores), key=lambda x: x[1], reverse=True)
#         final_docs = [doc for doc, score in reranked_parent_docs]
#         final_scores = [score for doc, score in reranked_parent_docs]
#         print_results(final_docs, "6. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„ÙØ§Ø¦Ù‚ (Parent + Reranker)", duration, scores=final_scores)

#     # 7. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker) (Ù…Ø­Ø³Ù‘Ù†)
#     combined_initial_docs = hybrid_docs + parent_docs
#     unique_docs_map = {doc.page_content: doc for doc in reversed(combined_initial_docs)}
#     unique_docs = list(unique_docs_map.values())[::-1]

#     if unique_docs:
#         start_time = time.time()
#         passages_for_reranking = [[question, doc.page_content] for doc in unique_docs]
#         reranked_scores = reranker.predict(passages_for_reranking)
#         duration = time.time() - start_time
#         reranked_ultimate_docs = sorted(zip(unique_docs, reranked_scores), key=lambda x: x[1], reverse=True)
#         ultimate_docs = [doc for doc, score in reranked_ultimate_docs]
#         ultimate_scores = [score for doc, score in reranked_ultimate_docs]
#         print_results(ultimate_docs, "7. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker)", duration, scores=ultimate_scores)


# # --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ†ÙÙŠØ° (Ù…Ø­Ø³Ù‘Ù†Ø©) ---
# async def main():
#     print("--- ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (v2.0) ğŸš€ ---")
#     try:
#         embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
#         vector_store = FAISS.load_local(UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
#         # **Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ CrossEncoder Ø§Ù„Ù‚ÙˆÙŠ**
#         reranker = CrossEncoder(RERANK_MODEL)

#         all_docs = list(vector_store.docstore._dict.values())
#         all_tenant_docs = {}
#         for doc in all_docs:
#             tenant_id = doc.metadata.get("tenant_id")
#             if tenant_id:
#                 if tenant_id not in all_tenant_docs:
#                     all_tenant_docs[tenant_id] = []
#                 all_tenant_docs[tenant_id].append(doc)

#         # **Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ù…Ø¤Ù‚ØªØ©**
#         print("--- â³ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„... ---")
#         retrievers_cache = {}
#         for tenant_id, tenant_docs in all_tenant_docs.items():
#             print(f"   -> ØªÙ‡ÙŠØ¦Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
#             faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
#             bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=TOP_K)
#             # **Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†**
#             ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
#             store = InMemoryStore()
#             parent_document_retriever = ParentDocumentRetriever(vectorstore=vector_store, docstore=store, child_splitter=RecursiveCharacterTextSplitter(chunk_size=400))
#             parent_document_retriever.add_documents(tenant_docs, ids=None)

#             retrievers_cache[tenant_id] = {
#                 'faiss': faiss_retriever,
#                 'bm25': bm25_retriever,
#                 'ensemble': ensemble_retriever,
#                 'parent': parent_document_retriever
#             }

#         print("--- âœ… Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¬Ø§Ù‡Ø²Ø©. ---")
#     except Exception as e:
#         print(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø©: {e}")
#         return

#     # --- ØªØ¹Ø±ÙŠÙ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ§Ù„Ù…ØªÙ†ÙˆØ¹Ø© ---
#     test_cases = [
#         {
#             "tenant_id": "school_beta",
#             "question": "Ù…Ø§ Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© TensorFlowØŸ"
#         },
#         {
#             "tenant_id": "school_beta",
#             "question": "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© ÙˆØ§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ«ÙŠÙØ©."
#         },
#         {
#             "tenant_id": "sys",
#             "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ Ø³Ø¯Ø§Ø¯ Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ØŸ"
#         },
#         {
#             "tenant_id": "school_beta",
#             "question": "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø´ÙƒÙ„Ø© ØªÙ„Ø§Ø´ÙŠ Ù…Ø´ØªÙ‚Ø© Ø§Ù„Ø®Ø·Ø£ (Vanishing Gradient)ØŸ"
#         },
#         {
#             "tenant_id": "university_alpha",
#             "question": "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ù„ØªØ·Ø¨ÙŠÙ‚ Plant Care Ù„Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ†ØŒ ÙˆÙ…Ø§ Ù‡ÙŠ Ø­Ø¯ÙˆØ¯Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©ØŸ"
#         }
#     ]

#     # --- ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
#     for case in test_cases:
#         await run_final_showdown_lab(
#             question=case["question"],
#             tenant_id=case["tenant_id"],
#             vector_store=vector_store,
#             reranker=reranker,
#             retrievers_cache=retrievers_cache
#         )

# if __name__ == "__main__":
#     # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ«Ø¨ÙŠØª sentence-transformers
#     # pip install -U sentence-transformers
#     asyncio.run(main())
# final_retrieval_lab.py - Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ø³ØªØ±Ø¬Ø§Ø¹

import os
import asyncio
import time
from typing import List, Dict
from dotenv import load_dotenv

# --- 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ---
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers.cross_encoder import CrossEncoder

# --- 2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---
load_dotenv()

# Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ Ø£Ø«Ø¨Øª ØªÙÙˆÙ‚Ù‡
FINAL_EMBEDDING_MODEL = HuggingFaceEmbeddings(
    model_name="paraphrase-multilingual-mpnet-base-v2",
    model_kwargs={'device': 'cpu'} # Ø§Ø³ØªØ®Ø¯Ù… 'cuda' Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ GPU
)
RERANK_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'

# Ù…Ø³Ø§Ø±Ø§Øª
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
UNIFIED_DB_PATH = os.path.abspath(os.path.join(BASE_DIR, "../3_shared_resources/vector_db/"))
TOP_K = 7

# --- 3. Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù„Ø§ ØªØºÙŠÙŠØ±) ---
def print_results(docs: List[Document], title: str, duration: float, scores: List[float] = None):
    print("\n" + "="*80)
    print(f"ğŸ”¬ Ù†ØªØ§Ø¦Ø¬ Ø·Ø±ÙŠÙ‚Ø©: {title}")
    print(f"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {duration:.4f} Ø«Ø§Ù†ÙŠØ©")
    print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(docs)}")
    print("="*80)
    if not docs:
        print("   -> Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.")
        return
    for i, doc in enumerate(docs):
        source = os.path.basename(doc.metadata.get('source', 'N/A'))
        content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:110]
        score_info = f"[Ø§Ù„Ø¯Ø±Ø¬Ø©: {scores[i]:.4f}]" if scores is not None else ""
        print(f"   {i+1}. {score_info} [{source}] -> \"{content_preview}...\"")
    print("-" * 80)

# --- 4. Ø§Ù„Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
async def run_retrieval_test(
    question: str,
    tenant_id: str,
    retrievers_cache: Dict[str, Dict],
    reranker: CrossEncoder
):
    print("\n" + "#"*30 + f" Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„: '{tenant_id}' | Ø§Ù„Ø³Ø¤Ø§Ù„: '{question}' " + "#"*30)

    tenant_retrievers = retrievers_cache.get(tenant_id)
    if not tenant_retrievers:
        print(f"âŒ Ø®Ø·Ø£: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ù‡ÙŠØ£Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")
        return

    # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ù‡ÙŠØ£Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§
    faiss_retriever = tenant_retrievers['faiss']
    bm25_retriever = tenant_retrievers['bm25']
    ensemble_retriever = tenant_retrievers['ensemble']
    parent_document_retriever = tenant_retrievers['parent']

    # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 1: Vector Search ---
    start_time = time.time()
    vector_docs = await faiss_retriever.ainvoke(question)
    duration = time.time() - start_time
    print_results(vector_docs, "1. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Vector Search)", duration)

    # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 2: Hybrid Search ---
    start_time = time.time()
    hybrid_docs = await ensemble_retriever.ainvoke(question)
    duration = time.time() - start_time
    print_results(hybrid_docs, "2. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid - 70/30)", duration)

    # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 3: Parent Document Retriever ---
    start_time = time.time()
    parent_docs = await asyncio.to_thread(parent_document_retriever.invoke, question)
    duration = time.time() - start_time
    print_results(parent_docs, "3. Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ø£ØµÙ„ (Parent)", duration)

    # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 4: Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker) ---
    combined_docs = list({doc.page_content: doc for doc in reversed(hybrid_docs + parent_docs)}.values())[::-1]
    if combined_docs:
        start_time = time.time()
        passages = [[question, doc.page_content] for doc in combined_docs]
        scores = reranker.predict(passages)
        rerank_duration = time.time() - start_time
        
        reranked_results = sorted(zip(combined_docs, scores), key=lambda x: x[1], reverse=True)
        final_docs = [doc for doc, score in reranked_results][:TOP_K]
        final_scores = [score for doc, score in reranked_results][:TOP_K]
        print_results(final_docs, "4. Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Hybrid + Parent + Reranker)", rerank_duration, scores=final_scores)

# --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ†ÙÙŠØ° ---
async def main():
    print("--- ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (v4.0) ğŸš€ ---")
    try:
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„ØµØ­ÙŠØ­
        print(f"[*] ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù†: '{UNIFIED_DB_PATH}'")
        vector_store = FAISS.load_local(
            UNIFIED_DB_PATH, 
            embeddings=FINAL_EMBEDDING_MODEL, 
            allow_dangerous_deserialization=True
        )
        print("[*] ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­.")

        # Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‡ÙŠØ¦Ø© Reranker
        reranker = CrossEncoder(RERANK_MODEL)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙˆØªÙˆØ²ÙŠØ¹Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…ÙŠÙ„
        # .docstore._dict Ù‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø¯Ø§Ø®Ù„ÙŠØ©ØŒ Ù…Ù† Ø§Ù„Ø£ÙØ¶Ù„ ØªØ¬Ù†Ø¨Ù‡Ø§. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± Ø¹Ù…ÙˆÙ…ÙŠØ©.
        # Ù†ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„ÙÙ‡Ø±Ø³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ù…Ù† 0 Ø¥Ù„Ù‰ N-1
        total_docs = len(vector_store.index_to_docstore_id)
        all_chunks = [vector_store.docstore.search(vector_store.index_to_docstore_id[i]) for i in range(total_docs)]
        
        all_tenant_chunks = {}
        for chunk in all_chunks:
            tenant_id = chunk.metadata.get("tenant_id")
            if tenant_id:
                if tenant_id not in all_tenant_chunks:
                    all_tenant_chunks[tenant_id] = []
                all_tenant_chunks[tenant_id].append(chunk)

        # Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„
        print("--- â³ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„... ---")
        retrievers_cache = {}
        for tenant_id, tenant_chunks in all_tenant_chunks.items():
            print(f"   -> ØªÙ‡ÙŠØ¦Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
            faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
            bm25_retriever = BM25Retriever.from_documents(tenant_chunks, k=TOP_K)
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
            
            # Parent retriever ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ø§ Ù†Ù…Ù„ÙƒÙ‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ù‡Ù†Ø§.
            # ÙƒØ­Ù„ Ø¨Ø¯ÙŠÙ„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ Ø³Ù†Ø¨Ù†ÙŠ Docstore Ù…Ø¤Ù‚Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªØ§Ø­Ø©.
            # ÙÙŠ Ù†Ø¸Ø§Ù… Ø¥Ù†ØªØ§Ø¬ÙŠØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„.
            store = InMemoryStore()
            store.mset([(str(i), doc) for i, doc in enumerate(tenant_chunks)])
            
            parent_document_retriever = ParentDocumentRetriever(
                vectorstore=vector_store, 
                docstore=store, 
                child_splitter=RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250),
            )
            
            retrievers_cache[tenant_id] = {
                'faiss': faiss_retriever,
                'bm25': bm25_retriever,
                'ensemble': ensemble_retriever,
                'parent': parent_document_retriever
            }

        print("--- âœ… Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±. ---")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø©: {e}")
        return

    # --- ØªØ¹Ø±ÙŠÙ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© ---
    test_cases = [
        {"tenant_id": "school_beta", "question": "Ù…Ø§ Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© TensorFlowØŸ"},
        {"tenant_id": "school_beta", "question": "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© ÙˆØ§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ«ÙŠÙØ©."},
        {"tenant_id": "sys", "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ Ø³Ø¯Ø§Ø¯ Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ØŸ"},
        {"tenant_id": "school_beta", "question": "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø´ÙƒÙ„Ø© ØªÙ„Ø§Ø´ÙŠ Ù…Ø´ØªÙ‚Ø© Ø§Ù„Ø®Ø·Ø£ (Vanishing Gradient)ØŸ"},
        {"tenant_id": "university_alpha", "question": "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ù„ØªØ·Ø¨ÙŠÙ‚ Plant Care Ù„Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ†ØŒ ÙˆÙ…Ø§ Ù‡ÙŠ Ø­Ø¯ÙˆØ¯Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©ØŸ"},
        {"tenant_id": "un", "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡ ÙˆÙ‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯ØŸ"}
    ]

    # --- ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
    for case in test_cases:
        await run_retrieval_test(
            question=case["question"],
            tenant_id=case["tenant_id"],
            retrievers_cache=retrievers_cache,
            reranker=reranker
        )

    print("\n--- ğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ. ---")

if __name__ == "__main__":
    asyncio.run(main())

