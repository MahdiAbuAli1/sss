# --- Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø³Ø³ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªÙ‚Ù†ÙŠØ§ØªÙ‡ ---

import os
import time
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv

# --- 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ---
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings, HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ParentDocumentRetriever
from langchain.storage import InMemoryStore
from sentence_transformers.cross_encoder import CrossEncoder

load_dotenv()

# --- 2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ---
DOCS_PATH = os.path.join(os.path.dirname(__file__), "..", "4_client_docs")
TOP_K = 7
OLLAMA_BASE_URL = os.getenv("OLLAMA_HOST", "http://localhost:11434" )
RERANK_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'

# --- 3. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£Ø³Ø³ (Foundations) ---
FOUNDATIONS = {
    "A_Fast_Compact": {
        "name": "Ø§Ù„Ø£Ø³Ø§Ø³ Ø£: Ø³Ø±Ø¹Ø© ÙˆØ£ØµØºØ± Ø­Ø¬Ù…Ù‹Ø§",
        "splitter": RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100),
        "embedding_model": OllamaEmbeddings(
            model=os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b"),
            base_url=OLLAMA_BASE_URL
        ),
    },
    "B_Accurate_Contextual": {
        "name": "Ø§Ù„Ø£Ø³Ø§Ø³ Ø¨: Ø¯Ù‚Ø© ÙˆØ³ÙŠØ§Ù‚",
        "splitter": RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200),
        "embedding_model": HuggingFaceEmbeddings(
            model_name="paraphrase-multilingual-mpnet-base-v2"
        ),
    }
}

# --- 4. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ---
def load_all_documents(path: str) -> List[Document]:
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡."""
    print("--- â³ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡... ---")
    all_docs = []
    for tenant_id in os.listdir(path):
        tenant_path = os.path.join(path, tenant_id)
        if os.path.isdir(tenant_path):
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… DirectoryLoader Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
            pdf_loader = DirectoryLoader(tenant_path, glob="**/*.pdf", loader_cls=PyPDFLoader, recursive=True)
            docx_loader = DirectoryLoader(tenant_path, glob="**/*.docx", loader_cls=Docx2txtLoader, recursive=True)
            txt_loader = DirectoryLoader(tenant_path, glob="**/*.txt", loader_cls=TextLoader, recursive=True)
            
            docs = pdf_loader.load() + docx_loader.load() + txt_loader.load()
            
            # Ø¥Ø¶Ø§ÙØ© tenant_id Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠØªØ§-Ø¨ÙŠØ§Ù†Ø§Øª
            for doc in docs:
                doc.metadata['tenant_id'] = tenant_id
            all_docs.extend(docs)
            print(f"   -> ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(docs)} Ù…Ø³ØªÙ†Ø¯ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
    return all_docs

def print_benchmark_results(title: str, docs: List[Document], duration: float, scores: List[float] = None):
    """Ø·Ø¨Ø§Ø¹Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…."""
    print("\n" + "-"*40)
    print(f"ğŸ”¬ {title}")
    print(f"â±ï¸ Ø§Ù„Ø²Ù…Ù†: {duration:.4f} Ø«Ø§Ù†ÙŠØ© | ğŸ“„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(docs)}")
    print("-"*40)
    if not docs:
        print("   -> Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.")
        return
    for i, doc in enumerate(docs):
        source = os.path.basename(doc.metadata.get('source', 'N/A'))
        content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:90]
        score_info = f"[Ø§Ù„Ø¯Ø±Ø¬Ø©: {scores[i]:.4f}]" if scores and i < len(scores) else ""
        print(f"   {i+1}. {score_info} [{source}] -> \"{content_preview}...\"")

# --- 5. Ø§Ù„Ù…Ø®ØªØ¨Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
async def run_benchmark(question: str, tenant_id: str, foundation_builds: Dict, reranker: CrossEncoder):
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯."""
    print("\n" + "#"*30 + f" Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„: '{tenant_id}' | Ø§Ù„Ø³Ø¤Ø§Ù„: '{question}' " + "#"*30)

    for key, build in foundation_builds.items():
        print(f"\n{'='*20} Ø§Ø³ØªØ®Ø¯Ø§Ù… [{build['name']}] {'='*20}")
        
        retrievers = build['retrievers'][tenant_id]
        faiss_retriever = retrievers['faiss']
        bm25_retriever = retrievers['bm25']
        ensemble_retriever = retrievers['ensemble']
        parent_document_retriever = retrievers['parent']

        # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 1: Vector Search ---
        start_time = time.time()
        vector_docs = await faiss_retriever.ainvoke(question)
        duration = time.time() - start_time
        print_benchmark_results(f"[{build['name']}] Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Vector)", vector_docs, duration)

        # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 2: Hybrid Search ---
        start_time = time.time()
        hybrid_docs = await ensemble_retriever.ainvoke(question)
        duration = time.time() - start_time
        print_benchmark_results(f"[{build['name']}] Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid)", hybrid_docs, duration)

        # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 3: Parent Document ---
        start_time = time.time()
        parent_docs = await asyncio.to_thread(parent_document_retriever.invoke, question)
        duration = time.time() - start_time
        print_benchmark_results(f"[{build['name']}] Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ø£ØµÙ„ (Parent)", parent_docs, duration)

        # --- Ø§Ù„ØªÙ‚Ù†ÙŠØ© 4: Ultimate Retriever ---
        combined_docs = list({doc.page_content: doc for doc in reversed(hybrid_docs + parent_docs)}.values())[::-1]
        if combined_docs:
            start_time = time.time()
            passages = [[question, doc.page_content] for doc in combined_docs]
            scores = reranker.predict(passages)
            duration = time.time() - start_time
            
            reranked_results = sorted(zip(combined_docs, scores), key=lambda x: x[1], reverse=True)
            final_docs = [doc for doc, score in reranked_results][:TOP_K]
            final_scores = [score for doc, score in reranked_results][:TOP_K]
            print_benchmark_results(f"[{build['name']}] Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Ultimate)", final_docs, duration, scores=final_scores)

# --- 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ†ÙÙŠØ° ---
async def main():
    print("--- ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„ (v3.0) ğŸš€ ---")
    
    all_docs = load_all_documents(DOCS_PATH)
    reranker = CrossEncoder(RERANK_MODEL)
    
    foundation_builds = {}

    for key, config in FOUNDATIONS.items():
        print(f"\n--- ğŸ—ï¸ Ø¬Ø§Ø±Ù Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ù„Ù€ [{config['name']}] ---")
        
        # 1. Ø§Ù„ØªÙ‚Ø·ÙŠØ¹
        chunks = config['splitter'].split_documents(all_docs)
        
        # 2. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
        print(f"   -> Ø¬Ø§Ø±Ù Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ {config['embedding_model'].__class__.__name__}...")
        vector_store = await asyncio.to_thread(FAISS.from_documents, chunks, config['embedding_model'])
        
        # 3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„
        tenant_chunks = {}
        for chunk in chunks:
            tenant_id = chunk.metadata['tenant_id']
            if tenant_id not in tenant_chunks:
                tenant_chunks[tenant_id] = []
            tenant_chunks[tenant_id].append(chunk)
            
        # 4. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„
        tenant_retrievers = {}
        for tenant_id, t_chunks in tenant_chunks.items():
            faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
            bm25_retriever = BM25Retriever.from_documents(t_chunks, k=TOP_K)
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
            
            # Parent retriever ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠØ©
            original_tenant_docs = [doc for doc in all_docs if doc.metadata['tenant_id'] == tenant_id]
            store = InMemoryStore()
            parent_document_retriever = ParentDocumentRetriever(
                vectorstore=vector_store, 
                docstore=store, 
                child_splitter=config['splitter'],
                parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000) # Ù…Ù‚Ø§Ø·Ø¹ Ø£ØµÙ„ÙŠØ© Ø£ÙƒØ¨Ø±
            )
            parent_document_retriever.add_documents(original_tenant_docs, ids=None)
            
            tenant_retrievers[tenant_id] = {
                'faiss': faiss_retriever,
                'bm25': bm25_retriever,
                'ensemble': ensemble_retriever,
                'parent': parent_document_retriever
            }
        
        foundation_builds[key] = {
            "name": config["name"],
            "retrievers": tenant_retrievers
        }
        print(f"--- âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ù„Ù€ [{config['name']}] Ø¨Ù†Ø¬Ø§Ø­ ---")

    # --- ØªØ¹Ø±ÙŠÙ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
    test_cases = [
        {"tenant_id": "school_beta", "question": "Ù…Ø§ Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© TensorFlowØŸ"},
        {"tenant_id": "school_beta", "question": "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© ÙˆØ§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ«ÙŠÙØ©."},
        {"tenant_id": "sys", "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ Ø³Ø¯Ø§Ø¯ Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ØŸ"},
        {"tenant_id": "school_beta", "question": "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø´ÙƒÙ„Ø© ØªÙ„Ø§Ø´ÙŠ Ù…Ø´ØªÙ‚Ø© Ø§Ù„Ø®Ø·Ø£ (Vanishing Gradient)ØŸ"},
        {"tenant_id": "university_alpha", "question": "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ù„ØªØ·Ø¨ÙŠÙ‚ Plant Care Ù„Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ†ØŒ ÙˆÙ…Ø§ Ù‡ÙŠ Ø­Ø¯ÙˆØ¯Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©ØŸ"},
        {"tenant_id": "un", "question": "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡ ÙˆÙ‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯ØŸ"}
    ]

    # --- ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ---
    for case in test_cases:
        await run_benchmark(case["question"], case["tenant_id"], foundation_builds, reranker)

    print("\n--- ğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ù…Ø®ØªØ¨Ø± Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„. ---")

if __name__ == "__main__":
    asyncio.run(main())
