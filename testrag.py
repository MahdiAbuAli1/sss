# testrag.py
import os
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "3_shared_resources/vector_db"))

if not EMBEDDING_MODEL_NAME:
    raise ValueError(" Ø®Ø·Ø£: Ù…ØªØºÙŠØ± EMBEDDING_MODEL_NAME ÙØ§Ø±Øº! ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ù„Ù .env")

def test_tenant_retrieval(tenant_id: str, question: str, k: int = 4):
    print(f"ğŸŸ¢ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù€ tenant_id='{tenant_id}' Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„: '{question}'")
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
    
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© FAISS
    if not os.path.exists(os.path.join(VECTOR_DB_PATH, "index.faiss")):
        raise FileNotFoundError(f"âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ {VECTOR_DB_PATH}")
    
    vector_store = FAISS.load_local(VECTOR_DB_PATH, embeddings=embeddings_model, allow_dangerous_deserialization=True)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡
    question_vector = embeddings_model.embed_query(question)

    # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØ¬Ù‡
    docs = vector_store.similarity_search_by_vector(question_vector, k=k)

    print(f"==== Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© ====")
    for i, doc in enumerate(docs):
        print(f"--- Document/Chunk {i+1} ---")
        print(f"Metadata: {doc.metadata}")
        print(doc.page_content[:500] + ('...' if len(doc.page_content) > 500 else ''))  # Ù†Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 500 Ø­Ø±Ù ÙÙ‚Ø·
        print("\n")

if __name__ == "__main__":
    test_tenant_retrieval("university_alpha", "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù‚ØªØ±Ø­ØŸ", k=4)
