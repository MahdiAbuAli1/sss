# production_app/core/agent.py

import logging
import asyncio
import json
import random
import time
import uuid
import os
from typing import AsyncGenerator, Dict, List

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from sentence_transformers.cross_encoder import CrossEncoder

from . import config

# # --- ุงููุงูุจ ุงูููุงุฆู ุงููุญุณูู ูุน ุดุฎุตูุฉ ูุงุถุญุฉ ---
# FINAL_PROMPT = ChatPromptTemplate.from_messages([
#     ("system", """ุฃูุช ูู ุงููุธุงู ุงูุฐู ูุชู ุงูุณุคุงู ุนูู. ุชุญุฏุซ ุฏุงุฆููุง ุจุตูุบุฉ ุงููุชููู (ุฃูุงุ ูุฏูุ ูุธุงุฆูู ูู...). ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑููุง** ุนูู "ุงูุณูุงู" ุงูููุฏู.

# ### ููุงุนุฏ ุตุงุฑูุฉ:
# 1.  **ุงููููุฉ:** ุฃูุช ูู ุงููุธุงู. ูุง ุชูู ุฃุจุฏูุง "ูุฐุง ุงููุธุงู" ุฃู "ุงููุธุงู ุงููุฐููุฑ". ูู "ุฃูุง" ุฃู "ูุธุงุฆfi ูู".
# 2.  **ุงูุชูุณูู:** ุงุณุชุฎุฏู ุชูุณูู Markdown ูุชูุธูู ุฅุฌุงุจุงุชู (ุนูุงููู ##ุ ููุงุฆู 1., 2., -).
# 3.  **ุงูุงูุชุฒุงู ุจุงูุณูุงู:** ุฅุฐุง ูุงู ุงูุณูุงู ูุง ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ุฃู ูุงู ุงูุณุคุงู ุนุงููุงุ ุฃุฌุจ ุจุฅุญุฏู ุงูุฌููุชูู ุงูุชุงููุชูู **ููุท** (ุจุฏูู ุฃู ุชูุณูู):
#     - "ุฃูุง ูุณุงุนุฏ ุฏุนู ููู ูุชุฎุตุตุ ููุง ูููููู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุนุงูุฉ."
#     - "ุจุฎุตูุต ุณุคุงูู '{input}'ุ ูุง ุชูุฌุฏ ูุฏู ูุนูููุงุช ูุงููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุญุงูููุง."
# 4.  **ุงูุฐุงูุฑุฉ:** ุฅุฐุง ุทูุจ ุงููุณุชุฎุฏู "ุงุฎุชุตุฑ" ุฃู "ูุถุญ"ุ ูุงุณุชุฎุฏู ุณูุงู ุงููุญุงุฏุซุฉ ุงูุณุงุจู ููุฅุฌุงุจุฉ."""),
#     MessagesPlaceholder(variable_name="history"),
#     ("user", "ุงูุณูุงู:\n{context}\n\nุงูุณุคุงู: {input}"),
# ])

# production_app/core/agent.py

# --- ุงููุงูุจ ุงูููุงุฆู ูุน ุฃูุซูุฉ (Few-Shot Prompting) ---
FINAL_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ุฃูุช ูู ุงููุธุงู ุงูุฐู ูุชู ุงูุณุคุงู ุนูู. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑููุง** ุนูู "ุงูุณูุงู" ุงูููุฏูุ ูุน ุชุจูู ุดุฎุตูุฉ ุงููุธุงู ููุณู.

### ููุงุนุฏ ุตุงุฑูุฉ:
1.  **ุงููููุฉ:** ุชุญุฏุซ ุฏุงุฆููุง ุจุตูุบุฉ ุงููุชููู (ุฃูุงุ ูุฏูุ ูุธุงุฆูู ูู...). ูุง ุชูู ุฃุจุฏูุง "ูุฐุง ุงููุธุงู" ุฃู "ุงููุธุงู ุงููุฐููุฑ".
2.  **ุงูุชูุณูู:** ุงุณุชุฎุฏู ุชูุณูู Markdown (ุนูุงููู ##ุ ููุงุฆู 1., 2., -).
3.  **ุงูุงูุชุฒุงู ุจุงูุณูุงู:** ุฅุฐุง ูุงู ุงูุณูุงู ูุงุฑุบูุง ุฃู ูุง ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ุฃุฌุจ ุจุฅุญุฏู ุงูุฌููุชูู ุงูุชุงููุชูู **ููุท**:
    - "ุฃูุง ูุณุงุนุฏ ุฏุนู ููู ูุชุฎุตุตุ ููุง ูููููู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุนุงูุฉ."
    - "ุจุฎุตูุต ุณุคุงูู '{input}'ุ ูุง ุชูุฌุฏ ูุฏู ูุนูููุงุช ูุงููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุญุงูููุง."

### ูุซุงู ุนูู ุงูุฅุฌุงุจุฉ ุงููุซุงููุฉ:

**ูุซุงู 1:**
---
<|user|>
ุงูุณูุงู:
- ูุซููุฉ ุชุตู ูุธุงู ุชุชุจุน ุงูุทูุจุงุช.
- ุงููุธุงุฆู: ุฅูุดุงุก ุทูุจุ ุชุนุฏูู ุทูุจุ ุญุฐู ุทูุจ.
- ุงููุฏู: ุฒูุงุฏุฉ ููุงุกุฉ ุฎุฏูุฉ ุงูุนููุงุก.

ุงูุณุคุงู: ูุง ูู ูุฐุง ุงููุธุงูุ

<|assistant|>
ุฃูุง ูุธุงู ูุชุฎุตุต ูู ุชุชุจุน ุงูุทูุจุงุช. ูุธุงุฆูู ุงูุฃุณุงุณูุฉ ูู:
1.  **ุฅูุดุงุก ุงูุทูุจุงุช:** ุฃุณูุญ ูููุณุชุฎุฏููู ุจุฅูุดุงุก ุทูุจุงุช ุฌุฏูุฏุฉ.
2.  **ุชุนุฏูู ุงูุทูุจุงุช:** ูููู ูููุณุชุฎุฏููู ุชุนุฏูู ุงูุทูุจุงุช ุงููุงุฆูุฉ.
3.  **ุญุฐู ุงูุทูุจุงุช:** ุฃุชูุญ ุฅููุงููุฉ ุญุฐู ุงูุทูุจุงุช ุบูุฑ ุงูุถุฑูุฑูุฉ.

ูุฏูู ูู ุฒูุงุฏุฉ ููุงุกุฉ ูุฑูู ุฎุฏูุฉ ุงูุนููุงุก.
---
**ูุซุงู 2:**
---
<|user|>
ุงูุณูุงู:
[ูุง ุชูุฌุฏ ูุนูููุงุช ุฐุงุช ุตูุฉ]

ุงูุณุคุงู: ูู ูู ุฃูุถู ูุงุนุจ ูู ุงูุนุงููุ

<|assistant|>
ุฃูุง ูุณุงุนุฏ ุฏุนู ููู ูุชุฎุตุตุ ููุง ูููููู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุนุงูุฉ.
---
"""),
    MessagesPlaceholder(variable_name="history"),
    ("user", "ุงูุขูุ ุงุชุจุน ุงูููุงุนุฏ ูุงูุฃูุซูุฉ ุจุฏูุฉ.\n\nุงูุณูุงู:\n{context}\n\nุงูุณุคุงู: {input}"),
])


class Agent:
    def __init__(self):
        self.llm = None
        self.cross_encoder = None
        self.vector_store = None
        self.retrievers_cache = {}
        self.input_map = {}
        self.response_map = {}
        self.concept_to_inputs_map = {}
        self.chain_with_history = None
        self._ready = False
        self.initialization_lock = asyncio.Lock()

    async def initialize(self):
        async with self.initialization_lock:
            if self._ready:
                return
            
            logging.info("๐ ุจุฏุก ุชููุฆุฉ ุงููููู (ูุถุน ุงูุฅูุชุงุฌ)...")
            try:
                llm_task = asyncio.to_thread(Ollama, model=config.CHAT_MODEL, base_url=config.OLLAMA_HOST, temperature=0.1)
                cross_encoder_task = asyncio.to_thread(CrossEncoder, config.CROSS_ENCODER_MODEL)
                embeddings_task = asyncio.to_thread(HuggingFaceEmbeddings, model_name=config.EMBEDDING_MODEL)
                
                self.llm, self.cross_encoder, embeddings = await asyncio.gather(llm_task, cross_encoder_task, embeddings_task)
                logging.info("โ ุชู ุชููุฆุฉ ููุงุฐุฌ LLM, CrossEncoder, ู Embeddings.")

                self.vector_store = await asyncio.to_thread(
                    FAISS.load_local, config.UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
                )
                logging.info("โ ุชู ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงููุชุฌูุฉ.")

                all_docs = list(self.vector_store.docstore._dict.values())
                tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
                for tenant_id in tenants:
                    tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
                    if not tenant_docs: continue
                    bm25_retriever = BM25Retriever.from_documents(tenant_docs)
                    faiss_retriever = self.vector_store.as_retriever(search_kwargs={'k': config.TOP_K, 'filter': {'tenant_id': tenant_id}})
                    self.retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
                logging.info("โ ุชู ุจูุงุก ุงููุณุชุฑุฌุนุงุช ุงููุฌููุฉ.")

                if os.path.exists(config.HIERARCHICAL_DB_PATH):
                    with open(config.HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
                        db_data = json.load(f)
                        self.input_map = db_data.get("input_map", {})
                        self.response_map = db_data.get("response_map", {})
                    for inp, concept in self.input_map.items():
                        if concept not in self.concept_to_inputs_map: self.concept_to_inputs_map[concept] = []
                        self.concept_to_inputs_map[concept].append(inp)
                    logging.info("โก ุชู ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงููุฑููุฉ.")
                
                base_chain = FINAL_PROMPT | self.llm | StrOutputParser()
                self.chain_with_history = RunnableWithMessageHistory(
                    base_chain,
                    self.get_session_history,
                    input_messages_key="input",
                    history_messages_key="history",
                )

                self._ready = True
                logging.info("โ ุงููููู ุฌุงูุฒ ููุนูู ูู ูุถุน ุงูุฅูุชุงุฌ.")
            except Exception as e:
                logging.critical(f"โ ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
                raise

    def is_ready(self) -> bool:
        return self._ready

    def get_tenants(self) -> List[str]:
        return list(self.retrievers_cache.keys())

    def get_session_history(self, session_id: str) -> ChatMessageHistory:
        if session_id not in config.SESSION_MEMORY:
            config.SESSION_MEMORY[session_id] = ChatMessageHistory()
        return config.SESSION_MEMORY[session_id]

    def _smart_match(self, question: str) -> str | None:
        normalized_question = question.lower().strip()
        if normalized_question in self.input_map:
            return self.input_map[normalized_question]
        for concept_id, inputs in self.concept_to_inputs_map.items():
            for keyword in inputs:
                if len(keyword) >= 3 and keyword in normalized_question:
                    return concept_id
        return None

    async def get_answer_stream(self, request: Dict) -> AsyncGenerator[Dict, None]:
        start_time = time.time()
        request_id = str(uuid.uuid4())
        question = request.get("question", "").strip()
        tenant_id = request.get("tenant_id")
        session_id = request.get("session_id")

        analysis_data = { "request_id": request_id, "session_id": session_id, "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"), "tenant_id": tenant_id, "question": question, "processing_path": "N/A", "total_duration_ms": 0, "steps": {}, "final_answer": "", "error": None }

        def finalize_analysis(data):
            end_time = time.time()
            data["total_duration_ms"] = round((end_time - start_time) * 1000)
            log_entry = json.dumps(data, ensure_ascii=False)
            config.ANALYSIS_LOGGER.info(log_entry)

        try:
            if len(question) < config.MIN_QUESTION_LENGTH:
                analysis_data["processing_path"] = "rejected_short"
                response = "ุนุฐุฑูุงุ ูู ุฃููู ุณุคุงูู. ูู ููููู ุชูุถูุญู ุฃูุซุฑุ"
                analysis_data["final_answer"] = response
                yield {"type": "chunk", "content": response}
                return

            concept_id = self._smart_match(question)
            if concept_id and concept_id in self.response_map:
                analysis_data["processing_path"] = "fast_path"
                response = random.choice(self.response_map[concept_id])
                analysis_data["final_answer"] = response
                yield {"type": "chunk", "content": response}
                return

            analysis_data["processing_path"] = "rag_path"
            retriever = self.retrievers_cache.get(tenant_id)
            if not retriever: raise ValueError(f"ูุง ููุฌุฏ ูุณุชุฑุฌุน ููุนููู '{tenant_id}'.")

            docs = await retriever.ainvoke(question)
            analysis_data["steps"]["retrieval"] = { "retrieved_count_initial": len(docs) }

            if not docs:
                final_docs = []
            else:
                pairs = [[question, doc.page_content] for doc in docs]
                scores = await asyncio.to_thread(self.cross_encoder.predict, pairs)
                
                relevant_docs = []
                for i, doc in enumerate(docs):
                    if scores[i] >= config.RELEVANCE_THRESHOLD:
                        doc.metadata['relevance_score'] = float(scores[i])
                        relevant_docs.append(doc)
                
                relevant_docs.sort(key=lambda x: x.metadata['relevance_score'], reverse=True)
                final_docs = relevant_docs
                analysis_data["steps"]["relevance_check"] = { "scores": [float(s) for s in scores], "relevant_count": len(final_docs) }

            # --- ุงูุชุนุฏูู: ุฅุฑุณุงู ุงููุตุงุฏุฑ ุฅูู ุงููุงุฌูุฉ ุงูุฃูุงููุฉ ---
            if final_docs:
                sources_data = [
                    {
                        "source": doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู"),
                        "content_preview": doc.page_content[:200] + "...",
                        "score": round(doc.metadata.get('relevance_score', 0), 2)
                    }
                    for doc in final_docs
                ]
                yield {"type": "sources", "content": sources_data}
            # ---------------------------------------------------

            full_answer = ""
            chain_input = {"input": question, "context": final_docs}
            chain_config = {"configurable": {"session_id": session_id}}
            
            async for chunk in self.chain_with_history.astream(chain_input, config=chain_config):
                if chunk:
                    full_answer += chunk
                    yield {"type": "chunk", "content": chunk}
            
            analysis_data["final_answer"] = full_answer.strip()

        except Exception as e:
            error_msg = f"ูุดู ูู ุณูุณูุฉ ุงููุนุงูุฌุฉ: {str(e)}"
            logging.error(f"[{session_id}] {error_msg}", exc_info=True)
            analysis_data["error"] = error_msg
            try: yield {"type": "error", "content": "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}
            except Exception: pass
        finally:
            finalize_analysis(analysis_data)

agent_instance = Agent()
