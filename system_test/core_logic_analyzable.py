# system_test/core_logic_analyzable.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø¬Ø¯Ø§Ø± Ø§Ù„Ø¹Ø²Ù„)

import os
import logging
import asyncio
import json
import random
import time
import uuid
from typing import AsyncGenerator, Dict, List

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
# --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ CrossEncoder ---
from sentence_transformers.cross_encoder import CrossEncoder

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Logger Ø§Ù„ØªØ­Ù„ÙŠÙ„ ---
analysis_logger = logging.getLogger('AnalysisLogger')
analysis_logger.setLevel(logging.INFO)
analysis_logger.propagate = False
if not analysis_logger.handlers:
    log_directory = os.path.dirname(__file__)
    log_file_path = os.path.join(log_directory, "analysis_log.jsonl")
    print(f"--- DIAGNOSTIC: Log file will be created at: {log_file_path} ---")
    handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
    formatter = logging.Formatter('%(message)s')
    handler.setFormatter(formatter)
    analysis_logger.addHandler(handler)

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ 2: Ø¥Ø¶Ø§ÙØ© Ù†Ù…ÙˆØ°Ø¬ CrossEncoder ---
CROSS_ENCODER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
HIERARCHICAL_DB_PATH = os.path.join(PROJECT_ROOT, "2_central_api_service", "agent_app", "hierarchical_db.json")

TOP_K = 7
MIN_QUESTION_LENGTH = 3
# --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ 3: Ø¥Ø¶Ø§ÙØ© Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø© ---
RELEVANCE_THRESHOLD = 0.3

# --- 2. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ---
ANSWER_PROMPT = ChatPromptTemplate.from_template(
    """<|system|>
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¯Ø¹Ù… ÙÙ†ÙŠ Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©.
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙØ§Ø±ØºÙ‹Ø§ Ø£Ùˆ ØºÙŠØ± Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…Ù‹Ø§ ÙˆÙ„Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ" Ø£Ùˆ "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§ØŸ")ØŒ ÙÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¥Ø¬Ø§Ø¨ØªÙƒ **ÙÙ‚Ø· Ø¥Ø­Ø¯Ù‰ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ØªÙŠÙ†**:
  1. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù…: "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ø¯Ø¹Ù… ÙÙ†ÙŠ Ù…ØªØ®ØµØµØŒ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©."
  2. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: "Ø¨Ø®ØµÙˆØµ Ø³Ø¤Ø§Ù„Ùƒ '{input}'ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù„Ø¯ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø­Ø§Ù„ÙŠÙ‹Ø§."
- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ù‹Ø§.

<|user|>
Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {input}

<|assistant|>
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
)

# --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
llm: Ollama = None
cross_encoder: CrossEncoder = None # Ø¥Ø¶Ø§ÙØ© CrossEncoder Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
vector_store: FAISS = None
retrievers_cache: Dict[str, EnsembleRetriever] = {}
input_map: Dict[str, str] = {}
response_map: Dict[str, List[str]] = {}
concept_to_inputs_map: Dict[str, List[str]] = {}
initialization_lock = asyncio.Lock()

# --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ---
async def initialize_agent():
    global llm, cross_encoder, vector_store, retrievers_cache, input_map, response_map, concept_to_inputs_map
    async with initialization_lock:
        if llm is not None: return
        logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v-final)...")
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            llm_task = asyncio.to_thread(Ollama, model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            cross_encoder_task = asyncio.to_thread(CrossEncoder, CROSS_ENCODER_MODEL)
            embeddings_task = asyncio.to_thread(HuggingFaceEmbeddings, model_name=EMBEDDING_MODEL_NAME)
            
            llm, cross_encoder, embeddings = await asyncio.gather(llm_task, cross_encoder_task, embeddings_task)
            logging.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ LLM, CrossEncoder, Ùˆ Embeddings Ø¨Ù†Ø¬Ø§Ø­.")

            vector_store = await asyncio.to_thread(
                FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
            )
            logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")

            # ... (Ø¨Ù‚ÙŠØ© ÙƒÙˆØ¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ)
            all_docs = list(vector_store.docstore._dict.values())
            tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
            for tenant_id in tenants:
                tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
                bm25_retriever = BM25Retriever.from_documents(tenant_docs)
                faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
                retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
            logging.info("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©.")

            if os.path.exists(HIERARCHICAL_DB_PATH):
                with open(HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
                    db_data = json.load(f)
                    input_map = db_data.get("input_map", {})
                    response_map = db_data.get("response_map", {})
                for inp, concept in input_map.items():
                    if concept not in concept_to_inputs_map: concept_to_inputs_map[concept] = []
                    concept_to_inputs_map[concept].append(inp)
                logging.info("âš¡ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­.")
            else:
                logging.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")

            logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡ (Ù…Ø¹ Ø¬Ø¯Ø§Ø± Ø§Ù„Ø¹Ø²Ù„).")
        except Exception as e:
            logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            raise

# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
def agent_ready(): return llm is not None
def get_all_tenants_from_cache(): return list(retrievers_cache.keys())
def smart_match(q):
    nq = q.lower().strip()
    if nq in input_map: return input_map[nq]
    for cid, inps in concept_to_inputs_map.items():
        for kw in inps:
            if len(kw) >= 3 and kw in nq: return cid
    return None

# --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…ØµØ­Ø­Ø©) ---
async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    start_time = time.time()
    request_id = str(uuid.uuid4())
    question = request_info.get("question", "").strip()
    tenant_id = request_info.get("tenant_id", "unknown_session")

    analysis_data = { "request_id": request_id, "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"), "tenant_id": tenant_id, "question": question, "processing_path": "N/A", "total_duration_ms": 0, "steps": {}, "final_answer": "", "error": None }

    def finalize_analysis(data):
        end_time = time.time()
        data["total_duration_ms"] = round((end_time - start_time) * 1000)
        log_entry = json.dumps(data, ensure_ascii=False, indent=2)
        analysis_logger.info(log_entry)

    try:
        # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 1: Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ù‘ Ø§Ù„Ø°ÙƒÙŠ ---
        if len(question) < MIN_QUESTION_LENGTH:
            analysis_data["processing_path"] = "rejected_short"
            response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­Ù‡ Ø£ÙƒØ«Ø±ØŸ"
            analysis_data["final_answer"] = response
            yield {"type": "chunk", "content": response}
            return

        # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 2: Ù…Ø­Ø±Ùƒ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠ ---
        concept_id = smart_match(question)
        if concept_id and concept_id in response_map:
            analysis_data["processing_path"] = "fast_path"
            response = random.choice(response_map[concept_id])
            analysis_data["final_answer"] = response
            yield {"type": "chunk", "content": response}
            return

        # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…Ø­Ø±Ùƒ RAG Ø§Ù„Ù…Ø¹Ø±ÙÙŠ ---
        analysis_data["processing_path"] = "rag_path"
        retriever = retrievers_cache.get(tenant_id)
        if not retriever: raise ValueError(f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")

        # --- Ø®Ø·ÙˆØ© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ---
        docs = await retriever.ainvoke(question)
        analysis_data["steps"]["2_retrieval"] = { "retrieved_count_initial": len(docs) }

        # --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ 4: Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙ„Ø© (Ø¬Ø¯Ø§Ø± Ø§Ù„Ø¹Ø²Ù„) ---
        if not docs:
            analysis_data["processing_path"] = "rag_path_no_docs"
            # Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ø¯Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ±Ø¯ Ø¨Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            final_docs = []
        else:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø²ÙˆØ§Ø¬ [Ø³Ø¤Ø§Ù„, Ù…Ø­ØªÙˆÙ‰ Ù…Ø³ØªÙ†Ø¯] Ù„Ù„ØªØ­Ù‚Ù‚
            pairs = [[question, doc.page_content] for doc in docs]
            scores = await asyncio.to_thread(cross_encoder.predict, pairs)
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØªØµÙÙŠØªÙ‡Ø§
            relevant_docs = []
            for i, doc in enumerate(docs):
                doc.metadata['relevance_score'] = float(scores[i])
                if scores[i] >= RELEVANCE_THRESHOLD:
                    relevant_docs.append(doc)
            
            # ÙØ±Ø² Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø©
            relevant_docs.sort(key=lambda x: x.metadata['relevance_score'], reverse=True)
            final_docs = relevant_docs

            analysis_data["steps"]["3_relevance_check"] = {
                "scores": [float(s) for s in scores],
                "threshold": RELEVANCE_THRESHOLD,
                "relevant_count": len(final_docs)
            }

        # --- Ø®Ø·ÙˆØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ---
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙ…Ø± Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„ØªØ­Ù‚Ù‚ØŒ Ø³ÙŠÙƒÙˆÙ† final_docs ÙØ§Ø±ØºÙ‹Ø§
        # ÙˆØ³ÙŠØ¹ØªÙ…Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù„Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
        full_answer = ""
        async for chunk in answer_chain.astream({"input": question, "context": final_docs}):
            if chunk:
                full_answer += chunk
                yield {"type": "chunk", "content": chunk}
        
        analysis_data["final_answer"] = full_answer.strip()

    except Exception as e:
        error_msg = f"ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"
        logging.error(f"[{tenant_id}][{request_id}] {error_msg}", exc_info=True)
        analysis_data["error"] = error_msg
        try: yield {"type": "error", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
        except Exception: pass
    finally:
        finalize_analysis(analysis_data)
