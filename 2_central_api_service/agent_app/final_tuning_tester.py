# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/final_tuning_tester.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 13.0: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø´Ø®ØµÙŠØ©ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø­ÙˆØ§Ø±ÙŠØŒ ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ---

import os
import logging
import asyncio
from typing import List, Dict

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from flashrank import Ranker, RerankRequest

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# (Ù†ÙØ³ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ---
SYSTEM_PROFILES = {
    "sys": {
        "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
        "description": "Ù†Ø¸Ø§Ù… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„ØªØªØ¨Ø¹ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯.",
        "keywords": ["Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©", "ÙØ§ØªÙˆØ±Ø©", "Ø´Ù‡Ø§Ø¯Ø©"]
    },
    "university_alpha": {
        "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care",
        "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©.",
        "keywords": ["ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¨Ø§Øª", "Ø¢ÙØ§Øª Ø²Ø±Ø§Ø¹ÙŠØ©", "Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "plant care"]
    },
    "school_beta": {
        "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
        "description": "Ù…Ø§Ø¯Ø© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ùˆ TensorFlow.",
        "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "cnn", "layer", "relu", "pooling", "optimizer"]
    },
    "un": {
        "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©",
        "description": "Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.",
        "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing", "ungm.org", "Ù…ÙˆØ±Ø¯ÙŠÙ†"]
    }
}

# --- 3. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 13.0) ---

# Ù‚Ø§Ù„Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© (ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ù† Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0 Ù„Ø£Ù†Ù‡ Ø£Ø«Ø¨Øª Ù†Ø¬Ø§Ø­Ù‡)
REWRITE_PROMPT_TEMPLATE = """
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© Ù…Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«.

**Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…:** {system_name}
**Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©:** {system_keywords}

---
**Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:**
1.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…Ù‹Ø§ Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…** (Ù…Ø«Ù„ "Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙ‚Ø·: `{system_name}`.
2.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø®Ø·ÙˆØ§Øª Ø£Ùˆ ÙƒÙŠÙÙŠØ© ÙØ¹Ù„ Ø´ÙŠØ¡** (Ù…Ø«Ù„ "ÙƒÙŠÙ Ø£Ø¶ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„ÙØ¹Ù„ ÙˆØ§Ù„Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡: `Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯`.
3.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† ØªØ¹Ø±ÙŠÙ Ù…ØµØ·Ù„Ø­** (Ù…Ø«Ù„ "Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù…ØµØ·Ù„Ø­ Ù†ÙØ³Ù‡: `Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©`.
4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§** (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ.
5.  **Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚ØµÙŠØ±Ù‹Ø§ Ø¬Ø¯Ù‹Ø§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ù‹Ø§.** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¬Ù…Ù„ ÙƒØ§Ù…Ù„Ø©.

---
**Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:
"""

# --- Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø­ÙˆØ§Ø±ÙŠ ---
FINAL_ANSWER_PROMPT = ChatPromptTemplate.from_template("""
**Ø´Ø®ØµÙŠØªÙƒ:** Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ Ù„Ù€ OpenSoft"ØŒ Ø®Ø¨ÙŠØ± ÙˆÙ…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠ: {system_name}.

**Ù…Ù‡Ù…ØªÙƒ:** Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….

**Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
1.  Ø§Ø¨Ø¯Ø£ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù€ "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯! Ø¨Ø®ØµÙˆØµ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù†..." Ø£Ùˆ ØµÙŠØºØ© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ø´Ø§Ø¨Ù‡Ø©.
2.  Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù… ÙˆÙ…ÙØµÙ„ ÙÙŠ Ù†Ù‚Ø§Ø·.
3.  Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ„ **ÙÙ‚Ø·**: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø¸Ø§Ù… '{system_name}'ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„." Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø§Øª.
4.  Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø§Ø®ØªØªÙ… Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø³Ø¤Ø§Ù„ ØªÙØ§Ø¹Ù„ÙŠ Ù…Ø«Ù„: "Ù‡Ù„ ØªÙˆØ¯ Ø´Ø±Ø­Ù‹Ø§ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ù†Ù‚Ø·Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ" Ø£Ùˆ "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ù‡ØŸ".
5.  Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø¨Ø¯Ù‹Ø§ ÙƒÙ„Ù…Ø© "Ø³ÙŠØ§Ù‚" Ø£Ùˆ "Ù…Ø³ØªÙ†Ø¯Ø§Øª" Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….

---
**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„ÙŠÙ‡:**
{context}

---
**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {input}

**Ø¥Ø¬Ø§Ø¨ØªÙƒ:**
""")


# --- 4. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ---
def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
    return list(vs.docstore._dict.values())

def _clean_rewritten_query(raw_query: str) -> str:
    # (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
    lines = raw_query.strip().split('\n')
    for line in reversed(lines):
        cleaned_line = line.strip()
        if cleaned_line:
            if cleaned_line.startswith("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"):
                return cleaned_line.replace("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:", "").strip()
            return cleaned_line
    return raw_query.strip()

def print_results(docs: List[Document], title: str):
    print(f"\n--- ğŸ“„ {title} (Ø¹Ø¯Ø¯: {len(docs)}) ---")
    if not docs:
        print("   -> Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
        return
    for i, doc in enumerate(docs):
        content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:100]
        print(f"   {i+1}. [Ù…ØµØ¯Ø±: {doc.metadata.get('source', 'N/A')}] -> \"{content_preview}...\"")

# --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
async def run_full_test_pipeline(question: str, tenant_id: str, llm: Ollama, vector_store: FAISS, reranker: Ranker):
    print("\n" + "="*80)
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± ÙƒØ§Ù…Ù„ Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}' | Ù„Ù„Ø¹Ù…ÙŠÙ„: '{tenant_id}'")
    print("="*80)

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 0: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ ---
    profile = SYSTEM_PROFILES.get(tenant_id)
    if not profile:
        print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ.")
        effective_question = question
    else:
        print(f"âœ… [1/5] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø´Ø®ØµÙŠ: '{profile['name']}'")
        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ---
        print("ğŸ§  [2/5] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ...")
        rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
        rewriter_chain = rewrite_prompt | llm | StrOutputParser()
        raw_rewritten_query = await rewriter_chain.ainvoke({
            "system_name": profile.get("name", ""),
            "system_keywords": ", ".join(profile.get("keywords", [])),
            "question": question
        })
        effective_question = _clean_rewritten_query(raw_rewritten_query)
        print(f"   -> Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}'")
        print(f"   -> Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø¨Ø­Ø«: '{effective_question}'")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† ---
    print("ğŸ” [3/5] Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† (BM25 + FAISS)...")
    all_docs = _load_all_docs_from_faiss(vector_store)
    tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
    
    if not tenant_docs:
        print(f"âŒ Ø®Ø·Ø£: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return

    bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=10)
    faiss_retriever = vector_store.as_retriever(search_kwargs={'k': 10, 'filter': {'tenant_id': tenant_id}})
    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
    
    initial_docs = await ensemble_retriever.ainvoke(effective_question)
    print_results(initial_docs, "Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Reranking) ---
    print("âœ¨ [4/5] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashRank...")
    passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
    rerank_request = RerankRequest(query=question, passages=passages)
    all_reranked_results = reranker.rerank(rerank_request)
    top_4_results = all_reranked_results[:4]
    
    original_docs_map = {doc.page_content: doc for doc in initial_docs}
    reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
    print_results(reranked_docs, "Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Top 4)")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---
    print("âœï¸ [5/5] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØªØ±ØªÙŠØ¨Ù‡Ø§...")
    answer_chain = FINAL_ANSWER_PROMPT | llm | StrOutputParser()
    
    final_context = "\n\n---\n\n".join([doc.page_content for doc in reranked_docs])
    
    final_answer = await answer_chain.ainvoke({
        "system_name": profile.get("name", "Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…"),
        "context": final_context,
        "input": question
    })

    print("\n" + "-"*30 + " ğŸ’¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸ’¬ " + "-"*30)
    print(final_answer)
    print("="*80)


async def main():
    print("--- ğŸ”¬ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸ”¬ ---")
    try:
        llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        vector_store = FAISS.load_local(UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        reranker = Ranker()
        print("--- âœ… Ø¨ÙŠØ¦Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¬Ø§Ù‡Ø²Ø© ---")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}")
        return

    # --- Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯Ù‡Ø§ ---
    
    # Ø§Ø®ØªØ¨Ø§Ø± 1: Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù… Ø¹Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯
    await run_full_test_pipeline("Ù…Ø§Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ…Ù† ÙŠØªØ¨Ø¹Ù‡", "sys", llm, vector_store, reranker)
    
    # Ø§Ø®ØªØ¨Ø§Ø± 2: Ø³Ø¤Ø§Ù„ ÙÙ†ÙŠ Ù…Ø­Ø¯Ø¯ Ø¹Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©
    await run_full_test_pipeline("Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠÙ‡", "school_beta", llm, vector_store, reranker)

    # Ø§Ø®ØªØ¨Ø§Ø± 3: Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø®Ø·ÙˆØ§Øª ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©
    await run_full_test_pipeline("ÙƒÙŠÙ Ù†Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù…", "un", llm, vector_store, reranker)

    # Ø§Ø®ØªØ¨Ø§Ø± 4: Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§
    await run_full_test_pipeline("Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§", "sys", llm, vector_store, reranker)
    
    # Ø§Ø®ØªØ¨Ø§Ø± 5: Ø³Ø¤Ø§Ù„ "Ù…Ù† Ø£Ù†ØªØŸ"
    await run_full_test_pipeline("Ù…Ù† Ø§Ù†Øª", "university_alpha", llm, vector_store, reranker)


if __name__ == "__main__":
    asyncio.run(main())
