# 2_central_api_service/agent_app/core_logic.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)

import os
import logging
from typing import List, Dict, Any, AsyncGenerator
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from dotenv import load_dotenv
import langchain
from langchain.cache import InMemoryCache

# --- ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© (Cache) ---
logging.info("ğŸš€ ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© (InMemoryCache) Ù„Ù€ LangChain...")
langchain.llm_cache = InMemoryCache()

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))

# --- Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ---
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# --- Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† ---
RAG_PROMPT_TEMPLATE = """
**Ù…Ù‡Ù…ØªÙƒ:** Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¯Ø¹Ù… ÙÙ†ÙŠ Ø®Ø¨ÙŠØ± ÙˆÙ…Ø®ØªØµ. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ "Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" Ø¨Ø¯Ù‚Ø© ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©.
- Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©.
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø£Ø¬Ø¨ Ø¨Ù€ "Ø£Ù†Ø§ Ø¢Ø³ÙØŒ Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„." ÙˆÙ„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø®ØªÙ„Ø§Ù‚ Ø¥Ø¬Ø§Ø¨Ø©.
- Ø£Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

**Ø§Ù„Ø³ÙŠØ§Ù‚:**
{context}

**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**
{question}

**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
"""

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
vector_store = None
llm = None
prompt = None

def initialize_agent():
    """ ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬. ØªÙØ³ØªØ¯Ø¹Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ API. """
    global vector_store, llm, prompt
    
    if vector_store:
        logging.info("Ø§Ù„ÙˆÙƒÙŠÙ„ Ù…ÙÙ‡ÙŠØ£ Ø¨Ø§Ù„ÙØ¹Ù„.")
        return

    try:
        logging.info("="*50)
        logging.info("ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ...")
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        logging.info(f"ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {EMBEDDING_MODEL_NAME}...")
        embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)

        # 2. ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª FAISS
        logging.info(f"ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù†: {VECTOR_DB_PATH}...")
        if not os.path.exists(os.path.join(VECTOR_DB_PATH, "index.faiss")):
            raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (index.faiss) ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {VECTOR_DB_PATH}. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹.")
        
        vector_store = FAISS.load_local(
            VECTOR_DB_PATH,
            embeddings=embeddings_model,
            allow_dangerous_deserialization=True
        )
        logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­.")

        # 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„ÙƒØ¨ÙŠØ± Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        logging.info(f"ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {CHAT_MODEL_NAME}...")
        llm = Ollama(
            model=CHAT_MODEL_NAME,
            temperature=0.1,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£ÙƒØ«Ø± Ø§ØªØ³Ø§Ù‚Ù‹Ø§
            # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‡Ù†Ø§ Ù…Ø«Ù„ top_p, top_k
        )

        # 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        
        logging.info("âœ… Ø§ÙƒØªÙ…Ù„Øª ØªÙ‡ÙŠØ¦Ø© ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ Ø¨Ù†Ø¬Ø§Ø­!")
        logging.info("="*50)
    except FileNotFoundError as e:
        logging.critical(f"âŒ ÙØ´Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. {e}", exc_info=True)
        raise
    except Exception as e:
        logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ ÙˆØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆÙƒÙŠÙ„: {e}", exc_info=True)
        raise

def format_docs_with_source(docs: List[Dict[str, Any]]) -> str:
    """ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø­Ø³Ù†Ø©: ØªÙ†Ø³Ù‚ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹ Ø°ÙƒØ± Ù…ØµØ¯Ø±Ù‡Ø§. """
    if not docs:
        return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ Ù…ØªÙˆÙØ±."
    
    sources = {doc.metadata.get('source', 'Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ') for doc in docs}
    formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
    return f"Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±: {', '.join(sources)}\n\n{formatted_docs}"

async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
    """
    ØªØ³ØªÙ‚Ø¨Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ ÙˆÙ‡ÙˆÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„ØŒ ÙˆØªØ³ØªØ®Ø¯Ù… Ø³Ù„Ø³Ù„Ø© RAG Ù„Ø¨Ø« Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ØªÙØ§Ø¹Ù„ÙŠ.
    """
    if not vector_store or not llm or not prompt:
        raise RuntimeError("Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ù…ÙÙ‡ÙŠØ£. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ initialize_agent() Ø£ÙˆÙ„Ø§Ù‹.")
    
    logging.info(f"Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø·Ù„Ø¨ Ø¨Ø« Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}' (k={k_results}): '{question}'")
    
    try:
        # --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³ØªØ±Ø¯ (Retriever) Ù…Ø¹ ÙÙ„ØªØ±Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© ---
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
        )
        
        # --- Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© RAG ---
        rag_chain = (
            RunnablePassthrough.assign(
                context=lambda x: retriever.get_relevant_documents(x["question"])
            )
            | RunnablePassthrough.assign(
                context=lambda x: format_docs_with_source(x["context"])
            )
            | prompt
            | llm
        )

        logging.info(f"Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ø¶Ù…Ù† Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'...")
        
        # --- Ø§Ù„Ø¨Ø« Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ (Streaming) ---
        async for chunk in rag_chain.astream({"question": question}):
            yield chunk
            
    except Exception as e:
        logging.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¨Ø« Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}': {e}", exc_info=True)
        yield "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ."

