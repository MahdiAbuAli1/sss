# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- ูุฐุง ูู ุงููุณู ุงูุฐู ูุฌุจ ุชุนุฏููู ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # ุงูุงุณุชูุฑุงุฏ ุงูุตุญูุญ ููุฅุตุฏุงุฑุงุช ุงูุญุฏูุซุฉ ูู LangChain
# # ูุชู ุงุณุชูุฑุงุฏ ูู ูุธููุฉ ูู ูุณุงุฑูุง ุงููุงูู ูุงูุฏููู ุฏุงุฎู ุงูุญุฒูุฉ

# try:
#     from langchain.chains import create_history_aware_retriever
#     from langchain.chains.combine_documents import create_stuff_documents_chain
#     from langchain.chains import create_retrieval_chain
# except ImportError:
#     try:
#         from langchain.chains.history_aware_retriever import create_history_aware_retriever
#         from langchain.chains.combine_documents import create_stuff_documents_chain
#         from langchain.chains.retrieval import create_retrieval_chain
#     except ImportError:
#         # ููุฅุตุฏุงุฑุงุช ุงููุฏููุฉ ุฌุฏุงู
#         from langchain.chains import (
#             create_history_aware_retriever,
#             create_stuff_documents_chain,
#             create_retrieval_chain
#         )


# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- ููุงูุฉ ุงููุณู ุงูุฐู ูุฌุจ ุชุนุฏููู ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# # ุงุณุชูุฑุงุฏ ูุณุฌู ุงูุฃุฏุงุก
# from .performance_tracker import PerformanceLogger

# # --- 1. ุงูุฅุนุฏุงุฏุงุช ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- ูุชุบูุฑุงุช ุนุงูููุฉ ---
# llm: Ollama = None
# vector_store: FAISS = None
# embeddings: OllamaEmbeddings = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# # --- ุฅูุดุงุก ูุณุฎุฉ ูู ูุณุฌู ุงูุฃุฏุงุก ---
# perf_logger = PerformanceLogger()

# # --- 2. ุงูููุงูุจ (ูุง ุชุบููุฑ ููุง) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# ุจุงููุธุฑ ุฅูู ุณุฌู ุงููุญุงุฏุซุฉ ูุงูุณุคุงู ุงูุฃุฎูุฑุ ูู ุจุตูุงุบุฉ ุณุคุงู ูุณุชูู ูููู ูููู ุจุฏูู ุณุฌู ุงููุญุงุฏุซุฉ.
# ุณุฌู ุงููุญุงุฏุซุฉ: {chat_history}
# ุงูุณุคุงู ุงูุฃุฎูุฑ: {input}
# ุงูุณุคุงู ุงููุณุชูู:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# ุฃูุช "ูุฑุดุฏ ุงูุฏุนู"ุ ูุณุงุนุฏ ุฐูู ูุฎุจูุฑ. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑูุงู** ุนูู "ุงูุณูุงู" ุงูููุฏู.
# - ูู ุฏุงุฆูุงู ูุชุนุงููุงู ููุญุชุฑูุงู.
# - ุฅุฐุง ูุงู ุงูุณูุงู ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ูุฏููุง ุจุดูู ูุจุงุดุฑ ูููุธู.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุจุดูู ูุงุถุญ ูู ุงูุณูุงูุ ูู ุจุฃุณููุจ ูุทูู: "ุจุญุซุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ูููู ูู ุฃุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ุจุฎุตูุต ูุฐุง ุงูุณุคุงู."
# - ูุง ุชุฎุชุฑุน ุฅุฌุงุจุงุช ุฃุจุฏุงู. ุงูุชุฒู ุจุงูุณูุงู.

# ุงูุณูุงู:
# {context}

# ุงูุณุคุงู: {input}
# ุงูุฅุฌุงุจุฉ:""")

# # --- 3. ุงูุฏูุงู ุงูุฃุณุงุณูุฉ (ูุง ุชุบููุฑ ููุง) ---
# async def initialize_agent():
#     global llm, embeddings, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("ุจุฏุก ุชููุฆุฉ ุงูููุงุฐุฌ ููุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุบูุฑ ููุฌูุฏุฉ. ูุฑุฌู ุชุดุบูู ุณูุฑุช 'main_builder.py' ุฃููุงู.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("โ ุงููููู ุฌุงูุฒ ููุนูู ุจูุงุนุฏุฉ ุจูุงูุงุช ููุญุฏุฉ.")
#         except Exception as e:
#             logging.error(f"ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#             raise

# # --- 4. ุฏุงูุฉ ููุชุญูู ูู ุฌุงูุฒูุฉ ุงููููู ---
# def agent_ready() -> bool:
#     """ุงูุชุญูู ูู ุฃู ุงููููู ุฌุงูุฒ ููุนูู"""
#     return vector_store is not None and llm is not None

# # --- 5. ุฏุงูุฉ get_answer_stream ูุน ุชุณุฌูู ุงูุฃุฏุงุก ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     """ุฏุงูุฉ ุฑุฆูุณูุฉ ูุชูููุฏ ุงูุฅุฌุงุจุงุช ุจุดูู ูุชุฏูู"""
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 4)
    
#     session_id = tenant_id or "default_session"

#     if not vector_store:
#         yield {"type": "error", "content": "ุงููููู ุบูุฑ ุฌุงูุฒ. ูุฑุฌู ุฅุนุงุฏุฉ ุชุญููู ุงูุตูุญุฉ."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"k_results": k_results})

#     retriever = vector_store.as_retriever(
#         search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#     )
    
#     user_chat_history = chat_history.get(session_id, [])

#     # --- ุจูุงุก ุงูุณูุงุณู ---
#     history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] ุจุฏุก ูุนุงูุฌุฉ ุงูุณุคุงู '{question}'...")
#     try:
#         full_answer = ""
#         # ุจุฏุก ุชุณุฌูู ููุช ุชุฏูู ุงูุฅุฌุงุจุฉ
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         # ุฅููุงุก ุชุณุฌูู ููุช ุชุฏูู ุงูุฅุฌุงุจุฉ
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         # ุชุญุฏูุซ ุณุฌู ุงููุญุงุฏุซุฉ
#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:] # ุงูุงุญุชูุงุธ ุจุขุฎุฑ 10 ุฑุณุงุฆู
#         logging.info(f"[{session_id}] ุงูุฅุฌุงุจุฉ ุงููุงููุฉ: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ูุดู ูู ุณูุณูุฉ RAG. ุงูุฎุทุฃ: {e}", exc_info=True)
#         yield {"type": "error", "content": "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}
#     finally:
#         # ุชุณุฌูู ุฅุฌูุงูู ููุช ุงูุทูุจ ูู ูู ุงูุญุงูุงุช (ูุฌุงุญ ุฃู ูุดู)
#         perf_logger.end("total_request", tenant_id, question)
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ ุงูููุงุฆูุฉ ุงููุฏูุฌุฉ ูุน ุงูุจุญุซ ุงููุฌูู ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ ุงูููุงุฆูุฉ ุงููุตุญุญุฉ ููุดููุฉ ุงูุงุณุชูุฑุงุฏ ---

# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List, cast

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.documents import Document

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- ูุฐุง ูู ุงููุณู ุงูุฐู ุชู ุชุนุฏููู ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # 1. ุฅุถุงูุฉ ุงุณุชูุฑุงุฏุงุช ุฌุฏูุฏุฉ ููุจุญุซ ุงููุฌูู
# from langchain.retrievers import BM25Retriever, EnsembleRetriever

# # 2. ุงุณุชุฎุฏุงู ุงููุณุงุฑุงุช ุงูุตุญูุญุฉ ูุงูุญุฏูุซุฉ ููุธุงุฆู ุงูุณูุงุณู
# # ูุฐุง ูุญู ูุดููุฉ ImportError
# from langchain.chains.history_aware_retriever import create_history_aware_retriever
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain.chains.retrieval import create_retrieval_chain

# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- ููุงูุฉ ุงููุณู ุงูุฐู ุชู ุชุนุฏููู ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# from .performance_tracker import PerformanceLogger

# # --- 1. ุงูุฅุนุฏุงุฏุงุช (ูุง ุชุบููุฑ ููุง ) ---
# # ... (ุจููุฉ ุงูููุฏ ูุจูู ููุง ูู ุฏูู ุฃู ุชุบููุฑ) ...
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- ูุชุบูุฑุงุช ุนุงูููุฉ ---
# llm: Ollama = None
# ensemble_retriever: EnsembleRetriever = None 
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 2. ุงูููุงูุจ (ูุง ุชุบููุฑ ููุง) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# ุจุงููุธุฑ ุฅูู ุณุฌู ุงููุญุงุฏุซุฉ ูุงูุณุคุงู ุงูุฃุฎูุฑุ ูู ุจุตูุงุบุฉ ุณุคุงู ูุณุชูู ูููู ูููู ุจุฏูู ุณุฌู ุงููุญุงุฏุซุฉ.
# ุณุฌู ุงููุญุงุฏุซุฉ: {chat_history}
# ุงูุณุคุงู ุงูุฃุฎูุฑ: {input}
# ุงูุณุคุงู ุงููุณุชูู:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# ุฃูุช "ูุฑุดุฏ ุงูุฏุนู"ุ ูุณุงุนุฏ ุฐูู ูุฎุจูุฑ. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑูุงู** ุนูู "ุงูุณูุงู" ุงูููุฏู.
# - ูู ุฏุงุฆูุงู ูุชุนุงููุงู ููุญุชุฑูุงู.
# - ุฅุฐุง ูุงู ุงูุณูุงู ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ูุฏููุง ุจุดูู ูุจุงุดุฑ ูููุธู.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุจุดูู ูุงุถุญ ูู ุงูุณูุงูุ ูู ุจุฃุณููุจ ูุทูู: "ุจุญุซุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ูููู ูู ุฃุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ุจุฎุตูุต ูุฐุง ุงูุณุคุงู."
# - ูุง ุชุฎุชุฑุน ุฅุฌุงุจุงุช ุฃุจุฏุงู. ุงูุชุฒู ุจุงูุณูุงู.

# ุงูุณูุงู:
# {context}

# ุงูุณุคุงู: {input}
# ุงูุฅุฌุงุจุฉ:""")

# # --- 3. ุงูุฏูุงู ุงูุฃุณุงุณูุฉ (ูุง ุชุบููุฑ ููุง) ---
# def _load_all_docs_from_faiss(vector_store: FAISS) -> List[Document]:
#     return list(cast(dict, vector_store.docstore._dict).values())

# async def initialize_agent():
#     global llm, ensemble_retriever
#     async with initialization_lock:
#         if ensemble_retriever is not None: return
#         logging.info("ุจุฏุก ุชููุฆุฉ ุงูููุงุฐุฌ ูุงููุณุชุฑุฌุน ุงููุฌูู...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            
#             logging.info("ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช FAISS...")
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุบูุฑ ููุฌูุฏุฉ. ูุฑุฌู ุชุดุบูู 'main_builder.py' ุฃููุงู.")
            
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
#             faiss_vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={'k': 4})
#             logging.info("โ ุชู ุชุญููู ุงููุณุชุฑุฌุน ุงูุฏูุงูู (FAISS).")

#             logging.info("ุจูุงุก ูุณุชุฑุฌุน ุงููููุงุช ุงูููุชุงุญูุฉ (BM25)...")
#             all_docs = await asyncio.to_thread(_load_all_docs_from_faiss, faiss_vector_store)
#             bm25_retriever = BM25Retriever.from_documents(all_docs)
#             bm25_retriever.k = 4
#             logging.info("โ ุชู ุจูุงุก ุงููุณุชุฑุฌุน (BM25).")

#             ensemble_retriever = EnsembleRetriever(
#                 retrievers=[bm25_retriever, faiss_retriever],
#                 weights=[0.5, 0.5]
#             )
#             logging.info("๐ ุงููููู ุฌุงูุฒ ููุนูู ูุน ุงููุณุชุฑุฌุน ุงููุฌูู.")

#         except Exception as e:
#             logging.error(f"ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#             raise

# # --- 4. ุฏุงูุฉ ููุชุญูู ูู ุฌุงูุฒูุฉ ุงููููู (ูุง ุชุบููุฑ ููุง) ---
# def agent_ready() -> bool:
#     return ensemble_retriever is not None and llm is not None

# # --- 5. ุฏุงูุฉ get_answer_stream (ูุง ุชุบููุฑ ููุง) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
    
#     session_id = tenant_id or "default_session"

#     if not ensemble_retriever:
#         yield {"type": "error", "content": "ุงููููู ุบูุฑ ุฌุงูุฒ. ูุฑุฌู ุฅุนุงุฏุฉ ุชุญููู ุงูุตูุญุฉ."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"retriever_type": "hybrid"})
    
#     user_chat_history = chat_history.get(session_id, [])

#     history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] ุจุฏุก ูุนุงูุฌุฉ ุงูุณุคุงู '{question}'...")
#     try:
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] ุงูุฅุฌุงุจุฉ ุงููุงููุฉ: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ูุดู ูู ุณูุณูุฉ RAG. ุงูุฎุทุฃ: {e}", exc_info=True)
#         yield {"type": "error", "content": "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)


# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ุณูุณูุฉ RAG ุงููุชูุฏูุฉ (ุฅุนุงุฏุฉ ุตูุงุบุฉ + ุจุญุซ ูุฌูู) ---
#ุงูุงุตุฏุงุฑ ุงูุซุงูู
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_history_aware_retriever, create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from .performance_tracker import PerformanceLogger

# # --- 1. ุงูุฅุนุฏุงุฏุงุช ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. ุงููููุงุช ุงูุดุฎุตูุฉ ููุฃูุธูุฉ (ููุฑุชู ุงูุฑุงุฆุนุฉ!) ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ",
#         "description": "ูุธุงู ูุชุชุจุน ูุฑุงุญู ุงูุญุตูู ุนูู ุงูุงุนุชูุงุฏ ูู ุงูุชูุฏูู ุญุชู ุฅุตุฏุงุฑ ุงูุดูุงุฏุฉ.",
#         "keywords": ["ุทูุจ ุงุนุชูุงุฏ", "ููุงุฆู ุงูุชุญูู", "ุฏุฑุงุณุฉ ููุชุจูุฉ", "ุฒูุงุฑุฉ ููุฏุงููุฉ", "ุฅุฌุฑุงุกุงุช ุชุตุญูุญูุฉ"]
#     },
#     "university_alpha": {
#         "name": "ุชุทุจูู Plant Care",
#         "description": "ุชุทุจูู ุฐูู ููุณุงุนุฏุฉ ุงููุฒุงุฑุนูู ูู ุงูุชุนุฑู ุนูู ุงูุขูุงุช ุงูุฒุฑุงุนูุฉ.",
#         "keywords": ["ูุชุทูุจุงุช ูุธูููุฉ", "ุญุงูุงุช ุงุณุชุฎุฏุงู", "ุชุตููู ุงููุธุงู", "ูุฎุทุท ุนูุงูุงุช", "plant care"]
#     },
#     "school_beta": {
#         "name": "ูุณุชูุฏุงุช ุงูุดุจูุงุช ุงูุนุตุจูุฉ",
#         "description": "ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุงูุชุนููููุฉ ุญูู ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุงูุชูุงููููุฉ (CNN) ูููุชุจุฉ TensorFlow.",
#         "keywords": ["ุดุจูุฉ ุนุตุจูุฉ", "tensorflow", "convolutional layer", "relu", "pooling"]
#     },
#     "un": {
#         "name": "ุจูุงุจุฉ ุงููุดุชุฑูุงุช ุงูุฅููุชุฑูููุฉ ููุฃูู ุงููุชุญุฏุฉ",
#         "description": "ุฏููู ุงุณุชุฎุฏุงู ูุธุงู ุงูุดุฑุงุก ุงูุฅููุชุฑููู ุงูุฎุงุต ุจููุชุจ ุงูุฃูู ุงููุชุญุฏุฉ ูุฎุฏูุงุช ุงููุดุงุฑูุน (UNOPS).",
#         "keywords": ["ููุงูุตุงุช", "ุชุณุฌูู ุงูุฏุฎูู", "ุนุทุงุกุงุช", "unops", "esourcing"]
#     }
# }

# # --- 3. ุงูููุงูุจ ุงููุชูุฏูุฉ ---
# REWRITE_PROMPT_TEMPLATE = """
# ุฃูุช ุฎุจูุฑ ูู ุงููุธุงู ุงูุชุงูู:
# - ุงุณู ุงููุธุงู: {system_name}
# - ูุตูู: {system_description}
# - ูุตุทูุญุงุช ูุงูุฉ: {system_keywords}

# ูููุชู ูู ุชุญููู ุณุคุงู ุงููุณุชุฎุฏู ุงูุนุงู ุฅูู ุงุณุชุนูุงู ุจุญุซ ุฏููู ููุญุฏุฏ ูุงุณุชุฎุฏุงูู ูู ูุงุนุฏุฉ ุจูุงูุงุช ุชูููุฉ. ุงุณุชุฎุฏู ุงููุตุทูุญุงุช ุงููุงูุฉ ูุฅูุดุงุก ุฃูุถู ุงุณุชุนูุงู ูููู.

# ุณุคุงู ุงููุณุชุฎุฏู: {question}

# ุงูุงุณุชุนูุงู ุงููุญุณูู:"""

# REPHRASE_HISTORY_PROMPT = ChatPromptTemplate.from_template("""
# ุจุงููุธุฑ ุฅูู ุณุฌู ุงููุญุงุฏุซุฉ ูุงูุณุคุงู ุงูุฃุฎูุฑุ ูู ุจุตูุงุบุฉ ุณุคุงู ูุณุชูู ูููู ูููู ุจุฏูู ุณุฌู ุงููุญุงุฏุซุฉ.
# ุณุฌู ุงููุญุงุฏุซุฉ: {chat_history}
# ุงูุณุคุงู ุงูุฃุฎูุฑ: {input}
# ุงูุณุคุงู ุงููุณุชูู:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# ุฃูุช "ูุฑุดุฏ ุงูุฏุนู"ุ ูุณุงุนุฏ ุฐูู ูุฎุจูุฑ. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑูุงู** ุนูู "ุงูุณูุงู" ุงูููุฏู.
# - ูู ุฏุงุฆูุงู ูุชุนุงููุงู ููุญุชุฑูุงู.
# - ุฅุฐุง ูุงู ุงูุณูุงู ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ูุฏููุง ุจุดูู ูุจุงุดุฑ ูููุธู.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุจุดูู ูุงุถุญ ูู ุงูุณูุงูุ ูู ุจุฃุณููุจ ูุทูู: "ุจุญุซุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ูููู ูู ุฃุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ุจุฎุตูุต ูุฐุง ุงูุณุคุงู."
# - ูุง ุชุฎุชุฑุน ุฅุฌุงุจุงุช ุฃุจุฏุงู. ุงูุชุฒู ุจุงูุณูุงู.

# ุงูุณูุงู:
# {context}

# ุงูุณุคุงู: {input}
# ุงูุฅุฌุงุจุฉ:""")

# # --- 4. ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ---
# llm: Ollama = None
# vector_store: FAISS = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. ุงูุฏูุงู ุงูุฃุณุงุณูุฉ ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# async def initialize_agent():
#     global llm, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("ุจุฏุก ุชููุฆุฉ ุงูููุงุฐุฌ ููุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุบูุฑ ููุฌูุฏุฉ.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("โ ุงููููู ุฌุงูุฒ ููุนูู ุจูุงุนุฏุฉ ุจูุงูุงุช ููุญุฏุฉ.")
#         except Exception as e:
#             logging.error(f"ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 8)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "ุงููููู ุบูุฑ ุฌุงูุฒ. ูุฑุฌู ุฅุนุงุฏุฉ ุชุญููู ุงูุตูุญุฉ."}
#         return

#     perf_logger.start("total_request", tenant_id, question)
#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         # --- ุงููุฑุญูุฉ 1: ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู ุจูุงุกู ุนูู ุงูุณูุงู (ููุฑุชู!) ---
#         profile = SYSTEM_PROFILES.get(tenant_id, {})
#         if profile:
#             logging.info(f"[{session_id}] ุงุณุชุฎุฏุงู ููู ุดุฎุตู ูุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู ููุนููู '{tenant_id}'.")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # ูุฐู ูู ุงูุฎุทูุฉ ุงูุชู ูุฏ ุชููู ุจุทูุฆุฉ
#             effective_question = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
#             logging.info(f"[{session_id}] ุงูุณุคุงู ุงูุฃุตูู: '{question}' -> ุงูุณุคุงู ุงููุญุณูู: '{effective_question}'")
#         else:
#             effective_question = question
#             logging.warning(f"[{session_id}] ูู ูุชู ุงูุนุซูุฑ ุนูู ููู ุดุฎุตู ููุนููู '{tenant_id}'. ุณูุชู ุงุณุชุฎุฏุงู ุงูุณุคุงู ุงูุฃุตูู.")

#         # --- ุงููุฑุญูุฉ 2: ุจูุงุก ุงููุณุชุฑุฌุน ุงููุฌูู ุงููููุชุฑ ---
#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"ูุง ุชูุฌุฏ ุจูุงูุงุช ููุนููู '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#         bm25_retriever.k = k_results // 2
        
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results // 2, 'filter': {'tenant_id': tenant_id}}
#         )
        
#         ensemble_retriever = EnsembleRetriever(
#             retrievers=[bm25_retriever, faiss_retriever],
#             weights=[0.5, 0.5]
#         )

#         # --- ุงููุฑุญูุฉ 3: ุจูุงุก ุณูุณูุฉ RAG ุงููุงููุฉ ---
#         history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_HISTORY_PROMPT)
#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#         conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#         # --- ุงููุฑุญูุฉ 4: ุงูุชูููุฐ ูุงูุจุซ ---
#         logging.info(f"[{session_id}] ุจุฏุก ูุนุงูุฌุฉ ุงูุณุคุงู '{effective_question}'...")
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": effective_question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question)

#         # ุชุญุฏูุซ ุณุฌู ุงููุญุงุฏุซุฉ
#         user_chat_history.append(HumanMessage(content=question)) # ูุญูุธ ุงูุณุคุงู ุงูุฃุตูู
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] ุงูุฅุฌุงุจุฉ ุงููุงููุฉ: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ูุดู ูู ุณูุณูุฉ RAG. ุงูุฎุทุฃ: {e}", exc_info=True)
#         yield {"type": "error", "content": "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)



# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ 3.0: ูุน ูุญุฑู ุฅุนุงุฏุฉ ุตูุงุบุฉ ูุชูุฏู (ุชูููุฑ + ุฃูุซูุฉ + ููุงุนุฏ ุตุงุฑูุฉ) ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ 4.0: ูุน ูุญุฑู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุฐูู ููุชูุงุฒู (ุฃูุซูุฉ ูุชุนุฏุฏุฉ + ููุงุนุฏ ูุฑูุฉ) ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงููุณุฎุฉ 4.1: ุฅุตูุงุญ ุฎุทุฃ NameError ูุฅุนุงุฏุฉ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 5.0: ูุน ูุญุฑู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุฐูู ููุชูุงุฒู (ุณูุณูุฉ ุงูุชูููุฑ ุงูููุทููุฉ) ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.0: ูุน Reranker ูุชุญููู ุฃูุตู ุฏูุฉ (ุงูุญู ุงูููุงุฆู ููุดููุฉ ุงูุณูุงู) ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.1: ุฅุตูุงุญ ูุณุงุฑ ุงุณุชูุฑุงุฏ FlashrankRerank ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.2: ุฅุตูุงุญ ููุงุฆู ููุณุงุฑ ุงุณุชูุฑุงุฏ FlashRankRerank ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.3: ุชุทุจูู ุงูุญู ุงูุตุญูุญ ุจุงุณุชุฎุฏุงู ููุชุจุฉ flashrank ูุจุงุดุฑุฉ ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.4: ุงุณุชุฎุฏุงู ุงูุงุณู ุงูุตุญูุญ 'Ranker' ุจูุงุกู ุนูู ุจูุฆุฉ ุงููุณุชุฎุฏู ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.5: ุฅุตูุงุญ ููุงุฆู ูู TypeError ูู Ranker ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.6: ุฅุตูุงุญ ููุงุฆู ูู KeyError ูู rewriter_chain ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.7: ุฅุตูุงุญ ููุงุฆู ูู TypeError ูู reranker.rerank ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.8: ุฅุตูุงุญ ููุงุฆู ูู TypeError ุจุงุณุชุฎุฏุงู ุงููุณุงุฆุท ุงูููุถุนูุฉ ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 6.9: ุงูุฅุตูุงุญ ุงูุญุงุณู ูู TypeError ูู reranker.rerank ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 7.0: ุงูุฅุตูุงุญ ุงูุฌุฐุฑู ูุงูุฃุฎูุฑ ูู rerank ุจุงุณุชุฎุฏุงู RerankRequest ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 8.0: ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ุงููุงูุจ ุงูุฃูุซุฑ ุฐูุงุกู ---
# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 10.0: ุงูุฅุตุฏุงุฑ ุงูููุงุฆู ูุน ุฅุตูุงุญ chat_history ---

# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 11.0: ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ูุงูุจ ุงูุจุณุงุทุฉ ุงููุทููุฉ ---
#ุงูุตูุฑ ุงููุฑุณูู ูุฑูุฒู ูู ูุชูุฌู ููุฐุง 
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from flashrank import Ranker, RerankRequest

# from .performance_tracker import PerformanceLogger

# # --- 1. ุงูุฅุนุฏุงุฏุงุช ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. ุงููููุงุช ุงูุดุฎุตูุฉ ููุฃูุธูุฉ ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ",
#         "description": "ูุธุงู ุฅููุชุฑููู ูุชุชุจุน ุฑุญูุฉ ุงูุญุตูู ุนูู ุงูุงุนุชูุงุฏุ ุจุฏุกูุง ูู ุฅูุดุงุก ุงูุญุณุงุจุ ุชูุฏูู ุงูุทูุจุ ุฏูุน ุงูููุงุชูุฑุ ูุฑูุฑูุง ุจูุฑุงุญู ุงูุชูููู ูุงูุฒูุงุฑุงุช ุงูููุฏุงููุฉุ ูุงูุชูุงุกู ุจุงุชุฎุงุฐ ุงููุฑุงุฑ ูุฅุตุฏุงุฑ ุงูุดูุงุฏุฉ.",
#         "keywords": ["ุฅูุดุงุก ุญุณุงุจ", "ุชุณุฌูู ุงูุฏุฎูู", "ุทูุจ ุงุนุชูุงุฏ ุฌุฏูุฏ", "ููุงุฆู ุงูุชุญูู", "ุฏุฑุงุณุฉ ููุชุจูุฉ", "ุฒูุงุฑุฉ ููุฏุงููุฉ", "ุฅุฌุฑุงุกุงุช ุชุตุญูุญูุฉ", "ูุงุชูุฑุฉ", "ุดูุงุฏุฉ ุงูุงุนุชูุงุฏ"]
#     },
#     "university_alpha": {
#         "name": "ุชุทุจูู Plant Care ุงูุฒุฑุงุนู",
#         "description": "ุชุทุจูู ุฐูู ููุณุงุนุฏุฉ ุงููุฒุงุฑุนูู ูู ุชุดุฎูุต ุฃูุฑุงุถ ุงููุจุงุชุงุช ูุงูุขูุงุช ุงูุฒุฑุงุนูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนูุ ูุน ุงูุชุฑููุฒ ุนูู ูุญุตููู ุงููุงุช ูุงูุนูุจ.",
#         "keywords": ["ุชุดุฎูุต ุงููุจุงุช", "ุงูุขูุงุช ุงูุฒุฑุงุนูุฉ", "ูุชุทูุจุงุช ูุธูููุฉ", "ุญุงูุงุช ุงุณุชุฎุฏุงู", "ุชุตููู ุงููุธุงู", "plant care", "ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู ุงูุฒุฑุงุนุฉ"]
#     },
#     "school_beta": {
#         "name": "ูุณุชูุฏุงุช ุงูุดุจูุงุช ุงูุนุตุจูุฉ",
#         "description": "ูุงุฏุฉ ุชุนููููุฉ ุชุดุฑุญ ููุงููู ุงูุดุจูุงุช ุงูุนุตุจูุฉุ ููุชุจุฉ TensorFlowุ ูุงูุดุจูุงุช ุงูุชูุงููููุฉ (CNN)ุ ุจูุง ูู ุฐูู ุงูุทุจูุงุชุ ุฏูุงู ุงูุชูุดูุทุ ูุฎูุงุฑุฒููุงุช ุงูุชุญุณูู.",
#         "keywords": ["ุดุจูุฉ ุนุตุจูุฉ", "tensorflow", "convolutional layer", "relu", "pooling", "dense layer", "loss function", "optimizer", "backpropagation"]
#     },
#     "un": {
#         "name": "ุจูุงุจุฉ ุงููุดุชุฑูุงุช ุงูุฅููุชุฑูููุฉ ููุฃูู ุงููุชุญุฏุฉ (UNOPS eSourcing)",
#         "description": "ุฏููู ุฅุฑุดุงุฏู ููููุฑุฏูู ุญูู ููููุฉ ุงุณุชุฎุฏุงู ูุธุงู ุงูุดุฑุงุก ุงูุฅููุชุฑููู ุงูุฎุงุต ุจููุชุจ ุงูุฃูู ุงููุชุญุฏุฉ ูุฎุฏูุงุช ุงููุดุงุฑูุน (UNOPS)ุ ููุดูู ุงูุชุณุฌููุ ุงูุจุญุซ ุนู ุงูููุงูุตุงุชุ ูุชูุฏูู ุงูุนุทุงุกุงุช.",
#         "keywords": ["ููุงูุตุงุช", "ุชุณุฌูู ุงูุฏุฎูู", "ุชูุฏูู ุงูุนุทุงุกุงุช", "unops", "esourcing", "ungm.org", "ููุฑุฏูู", "ุญุงูุฉ ุงูููุงูุตุฉ"]
#     }
# }

# # --- 3. ุงููุงูุจ ุงูููุงุฆู ูุฅุนุงุฏุฉ ุงูุตูุงุบุฉ (ุงูุฅุตุฏุงุฑ 11.0: ุงูุจุณุงุทุฉ ุงููุทููุฉ) ---
# REWRITE_PROMPT_TEMPLATE = """
# ูููุชู ูุงุถุญุฉ ููุญุฏุฏุฉ: ุญูู ุณุคุงู ุงููุณุชุฎุฏู ุฅูู ุฌููุฉ ุจุญุซ ูุตูุฑุฉ ููุฑูุฒุฉ.

# **ุณูุงู ุงููุธุงู:**
# - ุงุณู ุงููุธุงู: {system_name}
# - ูุตูู: {system_description}
# - ูุตุทูุญุงุช ูุงูุฉ: {system_keywords}

# ---
# **ููุงุนุฏ ุตุงุฑูุฉ ูุง ูููู ูุณุฑูุง:**
# 1.  **ุงููุงุชุฌ ุฌููุฉ ูุงุญุฏุฉ ููุท:** ูุฌุจ ุฃู ูููู ุงููุงุชุฌ ุฌููุฉ ูุตูุฑุฉ ูููุฌุฒุฉ.
# 2.  **ุงูุชุฑููุฒ ุนูู ุงูููุฉ:** ุงุณุชุฎุฏู ุงููููุงุช ุงูุฃุณุงุณูุฉ ูู ุณุคุงู ุงููุณุชุฎุฏู ูุงููุตุทูุญุงุช ุงููุงูุฉ ูุจูุงุก ุฌููุฉ ุชุนุจุฑ ุนู ุงููุตุฏ.
# 3.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุนู ุชุนุฑูู ุงููุธุงู:** (ูุซู "ูุง ูู ูุฐุง ุงููุธุงูุ")ุ ูุฌุจ ุฃู ูููู ุงููุงุชุฌ "ูุตู {system_name}".
# 4.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุฎุงุฑุฌ ุงูุณูุงู ุชูุงููุง:** (ูุซู "ูู ูู ููุณูุ")ุ **ุฃุนุฏ ุงูุณุคุงู ุงูุฃุตูู ููุง ูู ุจุงูุถุจุท.**
# 5.  **ููููุน ุงูุดุฑุญ:** ูุง ุชูู ุฃุจุฏูุง ุจุดุฑุญ ุงูุงุณุชุนูุงู ุฃู ุฅุถุงูุฉ ุฃู ูุต ุฅุถุงูู. ุงููุงุชุฌ ูู ุฌููุฉ ุงูุจุญุซ ููุท.

# ---
# **ุฃูุซูุฉ ููุชูููุฐ ุงูุตุญูุญ:**

# ุณุคุงู ุงููุณุชุฎุฏู: ูุงูู ูุฐุง ุงููุธุงูุ
# ุงูุงุณุชุนูุงู ุงููุญุณูู: ูุตู ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ

# ุณุคุงู ุงููุณุชุฎุฏู: ููู ุงุถูู ุญุณุงุจ ุฌุฏูุฏุ
# ุงูุงุณุชุนูุงู ุงููุญุณูู: ุฎุทูุงุช ุฅุถุงูุฉ ุญุณุงุจ ุฌุฏูุฏ ูู ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ

# ุณุคุงู ุงููุณุชุฎุฏู: ูู ูู ุฌูุฑุฌููุงุ
# ุงูุงุณุชุนูุงู ุงููุญุณูู: ูู ูู ุฌูุฑุฌููุงุ
# ---

# **ุงููููุฉ ุงููุทููุจุฉ:**

# ุณุคุงู ุงููุณุชุฎุฏู: {question}

# ุงูุงุณุชุนูุงู ุงููุญุณูู:
# """

# # --- ุจุงูู ุงูููุงูุจ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template("ุฃูุช \"ูุฑุดุฏ ุงูุฏุนู\"ุ ูุณุงุนุฏ ุฐูู ูุฎุจูุฑ. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑูุงู** ุนูู \"ุงูุณูุงู\" ุงูููุฏู.\n- ูู ุฏุงุฆูุงู ูุชุนุงููุงู ููุญุชุฑูุงู.\n- ุฅุฐุง ูุงู ุงูุณูุงู ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ูุฏููุง ุจุดูู ูุจุงุดุฑ ูููุธู.\n- ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุจุดูู ูุงุถุญ ูู ุงูุณูุงูุ ูู ุจุฃุณููุจ ูุทูู: \"ุจุญุซุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ูููู ูู ุฃุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ุจุฎุตูุต ูุฐุง ุงูุณุคุงู.\"\n- ูุง ุชุฎุชุฑุน ุฅุฌุงุจุงุช ุฃุจุฏุงู. ุงูุชุฒู ุจุงูุณูุงู.\n\nุงูุณูุงู:\n{context}\n\nุงูุณุคุงู: {input}\nุงูุฅุฌุงุจุฉ:")

# # --- 4. ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ---
# llm: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. ุงูุฏูุงู ุงูุฃุณุงุณูุฉ ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# def _clean_rewritten_query(raw_query: str) -> str:
#     lines = raw_query.strip().split('\n')
#     for line in reversed(lines):
#         cleaned_line = line.strip()
#         if cleaned_line:
#             if cleaned_line.startswith("ุงูุงุณุชุนูุงู ุงููุญุณูู:"):
#                 return cleaned_line.replace("ุงูุงุณุชุนูุงู ุงููุญุณูู:", "").strip()
#             return cleaned_line
#     return raw_query

# async def initialize_agent():
#     global llm, vector_store, reranker
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("ุจุฏุก ุชููุฆุฉ ุงูููุงุฐุฌ ููุงุนุฏุฉ ุงูุจูุงูุงุช ู Reranker...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุบูุฑ ููุฌูุฏุฉ.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
            
#             reranker = Ranker()
            
#             logging.info("โ ุงููููู ุฌุงูุฒ ููุนูู (ูุน Reranker).")
#         except Exception as e:
#             logging.error(f"ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None and reranker is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 10)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "ุงููููู ุบูุฑ ุฌุงูุฒ. ูุฑุฌู ุฅุนุงุฏุฉ ุชุญููู ุงูุตูุญุฉ."}
#         return

#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         effective_question = question
#         profile = SYSTEM_PROFILES.get(tenant_id)
        
#         if profile:
#             logging.info(f"[{session_id}] ุงุณุชุฎุฏุงู ููู ุดุฎุตู ูุชูุฏู ูุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู...")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # ููุง ูุง ููุฌุฏ ูุชุบูุฑ {ุงููุนู}ุ ูุฐุง ูู ูุญุฏุซ ุงูุฎุทุฃ
#             raw_rewritten_query = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
            
#             effective_question = _clean_rewritten_query(raw_rewritten_query)
#             logging.info(f"[{session_id}] ุงูุณุคุงู ุงูุฃุตูู: '{question}' -> ุงูุณุคุงู ุงููุญุณูู: '{effective_question}'")

#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"ูุง ุชูุฌุฏ ุจูุงูุงุช ููุนููู '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
#         ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         logging.info(f"[{session_id}] ุจุฏุก ุงูุงุณุชุฑุฌุงุน ุงูุฃููู ูู '{effective_question}'...")
#         initial_docs = await ensemble_retriever.ainvoke(effective_question)
#         logging.info(f"[{session_id}] ุชู ุงุณุชุฑุฌุงุน {len(initial_docs)} ูุณุชูุฏ ุฃููู.")

#         logging.info(f"[{session_id}] ุจุฏุก ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ูุงูููุชุฑุฉ...")
        
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
#         rerank_request = RerankRequest(query=question, passages=passages)
#         all_reranked_results = reranker.rerank(rerank_request)
#         top_4_results = all_reranked_results[:4]
        
#         original_docs_map = {doc.page_content: doc for doc in initial_docs}
#         reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
#         logging.info(f"[{session_id}] ุชู ููุชุฑุฉ ุงููุณุชูุฏุงุช ุฅูู {len(reranked_docs)} ูุณุชูุฏ ุนุงูู ุงูุตูุฉ.")

#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
#         logging.info(f"[{session_id}] ุจุฏุก ุชูููุฏ ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ...")
#         full_answer = ""
        
#         async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}

#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] ุงูุฅุฌุงุจุฉ ุงููุงููุฉ: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ูุดู ูู ุณูุณูุฉ RAG. ุงูุฎุทุฃ: {e}", exc_info=True)
#         yield {"type": "error", "content": "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}


# ุงููุณุงุฑ: 2_central_api_service/agent_app/core_logic.py
# --- ุงูุฅุตุฏุงุฑ 12.0: ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ูุงูุจ ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ ---

import os
import logging
import asyncio
import httpx
from typing import AsyncGenerator, Dict, List

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.chains.combine_documents import create_stuff_documents_chain

from flashrank import Ranker, RerankRequest

from .performance_tracker import PerformanceLogger

# --- 1. ุงูุฅุนุฏุงุฏุงุช ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# --- 2. ุงููููุงุช ุงูุดุฎุตูุฉ ููุฃูุธูุฉ ---
SYSTEM_PROFILES = {
    "sys": {
        "name": "ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ",
        "description": "ูุธุงู ุฅููุชุฑููู ูุชุชุจุน ุฑุญูุฉ ุงูุญุตูู ุนูู ุงูุงุนุชูุงุฏ.",
        "keywords": ["ุฅูุดุงุก ุญุณุงุจ", "ุชุณุฌูู ุงูุฏุฎูู", "ุทูุจ ุงุนุชูุงุฏ", "ููุงุฆู ุงูุชุญูู", "ุฏุฑุงุณุฉ ููุชุจูุฉ", "ุฒูุงุฑุฉ ููุฏุงููุฉ", "ุฅุฌุฑุงุกุงุช ุชุตุญูุญูุฉ", "ูุงุชูุฑุฉ", "ุดูุงุฏุฉ"]
    },
    "university_alpha": {
        "name": "ุชุทุจูู Plant Care",
        "description": "ุชุทุจูู ุฐูู ูุชุดุฎูุต ุฃูุฑุงุถ ุงููุจุงุชุงุช ูุงูุขูุงุช ุงูุฒุฑุงุนูุฉ.",
        "keywords": ["ุชุดุฎูุต ุงููุจุงุช", "ุขูุงุช ุฒุฑุงุนูุฉ", "ูุชุทูุจุงุช ูุธูููุฉ", "ุญุงูุงุช ุงุณุชุฎุฏุงู", "ุชุตููู ุงููุธุงู", "plant care"]
    },
    "school_beta": {
        "name": "ูุณุชูุฏุงุช ุงูุดุจูุงุช ุงูุนุตุจูุฉ",
        "description": "ูุงุฏุฉ ุชุนููููุฉ ุนู ุงูุดุจูุงุช ุงูุนุตุจูุฉ ู TensorFlow.",
        "keywords": ["ุดุจูุฉ ุนุตุจูุฉ", "tensorflow", "cnn", "layer", "relu", "pooling", "optimizer"]
    },
    "un": {
        "name": "ุจูุงุจุฉ ุงููุดุชุฑูุงุช ุงูุฅููุชุฑูููุฉ ููุฃูู ุงููุชุญุฏุฉ",
        "description": "ุฏููู ุฅุฑุดุงุฏู ููููุฑุฏูู ูุงุณุชุฎุฏุงู ูุธุงู ุงูุดุฑุงุก ุงูุฅููุชุฑููู.",
        "keywords": ["ููุงูุตุงุช", "ุชุณุฌูู ุงูุฏุฎูู", "ุนุทุงุกุงุช", "unops", "esourcing", "ungm.org", "ููุฑุฏูู"]
    }
}

# --- 3. ุงููุงูุจ ุงูููุงุฆู (ุงูุฅุตุฏุงุฑ 12.0: ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ) ---
REWRITE_PROMPT_TEMPLATE = """
ูููุชู ูู ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ ุงูุฃูุซุฑ ุฃูููุฉ ูู ุณุคุงู ุงููุณุชุฎุฏู ูุชุญุณูู ุงูุจุญุซ.

**ุณูุงู ุงููุธุงู:** {system_name}
**ูุตุทูุญุงุช ูุงูุฉ:** {system_keywords}

---
**ุงูููุงุนุฏ:**
1.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุนุงููุง ุนู ุงููุธุงู** (ูุซู "ูุง ูู ูุฐุง ุงููุธุงูุ")ุ ุฃุฑุฌุน ุงุณู ุงููุธุงู ููุท: `{system_name}`.
2.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุนู ุฎุทูุงุช ุฃู ููููุฉ ูุนู ุดูุก** (ูุซู "ููู ุฃุถูู ูุณุชุฎุฏูุ")ุ ุฃุฑุฌุน ุงููุนู ูุงูููุนูู ุจู: `ุฅุถุงูุฉ ูุณุชุฎุฏู ุฌุฏูุฏ`.
3.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุนู ุชุนุฑูู ูุตุทูุญ** (ูุซู "ูุงูู ุงูุดุจูุงุช ุงูุนุตุจูุฉุ")ุ ุฃุฑุฌุน ุงููุตุทูุญ ููุณู: `ุงูุดุจูุงุช ุงูุนุตุจูุฉ`.
4.  **ุฅุฐุง ูุงู ุงูุณุคุงู ุฎุงุฑุฌ ุงูุณูุงู ุชูุงููุง** (ูุซู "ูู ูู ููุณูุ")ุ ุฃุฑุฌุน ุงูุณุคุงู ุงูุฃุตูู ููุง ูู.
5.  **ุงููุงุชุฌ ูุฌุจ ุฃู ูููู ูุตูุฑูุง ุฌุฏูุง ููุจุงุดุฑูุง.** ูุง ุชุณุชุฎุฏู ุฌูู ูุงููุฉ.

---
**ุฃูุซูุฉ:**

ุณุคุงู ุงููุณุชุฎุฏู: ูุงูู ูุฐุง ุงููุธุงู ุจุงุฎุชุตุงุฑ
ุงูุงุณุชุนูุงู ุงููุญุณูู: ูุธุงู ุฅุฏุงุฑุฉ ุทูุจุงุช ุงูุงุนุชูุงุฏ

ุณุคุงู ุงููุณุชุฎุฏู: ููููู ุงููุตูู ูููุธุงู
ุงูุงุณุชุนูุงู ุงููุญุณูู: ููููุฉ ุชุณุฌูู ุงูุฏุฎูู

ุณุคุงู ุงููุณุชุฎุฏู: ูุงูู ุงูุดุจูุงุช ุงูุนุตุจูู
ุงูุงุณุชุนูุงู ุงููุญุณูู: ุงูุดุจูุงุช ุงูุนุตุจูุฉ

ุณุคุงู ุงููุณุชุฎุฏู: ูู ูู ุฌูุฑุฌููุง
ุงูุงุณุชุนูุงู ุงููุญุณูู: ูู ูู ุฌูุฑุฌููุง
---

**ุงููููุฉ ุงููุทููุจุฉ:**

ุณุคุงู ุงููุณุชุฎุฏู: {question}

ุงูุงุณุชุนูุงู ุงููุญุณูู:
"""

# --- ุจุงูู ุงูููุงูุจ ---
ANSWER_PROMPT = ChatPromptTemplate.from_template("ุฃูุช \"ูุฑุดุฏ ุงูุฏุนู\"ุ ูุณุงุนุฏ ุฐูู ูุฎุจูุฑ. ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู ุจุงูุงุนุชูุงุฏ **ุญุตุฑูุงู** ุนูู \"ุงูุณูุงู\" ุงูููุฏู.\n- ูู ุฏุงุฆูุงู ูุชุนุงููุงู ููุญุชุฑูุงู.\n- ุฅุฐุง ูุงู ุงูุณูุงู ูุญุชูู ุนูู ุฅุฌุงุจุฉุ ูุฏููุง ุจุดูู ูุจุงุดุฑ ูููุธู.\n- ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ุจุดูู ูุงุถุญ ูู ุงูุณูุงูุ ูู ุจุฃุณููุจ ูุทูู: \"ุจุญุซุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ูููู ูู ุฃุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ุจุฎุตูุต ูุฐุง ุงูุณุคุงู.\"\n- ูุง ุชุฎุชุฑุน ุฅุฌุงุจุงุช ุฃุจุฏุงู. ุงูุชุฒู ุจุงูุณูุงู.\n\nุงูุณูุงู:\n{context}\n\nุงูุณุคุงู: {input}\nุงูุฅุฌุงุจุฉ:")

# --- 4. ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ---
llm: Ollama = None
vector_store: FAISS = None
reranker: Ranker = None
chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
initialization_lock = asyncio.Lock()
perf_logger = PerformanceLogger()

# --- 5. ุงูุฏูุงู ุงูุฃุณุงุณูุฉ (ูุนุธููุง ูุจูู ููุง ูู) ---

def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
    return list(vs.docstore._dict.values())

def _clean_rewritten_query(raw_query: str) -> str:
    lines = raw_query.strip().split('\n')
    for line in reversed(lines):
        cleaned_line = line.strip()
        if cleaned_line:
            if cleaned_line.startswith("ุงูุงุณุชุนูุงู ุงููุญุณูู:"):
                return cleaned_line.replace("ุงูุงุณุชุนูุงู ุงููุญุณูู:", "").strip()
            return cleaned_line
    return raw_query.strip()

async def initialize_agent():
    global llm, vector_store, reranker
    async with initialization_lock:
        if vector_store is not None: return
        logging.info("ุจุฏุก ุชููุฆุฉ ุงูููุงุฐุฌ ููุงุนุฏุฉ ุงูุจูุงูุงุช ู Reranker...")
        try:
            async with httpx.AsyncClient( ) as client:
                await client.get(OLLAMA_HOST, timeout=10.0)
            
            llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
            embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
            if not os.path.isdir(UNIFIED_DB_PATH):
                raise FileNotFoundError("ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุบูุฑ ููุฌูุฏุฉ.")

            vector_store = await asyncio.to_thread(
                FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
            )
            
            reranker = Ranker()
            
            logging.info("โ ุงููููู ุฌุงูุฒ ููุนูู (ูุน Reranker).")
        except Exception as e:
            logging.error(f"ูุดู ูุงุฏุญ ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
            raise

def agent_ready() -> bool:
    return vector_store is not None and llm is not None and reranker is not None

async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    question = request_info.get("question", "")
    tenant_id = request_info.get("tenant_id", "default_session")
    k_results = request_info.get("k_results", 10)
    session_id = tenant_id or "default_session"

    if not agent_ready():
        yield {"type": "error", "content": "ุงููููู ุบูุฑ ุฌุงูุฒ. ูุฑุฌู ุฅุนุงุฏุฉ ุชุญููู ุงูุตูุญุฉ."}
        return

    user_chat_history = chat_history.get(session_id, [])

    try:
        effective_question = question
        profile = SYSTEM_PROFILES.get(tenant_id)
        
        if profile:
            logging.info(f"[{session_id}] ุงุณุชุฎุฏุงู ููู ุดุฎุตู ูุชูุฏู ูุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู...")
            rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
            rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
            raw_rewritten_query = await rewriter_chain.ainvoke({
                "system_name": profile.get("name", ""),
                "system_description": profile.get("description", ""),
                "system_keywords": ", ".join(profile.get("keywords", [])),
                "question": question
            })
            
            effective_question = _clean_rewritten_query(raw_rewritten_query)
            logging.info(f"[{session_id}] ุงูุณุคุงู ุงูุฃุตูู: '{question}' -> ุงูุงุณุชุนูุงู ุงููุญุณูู: '{effective_question}'")

        all_docs = _load_all_docs_from_faiss(vector_store)
        tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

        if not tenant_docs:
            yield {"type": "error", "content": f"ูุง ุชูุฌุฏ ุจูุงูุงุช ููุนููู '{tenant_id}'."}
            return

        bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
        faiss_retriever = vector_store.as_retriever(
            search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
        )
        ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
        logging.info(f"[{session_id}] ุจุฏุก ุงูุงุณุชุฑุฌุงุน ุงูุฃููู ูู '{effective_question}'...")
        initial_docs = await ensemble_retriever.ainvoke(effective_question)
        logging.info(f"[{session_id}] ุชู ุงุณุชุฑุฌุงุน {len(initial_docs)} ูุณุชูุฏ ุฃููู.")

        logging.info(f"[{session_id}] ุจุฏุก ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ูุงูููุชุฑุฉ...")
        
        passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
        rerank_request = RerankRequest(query=question, passages=passages)
        all_reranked_results = reranker.rerank(rerank_request)
        top_4_results = all_reranked_results[:4]
        
        original_docs_map = {doc.page_content: doc for doc in initial_docs}
        reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
        logging.info(f"[{session_id}] ุชู ููุชุฑุฉ ุงููุณุชูุฏุงุช ุฅูู {len(reranked_docs)} ูุณุชูุฏ ุนุงูู ุงูุตูุฉ.")

        document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
        logging.info(f"[{session_id}] ุจุฏุก ุชูููุฏ ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ...")
        full_answer = ""
        
        async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
            if chunk:
                full_answer += chunk
                yield {"type": "chunk", "content": chunk}

        user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
        chat_history[session_id] = user_chat_history[-10:]
        logging.info(f"[{session_id}] ุงูุฅุฌุงุจุฉ ุงููุงููุฉ: '{full_answer}'")

    except Exception as e:
        logging.error(f"[{session_id}] ูุดู ูู ุณูุณูุฉ RAG. ุงูุฎุทุฃ: {e}", exc_info=True)
        yield {"type": "error", "content": "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูุงุฏุญ."}
