# # 2_central_api_service/agent_app/core_logic.py (ุงููุณุฎุฉ ุงูุงุญุชุฑุงููุฉ ุงูููุงุฆูุฉ)

# import os
# import logging
# from typing import List, Dict, Any, AsyncGenerator
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache

# from .performance_tracker import PerformanceLogger
# #ูุฐุง ุงูููุฏ ุงูุงุณุชุฏุนุง ุงูุฎุงุต
# perf_logger = PerformanceLogger()
# # --- ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (Cache) ---
# logging.info("๐ ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (InMemoryCache) ูู LangChain...")
# langchain.llm_cache = InMemoryCache()

# # --- ุงูุฅุนุฏุงุฏุงุช ุงูุฃูููุฉ ---
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))


# global vector_store, llm, prompt, embeddings_model
# embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)

# # --- ูุฑุงุกุฉ ุงูุฅุนุฏุงุฏุงุช ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ---
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# # --- ูุงูุจ ุงูุฃุณุฆูุฉ ุงููุญุณู ---
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต. ุงุณุชุฎุฏู ุงููุนูููุงุช ุงููุชููุฑุฉ ูู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ ูุงุญุชุฑุงููุฉ.
# - ุงูุณูุงู ุงูููุฏู ุนุจุงุฑุฉ ุนู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุฐุงุช ุงูุตูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู." ููุง ุชุญุงูู ุงุฎุชูุงู ุฅุฌุงุจุฉ.
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

# **ุงูุณูุงู:**
# {context}

# **ุณุคุงู ุงููุณุชุฎุฏู:**
# {question}

# **ุงูุฅุฌุงุจุฉ:**
# """

# # --- ูุชุบูุฑุงุช ุนุงูููุฉ ---
# vector_store = None
# llm = None
# prompt = None

# def initialize_agent():
#     """ ุชููู ุจุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุงูููุงุฐุฌ. ุชูุณุชุฏุนู ูุฑุฉ ูุงุญุฏุฉ ุนูุฏ ุจุฏุก ุชุดุบูู ุงูู API. """
#     global vector_store, llm, prompt
    
#     if vector_store:
#         logging.info("ุงููููู ููููุฃ ุจุงููุนู.")
#         return

#     try:
#         logging.info("="*50)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู...")
        
#         # 1. ุชุญููู ูููุฐุฌ ุงูุชุถููู
#         logging.info(f"ุชุญููู ูููุฐุฌ ุงูุชุถููู: {EMBEDDING_MODEL_NAME}...")
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)

#         # 2. ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช ุงููุชุฌูุงุช FAISS
#         logging.info(f"ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู: {VECTOR_DB_PATH}...")
#         if not os.path.exists(os.path.join(VECTOR_DB_PATH, "index.faiss")):
#             raise FileNotFoundError(f"ูุงุนุฏุฉ ุงููุนุฑูุฉ (index.faiss) ุบูุฑ ููุฌูุฏุฉ ูู ุงููุณุงุฑ: {VECTOR_DB_PATH}. ูุฑุฌู ุชุดุบูู ุฎุท ุฃูุงุจูุจ ุงูุจูุงูุงุช ุฃููุงู.")
        
#         vector_store = FAISS.load_local(
#             VECTOR_DB_PATH,
#             embeddings=embeddings_model,
#             allow_dangerous_deserialization=True
#         )
#         logging.info("โ ุชู ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ.")

#         # 3. ุชุญููู ุงููููุฐุฌ ุงููุบูู ุงููุจูุฑ ูููุญุงุฏุซุฉ ูุน ุฅุนุฏุงุฏุงุช ุฅุถุงููุฉ
#         logging.info(f"ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ: {CHAT_MODEL_NAME}...")
#         llm = Ollama(
#             model=CHAT_MODEL_NAME,
#             temperature=0.1,  # ุชูููู ุงูุนุดูุงุฆูุฉ ูุฌุนู ุงูุฅุฌุงุจุงุช ุฃูุซุฑ ุงุชุณุงููุง
#             # ููููู ุฅุถุงูุฉ ุงููุฒูุฏ ูู ุงูุฅุนุฏุงุฏุงุช ููุง ูุซู top_p, top_k
#         )

#         # 4. ุฅุนุฏุงุฏ ูุงูุจ ุงูุฃุณุฆูุฉ
#         prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        
#         logging.info("โ ุงูุชููุช ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู ุจูุฌุงุญ!")
#         logging.info("="*50)
#     except FileNotFoundError as e:
#         logging.critical(f" ูุดู ุงูุชููุฆุฉ: ููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุบูุฑ ููุฌูุฏ. {e}", exc_info=True)
#         raise
#     except Exception as e:
#         logging.critical(f" ูุดู ูุงุฏุญ ูุบูุฑ ูุชููุน ุฃุซูุงุก ุชููุฆุฉ ุงููููู: {e}", exc_info=True)
#         raise

# def format_docs_with_source(docs: List[Dict[str, Any]]) -> str:
#     """ ุฏุงูุฉ ูุณุงุนุฏุฉ ูุญุณูุฉ: ุชูุณู ุงููุณุชูุฏุงุช ูุน ุฐูุฑ ูุตุฏุฑูุง. """
#     if not docs:
#         return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
    
#     sources = {doc.metadata.get('source', 'ูุตุฏุฑ ุบูุฑ ูุนุฑูู') for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     """
#     ุชุณุชูุจู ุณุคุงูุงู ููููุฉ ุงูุนูููุ ูุชุณุชุฎุฏู ุณูุณูุฉ RAG ูุจุซ ุงูุฅุฌุงุจุฉ ุจุดูู ุชูุงุนูู.
#     ูุน ุชุชุจุน ุงูุฃุฏุงุก ููู ูุฑุญูุฉ: ุงูุชุถูููุ ุงูุงุณุชุฑุฌุงุนุ ุชูุณูู ุงููุณุชูุฏุงุชุ ูุงุณุชุฏุนุงุก ุงููููุฐุฌ.
#     """
#     if not vector_store or not llm or not prompt:
#         raise RuntimeError("ุงููููู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")

#     logging.info(f"ุงุณุชูุจุงู ุทูุจ ุจุซ ููุนููู '{tenant_id}' (k={k_results}): '{question}'")

#     try:
#         # --- ูุฑุญูุฉ ุงูุชุถููู (Embedding) ---
#         perf_logger.start("embedding")
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         question_vector = embeddings_model.embed_query(question)
#         perf_logger.end("embedding", tenant_id, question)

#         # --- ูุฑุญูุฉ ุงุณุชุฑุฌุงุน ุงููุณุชูุฏุงุช (Retriever) ---
#         perf_logger.start("retriever")
#         retriever = vector_store.as_retriever(
#             search_type="similarity",
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
        
#         relevant_docs = retriever.invoke(question)

#         perf_logger.end("retriever", tenant_id, question, extra_info={"retrieved_docs": len(relevant_docs)})

#         # --- ูุฑุญูุฉ ุชูุณูู ุงููุณุชูุฏุงุช (Format Docs) ---
#         perf_logger.start("format_docs")
#         formatted_context = format_docs_with_source(relevant_docs)
#         perf_logger.end("format_docs", tenant_id, question, extra_info={"formatted_length": len(formatted_context)})

#         # --- ูุฑุญูุฉ ุงุณุชุฏุนุงุก ุงููููุฐุฌ (LLM Response) ---
#         perf_logger.start("llm_response")
#         rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: relevant_docs)
#             | RunnablePassthrough.assign(context=lambda x: formatted_context)
#             | prompt
#             | llm
#         )

#         logging.info(f"ุฌุงุฑู ุงูุจุญุซ ุนู ุฅุฌุงุจุฉ ุถูู ูุทุงู ุงูุนููู '{tenant_id}'...")

#         # --- ุงูุจุซ ุงูุชูุงุนูู ---
#         async for chunk in rag_chain.astream({"question": question}):
#             yield chunk

#         perf_logger.end("llm_response", tenant_id, question, extra_info={"k_results": k_results})

#     except Exception as e:
#         logging.error(f"ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ ููุนููู '{tenant_id}': {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุญุงููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})


# 22222222222222_central_api_service/agent_app/core_logic.py (ูุณุฎุฉ ูุญุณูุฉ ูุชุณุฑูุน ูุฑุญูุฉ ุงูุชุถููู)
# {
#   "question": "ูู ูู ูุดุฑู ูุฐุง ุงููุดุฑูุน ููู ูู ุงูุทูุงุจ ุงูุฐู ุนูููู ููู ุงู ุฌุงูุนู ",
#   "tenant_id": "university_alpha",
#   "k_results": 4
# }
# #"""ุงููุดุฑู: ุงูุฏูุชูุฑ ูููุฏ ุดุงูุฑ  
# ุงูุทูุงุจ ุงูุฐูู ุนูููุง ุนูู ุงููุดุฑูุน:  
# - ุนุจุฏ ุงูุนุฒูุฒ ุนูู ุญุณูู ุงููุงุถู  
# - ููุฏู ูุญูุฏ ููุฏู ุฃุจู ุนูู  
# - ุนูู ุฃุญูุฏ ุนุจุฏ ุงููู ุงูุณุนูุฏู  
# - ูุงุฑูู ุญุณูู ุงูุบุฑูุจู  
# ุงูุฌุงูุนุฉ: ุฌุงูุนุฉ ุงูุนููู ูุงูุชูููููุฌูุง"""
# import os
# import logging
# from typing import List, Dict, Any, AsyncGenerator
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache

# from .performance_tracker import PerformanceLogger

# # ------------------- ุชุณุฌูู ุงูุฃุฏุงุก -------------------
# perf_logger = PerformanceLogger()

# # ------------------- ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ -------------------
# logging.info("๐ ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (InMemoryCache) ูู LangChain...")
# langchain.llm_cache = InMemoryCache()

# # ------------------- ุงูุฅุนุฏุงุฏุงุช ุงูุนุงูุฉ -------------------
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))

# # ------------------- ูุชุบูุฑุงุช ุงูุจูุฆุฉ -------------------
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# # ------------------- ูุงูุจ ุงูู Prompt -------------------
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต. ุงุณุชุฎุฏู ุงููุนูููุงุช ุงููุชููุฑุฉ ูู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ ูุงุญุชุฑุงููุฉ.
# - ุงูุณูุงู ุงูููุฏู ุนุจุงุฑุฉ ุนู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุฐุงุช ุงูุตูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู." ููุง ุชุญุงูู ุงุฎุชูุงู ุฅุฌุงุจุฉ.
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

# **ุงูุณูุงู:**
# {context}

# **ุณุคุงู ุงููุณุชุฎุฏู:**
# {question}

# **ุงูุฅุฌุงุจุฉ:**
# """

# # ------------------- ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ -------------------
# vector_store = None
# llm = None
# prompt = None
# embeddings_model = None  # โ ูุถุงูุฉ: ููุงุญุชูุงุธ ุจูููุฐุฌ ุงูุชุถููู ูู ุงูุฐุงูุฑุฉ

# # ==============================================================
# # ๐ง ุชููุฆุฉ ุงููููู (ุชุญููู ุงูููุงุฑุฏ ูุฑุฉ ูุงุญุฏุฉ ููุท)
# # ==============================================================
# def initialize_agent():
#     """ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู (ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุงูููุงุฐุฌ ูุฑุฉ ูุงุญุฏุฉ ููุท)."""
#     global vector_store, llm, prompt, embeddings_model

#     if vector_store:
#         logging.info("ุงููููู ููููุฃ ุจุงููุนู.")
#         return

#     try:
#         logging.info("=" * 60)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู...")

#         # 1๏ธโฃ ุชุญููู ูููุฐุฌ ุงูุชุถููู ูุฑุฉ ูุงุญุฏุฉ ููุท (ุจุฏูุงู ูู ูู ุงุณุชุนูุงู)
#         logging.info(f"๐ฆ ุชุญููู ูููุฐุฌ ุงูุชุถููู: {EMBEDDING_MODEL_NAME}...")
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         logging.info("โ ุชู ุชุญููู ูููุฐุฌ ุงูุชุถููู ูู ุงูุฐุงูุฑุฉ ุจูุฌุงุญ.")

#         # 2๏ธโฃ ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช ุงููุชุฌูุงุช FAISS
#         logging.info(f"๐ ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู: {VECTOR_DB_PATH}...")
#         index_path = os.path.join(VECTOR_DB_PATH, "index.faiss")
#         if not os.path.exists(index_path):
#             raise FileNotFoundError(f"ูุงุนุฏุฉ ุงููุนุฑูุฉ (index.faiss) ุบูุฑ ููุฌูุฏุฉ ูู ุงููุณุงุฑ: {VECTOR_DB_PATH}.")
        
#         vector_store = FAISS.load_local(
#             VECTOR_DB_PATH,
#             embeddings=embeddings_model,
#             allow_dangerous_deserialization=True
#         )
#         logging.info("โ ุชู ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ.")

#         # 3๏ธโฃ ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ
#         logging.info(f"๐งฉ ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ: {CHAT_MODEL_NAME}...")
#         llm = Ollama(model=CHAT_MODEL_NAME, temperature=0.1)

#         # 4๏ธโฃ ุฅุนุฏุงุฏ ุงููุงูุจ
#         prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

#         logging.info("โ ุงูุชููุช ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู ุจูุฌุงุญ!")
#         logging.info("=" * 60)

#     except Exception as e:
#         logging.critical(f"โ ูุดู ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#         raise

# # ==============================================================
# # ๐ง ุชูุณูู ุงููุณุชูุฏุงุช
# # ==============================================================
# def format_docs_with_source(docs: List[Dict[str, Any]]) -> str:
#     """ุชูุณู ุงููุณุชูุฏุงุช ูุน ุฐูุฑ ุงููุตุฏุฑ."""
#     if not docs:
#         return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
    
#     sources = {doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู") for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"

# # ==============================================================
# # ๐ ุจุซ ุงูุฅุฌุงุจุฉ
# # ==============================================================
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     """ุชุจุซ ุงูุฅุฌุงุจุฉ ุนูู ุงูุณุคุงู ุจุดูู ุชูุงุนูู ุจุงุณุชุฎุฏุงู RAG."""
#     if not vector_store or not llm or not prompt or not embeddings_model:
#         raise RuntimeError("โ๏ธ ุงููููู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")

#     logging.info(f"๐ฃ๏ธ ุงุณุชูุจุงู ุณุคุงู ูู ุงูุนููู '{tenant_id}': {question}")

#     try:
#         # --- ูุฑุญูุฉ ุงูุชุถููู (Embedding) ---
#         perf_logger.start("embedding")
#         question_vector = embeddings_model.embed_query(question)
#         perf_logger.end("embedding", tenant_id, question)

#         # --- ูุฑุญูุฉ ุงูุงุณุชุฑุฌุงุน ---
#         perf_logger.start("retriever")
#         retriever = vector_store.as_retriever(
#             search_type="similarity",
#             search_kwargs={"k": k_results, "filter": {"tenant_id": tenant_id}},
#         )
#         relevant_docs = retriever.invoke(question)
#         perf_logger.end("retriever", tenant_id, question, extra_info={"retrieved_docs": len(relevant_docs)})

#         # --- ุชูุณูู ุงููุณุชูุฏุงุช ---
#         perf_logger.start("format_docs")
#         formatted_context = format_docs_with_source(relevant_docs)
#         perf_logger.end("format_docs", tenant_id, question, extra_info={"formatted_length": len(formatted_context)})

#         # --- ุงุณุชุฏุนุงุก ุงููููุฐุฌ ---
#         perf_logger.start("llm_response")
#         rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: relevant_docs)
#             | RunnablePassthrough.assign(context=lambda x: formatted_context)
#             | prompt
#             | llm
#         )

#         async for chunk in rag_chain.astream({"question": question}):
#             yield chunk

#         perf_logger.end("llm_response", tenant_id, question, extra_info={"k_results": k_results})

#     except Exception as e:
#         logging.error(f"โ ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ: {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุญุงููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})
#3333333333333
# #ูููุฐุฌ ูุงูุช ุณุฑุนุชู 5 ุฏูุงูู ููุนุชุจุฑ ุงูุถู ูู ุงูุณุงุจู 
# import os
# import logging
# import time
# from typing import List, Dict, Any, AsyncGenerator
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache

# from .performance_tracker import PerformanceLogger

# # -----------------------------------------------------------------------------
# # ๐งฉ ูุธุงู ุชุณุฌูู ุงูุฃุฏุงุก
# # -----------------------------------------------------------------------------
# perf_logger = PerformanceLogger()

# # -----------------------------------------------------------------------------
# # ๐ง ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ
# # -----------------------------------------------------------------------------
# logging.info("๐ ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (InMemoryCache) ูู LangChain...")
# langchain.llm_cache = InMemoryCache()

# # -----------------------------------------------------------------------------
# # โ๏ธ ุงูุฅุนุฏุงุฏุงุช ุงูุนุงูุฉ
# # -----------------------------------------------------------------------------
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))

# # -----------------------------------------------------------------------------
# # ๐ฆ ูุชุบูุฑุงุช ุงูุจูุฆุฉ
# # -----------------------------------------------------------------------------
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# # -----------------------------------------------------------------------------
# # ๐ง ูุงูุจ ุงูู Prompt
# # -----------------------------------------------------------------------------
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต. ุงุณุชุฎุฏู ุงููุนูููุงุช ุงููุชููุฑุฉ ูู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ ูุงุญุชุฑุงููุฉ.
# - ุงูุณูุงู ุงูููุฏู ุนุจุงุฑุฉ ุนู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุฐุงุช ุงูุตูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู." ููุง ุชุญุงูู ุงุฎุชูุงู ุฅุฌุงุจุฉ.
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

# **ุงูุณูุงู:**
# {context}

# **ุณุคุงู ุงููุณุชุฎุฏู:**
# {question}

# **ุงูุฅุฌุงุจุฉ:**
# """

# # -----------------------------------------------------------------------------
# # ๐ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ
# # -----------------------------------------------------------------------------
# vector_store = None
# llm = None
# prompt = None
# embeddings_model = None  # โ ูููุฐุฌ ุงูุชุถููู ููุญููู ูุฑุฉ ูุงุญุฏุฉ ููุท

# # -----------------------------------------------------------------------------
# # ๐ ุชููุฆุฉ ุงููููู (ุชุญููู ุงูููุงุฑุฏ ูุฑุฉ ูุงุญุฏุฉ ููุท)
# # -----------------------------------------------------------------------------
# def initialize_agent():
#     """ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู (ุชุญููู ุงูููุงุฐุฌ ูุงูุจูุงูุงุช ูุฑุฉ ูุงุญุฏุฉ ููุท)."""
#     global vector_store, llm, prompt, embeddings_model

#     if vector_store:
#         logging.info("โ ุงููููู ููููุฃ ูุณุจููุง.")
#         return

#     try:
#         logging.info("=" * 60)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู...")

#         # 1๏ธโฃ ุชุญููู ูููุฐุฌ ุงูุชุถููู ูุฑุฉ ูุงุญุฏุฉ
#         perf_logger.start("embedding_model_load")
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         perf_logger.end("embedding_model_load", "system", "initialization")
#         logging.info("โ ุชู ุชุญููู ูููุฐุฌ ุงูุชุถููู ูู ุงูุฐุงูุฑุฉ.")

#         # 2๏ธโฃ ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช ุงููุชุฌูุงุช FAISS
#         perf_logger.start("vector_db_load")
#         if not os.path.exists(os.path.join(VECTOR_DB_PATH, "index.faiss")):
#             raise FileNotFoundError(f"ูุงุนุฏุฉ ุงููุนุฑูุฉ (index.faiss) ุบูุฑ ููุฌูุฏุฉ ูู: {VECTOR_DB_PATH}")
#         vector_store = FAISS.load_local(
#             VECTOR_DB_PATH,
#             embeddings=embeddings_model,
#             allow_dangerous_deserialization=True
#         )
#         perf_logger.end("vector_db_load", "system", "initialization")
#         logging.info("โ ุชู ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ.")

#         # 3๏ธโฃ ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ (LLM)
#         perf_logger.start("chat_model_load")
#         llm = Ollama(model=CHAT_MODEL_NAME, temperature=0.1)
#         perf_logger.end("chat_model_load", "system", "initialization")
#         logging.info("โ ุชู ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ ุจูุฌุงุญ.")

#         # 4๏ธโฃ ุฅุนุฏุงุฏ ุงููุงูุจ (Prompt)
#         prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
#         logging.info("โ ุงูุชููุช ุงูุชููุฆุฉ ุจูุฌุงุญ!")
#         logging.info("=" * 60)

#     except Exception as e:
#         logging.critical(f"โ ูุดู ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#         raise

# # -----------------------------------------------------------------------------
# # ๐งพ ุชูุณูู ุงููุณุชูุฏุงุช
# # -----------------------------------------------------------------------------
# def format_docs_with_source(docs: List[Dict[str, Any]]) -> str:
#     """ุชูุณู ุงููุณุชูุฏุงุช ุงููุณุชุฑุฌุนุฉ ูุชุถูู ุงููุตุงุฏุฑ."""
#     if not docs:
#         return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
#     sources = {doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู") for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"

# # -----------------------------------------------------------------------------
# # ๐ง ุจุซ ุงูุฅุฌุงุจุฉ ุจุดูู ุชูุงุนูู (RAG Stream)
# # -----------------------------------------------------------------------------
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     """
#     ุจุซ ุงูุฅุฌุงุจุฉ ุจุดูู ุชูุงุนูู ูุน ุชุณุฌูู ุงูุฃุฏุงุก ููู ูุฑุญูุฉ.
#     """
#     if not vector_store or not llm or not prompt or not embeddings_model:
#         raise RuntimeError("โ๏ธ ุงููููู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")

#     logging.info(f"๐ฉ ุงุณุชูุจุงู ุณุคุงู ูู ุงูุนููู '{tenant_id}': {question}")

#     try:
#         # ================================
#         # 1๏ธโฃ ูุฑุญูุฉ ุงูุชุถููู (Embedding)
#         # ================================
#         perf_logger.start("embedding")
#         question_vector = embeddings_model.embed_query(question)
#         perf_logger.end("embedding", tenant_id, question)

#         # ================================
#         # 2๏ธโฃ ูุฑุญูุฉ ุงูุงุณุชุฑุฌุงุน (Retriever)
#         # ================================
#         perf_logger.start("retriever")
#         retriever = vector_store.as_retriever(
#             search_type="similarity",
#             search_kwargs={"k": k_results, "filter": {"tenant_id": tenant_id}},
#         )
#         relevant_docs = retriever.invoke(question)
#         perf_logger.end("retriever", tenant_id, question, extra_info={"retrieved_docs": len(relevant_docs)})

#         # ================================
#         # 3๏ธโฃ ูุฑุญูุฉ ุชูุณูู ุงููุณุชูุฏุงุช (Formatting)
#         # ================================
#         perf_logger.start("format_docs")
#         formatted_context = format_docs_with_source(relevant_docs)
#         perf_logger.end("format_docs", tenant_id, question, extra_info={"formatted_length": len(formatted_context)})

#         # ================================
#         # 4๏ธโฃ ูุฑุญูุฉ ุงุณุชุฏุนุงุก ุงููููุฐุฌ (LLM)
#         # ================================
#         perf_logger.start("llm_response")
#         rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: relevant_docs)
#             | RunnablePassthrough.assign(context=lambda x: formatted_context)
#             | prompt
#             | llm
#         )

#         async for chunk in rag_chain.astream({"question": question}):
#             yield chunk

#         perf_logger.end("llm_response", tenant_id, question, extra_info={"k_results": k_results})

#     except Exception as e:
#         logging.error(f"โ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ: {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุนุงูุฌุฉ ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})
 

#ุงูุชุนุฏูู ุงูุฌุฏูุฏ ูุน ุงุถุงูู ูููุฐุฌ ุชุฑุชูุจ ูุชุนุฏูู ูููุฐุฌ ุงูุจุญุซ ุงูู ูููุฐุฌ ุจุญุซ ูุฌูู 
# core_logic.py
# core_logic.py
# core_logic.py
# core_logic.py
# core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ - ูุน ุงูุชูุฌูู ูุงููููุฉ ุงูุฏููุงููููุฉ)
# 
# # ููุณู: ูุง ููุฌุฏ ุฃู ูุนูููุงุช ุนู ููุณู ูู ุงูุณูุงู ุงูููุฏู.  
#ุงูุฏูุชูุฑ ูููุฏ ุดุงูุฑ: ูู ุฑุฆูุณ ูุณู ุชูููููุฌูุง ุงููุนูููุงุช ูู ุฌุงูุนุฉ ุงูุนููู ูุงูุชูููููุฌูุง ูู ุงููููุ ููุนุชุจุฑ ูุดุฑููุง ุนูู ูุดุฑูุน ูุฐุง ุงูููุฏ ููุชุงุฒ ูู ุญูุซ ุงููุชุงุฆุฌ ูููู ุจุทู ุจุณ ูุงูู ูุณุชุฎุฏู ุงููููุฐุฌ ุงููุบูู ุงููุจูุฑ ูู ุชุตููู ููุน ุงูุณูุงู ูู ููุนุงู ุงู ููู ุงู ุฏุนู ููุฐู ุงููุฑุญูู ุชุณุชููู ุงููุซุจุฑ ูู ุงูููุช 
# import os
# import logging
# import time
# from typing import List, AsyncGenerator, Dict
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache
# from langchain_core.documents import Document
# from sentence_transformers import CrossEncoder
# from rank_bm25 import BM25Okapi

# from .performance_tracker import PerformanceLogger

# # -----------------------------------------------------------------------------
# # ๐งฉ ุฅุนุฏุงุฏุงุช ุนุงูุฉ ูุชุณุฌูู
# # -----------------------------------------------------------------------------
# perf_logger = PerformanceLogger()
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))
# langchain.llm_cache = InMemoryCache()

# # -----------------------------------------------------------------------------
# # ๐ฆ ูุชุบูุฑุงุช ุงูุจูุฆุฉ ูุงูููุงุฐุฌ
# # -----------------------------------------------------------------------------
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))
# RERANK_MODEL_NAME = "BAAI/bge-reranker-base"

# # -----------------------------------------------------------------------------
# # ๐ง ููุงูุจ ุงูู Prompts (ูุน ุฏุนู ุงูุดุฎุตูุฉ ุงูุฏููุงููููุฉ)
# # -----------------------------------------------------------------------------

# # --- 1. ูุงูุจ ุงูุชูุฌูู (Classifier) ---
# ROUTING_PROMPT_TEMPLATE = """
# ูููุชู ูู ุชุตููู ุณุคุงู ุงููุณุชุฎุฏู ุฅูู ุฃุญุฏ ุงููุฆุชูู ุงูุชุงููุชูู: "technical" ุฃู "general".
# - "technical": ุฅุฐุง ูุงู ุงูุณุคุงู ูุชุทูุจ ุงูุจุญุซ ุนู ูุนูููุงุช ุฃู ุชูุงุตูู ูู ูุงุนุฏุฉ ูุนุฑูุฉ. (ูุซู: ูู ูู ุงููุดุฑูุ ูุง ูู ุงูุฑูู ุงูุฃูุงุฏูููุ ููู ุฃุญู ุงููุดููุฉ).
# - "general": ุฅุฐุง ูุงู ุงูุณุคุงู ุนุจุงุฑุฉ ุนู ุชุญูุฉุ ุณุคุงู ุนุงู ูุง ูุชุทูุจ ุจุญุซ (ูุซู "ูู ุฃูุชุ"ุ "ููู ุญุงููุ")ุ ุญุฏูุซ ุตุบูุฑุ ุฃู ุฅูุงูุฉ.

# ุฃุฌุจ ุจุตูุบุฉ JSON ููุทุ ูุน ููุชุงุญ "category".

# ุฃูุซูุฉ:
# - ุณุคุงู ุงููุณุชุฎุฏู: "ุงุดุฑุญ ูู ุฎุทูุงุช ุชุซุจูุช ุงูุจุฑูุงูุฌ." -> {{"category": "technical"}}
# - ุณุคุงู ุงููุณุชุฎุฏู: "ูู ูู ููุฏู ุฃุจู ุนููุ" -> {{"category": "technical"}}
# - ุณุคุงู ุงููุณุชุฎุฏู: "ูุฑุญุจุงู ูุง ุณุงุนุฏ" -> {{"category": "general"}}
# - ุณุคุงู ุงููุณุชุฎุฏู: "ูู ุชูููุ" -> {{"category": "general"}}

# ุณุคุงู ุงููุณุชุฎุฏู:
# {question}
# """

# # --- 2. ูุงูุจ ูุธุงู RAG ุงูุชููู ---
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต ูู **{tenant_name}**. ุงุณุชุฎุฏู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

# **ุงูุณูุงู:**
# {context}

# **ุณุคุงู ุงููุณุชุฎุฏู:**
# {question}

# **ุงูุฅุฌุงุจุฉ:**
# """

# # --- 3. ูุงูุจ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ (ูุน ุดุฎุตูุฉ ุฏููุงููููุฉ) ---
# GENERAL_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช "ุณุงุนุฏ"ุ ุงููุณุงุนุฏ ุงูุขูู ูู **{tenant_name}**. ุฃูุช ุฐูู ููุฏูุฏ. ุชูุงุนู ูุน "ุณุคุงู ุงููุณุชุฎุฏู" ุจุทุฑููุฉ ููุงุณุจุฉ ูููุฐุจุฉ.
# - ุฅุฐุง ูุงู ุงูุณุคุงู "ูู ุฃูุชุ" ุฃู ูุง ุดุงุจู: ุนุฑูู ุจููุณู: "ุฃูุง ุณุงุนุฏุ ูุณุงุนุฏ ุงูุฏุนู ุงูุขูู ูู {tenant_name}. ููู ูููููู ุฎุฏูุชูุ"
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุชุญูุฉ: ุฑุฏ ุงูุชุญูุฉ ุจูุทู. (ูุซุงู: "ูุนูููู ุงูุณูุงู! ุฃููุงู ุจู ูู ุฎุฏูุฉ ุงูุฏุนู ูู {tenant_name}.")
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุฅูุงูุฉ: ุญุงูุธ ุนูู ูุฏูุฆู ูุฑุฏ ุจุงุญุชุฑุงููุฉ: "ุฃูุง ููุง ููุณุงุนุฏุชู ูู ุฃู ุงุณุชูุณุงุฑุงุช ูุฏูู ุญูู {tenant_name}."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

# ุณุคุงู ุงููุณุชุฎุฏู:
# {question}
# """

# # -----------------------------------------------------------------------------
# # ๐ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ูุณูุงุณู ุงูุนูู
# # -----------------------------------------------------------------------------
# vector_store: FAISS = None
# llm: Ollama = None
# embeddings_model: OllamaEmbeddings = None
# all_docs_for_bm25: List[Document] = []
# cross_encoder: CrossEncoder = None
# full_rag_chain = None
# general_chain = None
# routing_chain = None

# # -----------------------------------------------------------------------------
# # ๐ ุชููุฆุฉ ุงููููู (ูุน ุฅุนุงุฏุฉ ุงูุชูุฌูู)
# # -----------------------------------------------------------------------------
# def initialize_agent():
#     global vector_store, llm, embeddings_model, all_docs_for_bm25, cross_encoder, full_rag_chain, general_chain, routing_chain
#     if routing_chain:
#         logging.info("โ ุงููููู ุงูุฐูู (ูุน ุงูุชูุฌูู) ููููุฃ ูุณุจููุง.")
#         return
    
#     try:
#         logging.info("=" * 80)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ุงููููู ุงูุฐูู (ูุน ุงูุชูุฌูู ูุงูุดุฎุตูุฉ ุงูุฏููุงููููุฉ)...")
        
#         llm = Ollama(model=CHAT_MODEL_NAME, temperature=0.1)
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         vector_store = FAISS.load_local(VECTOR_DB_PATH, embeddings=embeddings_model, allow_dangerous_deserialization=True)
#         docstore_ids = list(vector_store.docstore._dict.keys())
#         all_docs_for_bm25 = [vector_store.docstore._dict[i] for i in docstore_ids]
#         cross_encoder = CrossEncoder(RERANK_MODEL_NAME)
        
#         # --- ุจูุงุก ุงูุณูุงุณู ---
#         rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
#         full_rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: format_docs_with_source(x["docs"]))
#             | rag_prompt
#             | llm
#             | StrOutputParser()
#         )

#         general_prompt = PromptTemplate.from_template(GENERAL_PROMPT_TEMPLATE)
#         general_chain = general_prompt | llm | StrOutputParser()

#         routing_prompt = PromptTemplate.from_template(ROUTING_PROMPT_TEMPLATE)
#         routing_chain = routing_prompt | llm | JsonOutputParser()

#         logging.info(" ุงูุชููุช ุชููุฆุฉ ุงููููู ุงูุฐูู ุจูุฌุงุญ! โจ")
#     except Exception as e:
#         logging.critical(f" ูุดู ุญุงุณู ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#         raise

# # -----------------------------------------------------------------------------
# # ํฌ ุฏูุงู ูุณุงุนุฏุฉ
# # -----------------------------------------------------------------------------
# def format_docs_with_source(docs: List[Document]) -> str:
#     """ุชูุณู ุงููุณุชูุฏุงุช ุงููุณุชุฑุฌุนุฉ ูุชุถูู ุงููุตุงุฏุฑ."""
#     if not docs:
#         return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
#     sources = {doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู") for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"

# def perform_hybrid_retrieval_and_rerank(question: str, tenant_id: str, k: int) -> List[Document]:
#     """ูููุฐ ุงูุจุญุซ ุงููุฌูู ุงููุงูู ูุน ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ."""
#     faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 15, "filter": {"tenant_id": tenant_id}})
#     faiss_docs = faiss_retriever.invoke(question)
    
#     tenant_docs_indices = [i for i, doc in enumerate(all_docs_for_bm25) if doc.metadata.get("tenant_id") == tenant_id]
#     bm25_docs = []
#     if tenant_docs_indices:
#         tenant_corpus = [all_docs_for_bm25[i].page_content.split(" ") for i in tenant_docs_indices]
#         bm25_for_tenant = BM25Okapi(tenant_corpus)
#         tokenized_query = question.split(" ")
#         doc_scores = bm25_for_tenant.get_scores(tokenized_query)
#         top_n_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:15]
#         bm25_docs = [all_docs_for_bm25[tenant_docs_indices[i]] for i in top_n_indices]
    
#     combined_docs_list = list({doc.page_content: doc for doc in faiss_docs + bm25_docs}.values())
#     if not combined_docs_list:
#         return []

#     model_input_pairs = [[question, doc.page_content] for doc in combined_docs_list]
#     scores = cross_encoder.predict(model_input_pairs)
#     docs_with_scores = sorted(zip(combined_docs_list, scores), key=lambda x: x[1], reverse=True)
    
#     return [doc for doc, score in docs_with_scores[:k]]

# # -----------------------------------------------------------------------------
# # ๐ง ุจุซ ุงูุฅุฌุงุจุฉ (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ)
# # -----------------------------------------------------------------------------
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     if not routing_chain:
#         raise RuntimeError("โ๏ธ ุงููููู ุงูุฐูู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")
    
#     logging.info(f"๐ฉ ุงุณุชูุจุงู ุณุคุงู ูู '{tenant_id}': {question}")
#     try:
#         # 1. ูุฑุญูุฉ ุงูุชูุฌูู
#         perf_logger.start("routing")
#         route_decision = await routing_chain.ainvoke({"question": question})
#         category = route_decision.get("category", "technical")
#         perf_logger.end("routing", tenant_id, question, extra_info={"decision": category})
#         logging.info(f"๐ง ูุฑุงุฑ ุงูุชูุฌูู: '{category}'")

#         # 2. ุชูููุฐ ุงููุณุงุฑ
#         if category == "technical":
#             logging.info("๐ ุชูููุฐ ูุณุงุฑ ุงูุฏุนู ุงูููู (RAG)...")
#             perf_logger.start("retrieval_rerank")
#             final_docs = perform_hybrid_retrieval_and_rerank(question, tenant_id, k_results)
#             perf_logger.end("retrieval_rerank", tenant_id, question, extra_info={"final_doc_count": len(final_docs)})
            
#             # ุงุณุชูุจุงุท ุงููููุฉ ุงูุฏููุงููููุฉ ูู ุงููุณุชูุฏุงุช ุงููุณุชุฑุฌุนุฉ
#             entity_name = "ุงูุฎุฏูุฉ" # ุงุณู ุงูุชุฑุงุถู
#             if final_docs and "entity_name" in final_docs[0].metadata:
#                 entity_name = final_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")
            
#             async for chunk in full_rag_chain.astream({"question": question, "docs": final_docs, "tenant_name": entity_name}):
#                 yield chunk
#         else: # general
#             logging.info("๐ฌ ุชูููุฐ ูุณุงุฑ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ...")
            
#             # ุงุณุชูุจุงุท ุงููููุฉ ุงูุฏููุงููููุฉ ุนุจุฑ ุจุญุซ ุฎููู ุฌุฏุงู
#             temp_docs = vector_store.similarity_search("", filter={"tenant_id": tenant_id}, k=1)
#             entity_name = "ุงูุฎุฏูุฉ" # ุงุณู ุงูุชุฑุงุถู
#             if temp_docs and "entity_name" in temp_docs[0].metadata:
#                 entity_name = temp_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")

#             async for chunk in general_chain.astream({"question": question, "tenant_name": entity_name}):
#                 yield chunk
#     except Exception as e:
#         logging.error(f"โ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ: {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุนุงูุฌุฉ ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})


# core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุงุฆูุฉ ุงูุณุฑุนุฉ)
#

# # #
# import os
# import logging
# from typing import List, AsyncGenerator
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache
# from langchain_core.documents import Document
# from sentence_transformers import CrossEncoder
# from rank_bm25 import BM25Okapi
# # ๐ด๐ด๐ด --- ุงุณุชูุฑุงุฏ ุฌุฏูุฏ ูููู --- ๐ด๐ด๐ด
# from transformers import pipeline

# from .performance_tracker import PerformanceLogger

# # -----------------------------------------------------------------------------
# # ๐งฉ ุฅุนุฏุงุฏุงุช ุนุงูุฉ ูุชุณุฌูู
# # -----------------------------------------------------------------------------
# perf_logger = PerformanceLogger()
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))
# langchain.llm_cache = InMemoryCache()

# # -----------------------------------------------------------------------------
# # ๐ฆ ูุชุบูุฑุงุช ุงูุจูุฆุฉ ูุงูููุงุฐุฌ
# # -----------------------------------------------------------------------------
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))
# RERANK_MODEL_NAME = "BAAI/bge-reranker-base"
# # ๐ด๐ด๐ด --- ุงุณู ูููุฐุฌ ุงูุชุตููู ุงูุณุฑูุน --- ๐ด๐ด๐ด
# CLASSIFIER_MODEL_NAME = "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"

# # -----------------------------------------------------------------------------
# # ๐ง ููุงูุจ ุงูู Prompts (ูู ูุนุฏ ูุญุชุงุฌ ูุงูุจ ุงูุชูุฌูู)
# # -----------------------------------------------------------------------------
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต ูู **{tenant_name}**. ุงุณุชุฎุฏู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
# **ุงูุณูุงู:** {context}
# **ุณุคุงู ุงููุณุชุฎุฏู:** {question}
# **ุงูุฅุฌุงุจุฉ:**"""

# GENERAL_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช "ุณุงุนุฏ"ุ ุงููุณุงุนุฏ ุงูุขูู ูู **{tenant_name}**. ุฃูุช ุฐูู ููุฏูุฏ. ุชูุงุนู ูุน "ุณุคุงู ุงููุณุชุฎุฏู" ุจุทุฑููุฉ ููุงุณุจุฉ ูููุฐุจุฉ.
# - ุฅุฐุง ูุงู ุงูุณุคุงู "ูู ุฃูุชุ" ุฃู ูุง ุดุงุจู: ุนุฑูู ุจููุณู: "ุฃูุง ุณุงุนุฏุ ูุณุงุนุฏ ุงูุฏุนู ุงูุขูู ูู {tenant_name}. ููู ูููููู ุฎุฏูุชูุ"
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุชุญูุฉ: ุฑุฏ ุงูุชุญูุฉ ุจูุทู. (ูุซุงู: "ูุนูููู ุงูุณูุงู! ุฃููุงู ุจู ูู ุฎุฏูุฉ ุงูุฏุนู ูู {tenant_name}.")
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุฅูุงูุฉ: ุญุงูุธ ุนูู ูุฏูุฆู ูุฑุฏ ุจุงุญุชุฑุงููุฉ: "ุฃูุง ููุง ููุณุงุนุฏุชู ูู ุฃู ุงุณุชูุณุงุฑุงุช ูุฏูู ุญูู {tenant_name}."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
# **ุณุคุงู ุงููุณุชุฎุฏู:** {question}
# """

# # -----------------------------------------------------------------------------
# # ๐ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ
# # -----------------------------------------------------------------------------
# vector_store: FAISS = None
# llm: Ollama = None
# embeddings_model: OllamaEmbeddings = None
# all_docs_for_bm25: List[Document] = []
# cross_encoder: CrossEncoder = None
# full_rag_chain = None
# general_chain = None
# # ๐ด๐ด๐ด --- ุชู ุงุณุชุจุฏุงู routing_chain ุจู classifier --- ๐ด๐ด๐ด
# classifier = None

# # -----------------------------------------------------------------------------
# # ๐ ุชููุฆุฉ ุงููููู (ูุน ุงููุตูู ุงูุณุฑูุน)
# # -----------------------------------------------------------------------------
# def initialize_agent():
#     global vector_store, llm, embeddings_model, all_docs_for_bm25, cross_encoder, full_rag_chain, general_chain, classifier
#     if classifier:
#         logging.info("โ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ููููุฃ ูุณุจููุง.")
#         return
    
#     try:
#         logging.info("=" * 80)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ (ูุน ูุตูู ูุฎุตุต)...")
        
#         # ุชุญููู ุงูููููุงุช ุงูุฃุณุงุณูุฉ
#         llm = Ollama(model=CHAT_MODEL_NAME, temperature=0.1)
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         vector_store = FAISS.load_local(VECTOR_DB_PATH, embeddings=embeddings_model, allow_dangerous_deserialization=True)
#         docstore_ids = list(vector_store.docstore._dict.keys())
#         all_docs_for_bm25 = [vector_store.docstore._dict[i] for i in docstore_ids]
#         cross_encoder = CrossEncoder(RERANK_MODEL_NAME)
        
#         # ๐ด๐ด๐ด --- ุชููุฆุฉ ุงููุตูู ุงูุณุฑูุน --- ๐ด๐ด๐ด
#         logging.info(f"[*] ุฌุงุฑู ุชุญููู ูููุฐุฌ ุงูุชุตููู ุงูุณุฑูุน: '{CLASSIFIER_MODEL_NAME}'...")
#         classifier = pipeline("zero-shot-classification", model=CLASSIFIER_MODEL_NAME)
#         logging.info("[*] ุชู ุชุญููู ุงููุตูู ุจูุฌุงุญ.")

#         # ุจูุงุก ุงูุณูุงุณู
#         rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
#         full_rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: format_docs_with_source(x["docs"]))
#             | rag_prompt
#             | llm
#             | StrOutputParser()
#         )

#         general_prompt = PromptTemplate.from_template(GENERAL_PROMPT_TEMPLATE)
#         general_chain = general_prompt | llm | StrOutputParser()

#         logging.info("โจ ุงูุชููุช ุชููุฆุฉ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ุจูุฌุงุญ! โจ")
#     except Exception as e:
#         logging.critical(f"โ ูุดู ุญุงุณู ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#         raise

# # -----------------------------------------------------------------------------
# # ํฌ ุฏูุงู ูุณุงุนุฏุฉ (ุจุฏูู ุชุบููุฑ)
# # -----------------------------------------------------------------------------
# def format_docs_with_source(docs: List[Document]) -> str:
#     # ... (ููุณ ุงูููุฏ)
#     if not docs: return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
#     sources = {doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู") for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"

# def perform_hybrid_retrieval_and_rerank(question: str, tenant_id: str, k: int) -> List[Document]:
#     # ... (ููุณ ุงูููุฏ)
#     faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 15, "filter": {"tenant_id": tenant_id}})
#     faiss_docs = faiss_retriever.invoke(question)
#     tenant_docs_indices = [i for i, doc in enumerate(all_docs_for_bm25) if doc.metadata.get("tenant_id") == tenant_id]
#     bm25_docs = []
#     if tenant_docs_indices:
#         tenant_corpus = [all_docs_for_bm25[i].page_content.split(" ") for i in tenant_docs_indices]
#         bm25_for_tenant = BM25Okapi(tenant_corpus)
#         tokenized_query = question.split(" ")
#         doc_scores = bm25_for_tenant.get_scores(tokenized_query)
#         top_n_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:15]
#         bm25_docs = [all_docs_for_bm25[tenant_docs_indices[i]] for i in top_n_indices]
#     combined_docs_list = list({doc.page_content: doc for doc in faiss_docs + bm25_docs}.values())
#     if not combined_docs_list: return []
#     model_input_pairs = [[question, doc.page_content] for doc in combined_docs_list]
#     scores = cross_encoder.predict(model_input_pairs)
#     docs_with_scores = sorted(zip(combined_docs_list, scores), key=lambda x: x[1], reverse=True)
#     return [doc for doc, score in docs_with_scores[:k]]

# # -----------------------------------------------------------------------------
# # ๐ง ุจุซ ุงูุฅุฌุงุจุฉ (ุงููุณุฎุฉ ูุงุฆูุฉ ุงูุณุฑุนุฉ)
# # -----------------------------------------------------------------------------
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     if not classifier:
#         raise RuntimeError("โ๏ธ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ุบูุฑ ููููุฃ.")
    
#     logging.info(f"๐ฉ ุงุณุชูุจุงู ุณุคุงู ูู '{tenant_id}': {question}")
#     try:
#         # ๐ด๐ด๐ด --- 1. ูุฑุญูุฉ ุงูุชูุฌูู ูุงุฆูุฉ ุงูุณุฑุนุฉ --- ๐ด๐ด๐ด
#         perf_logger.start("routing")
#         candidate_labels = ["ุณุคุงู ุชููู", "ูุญุงุฏุซุฉ ุนุงูุฉ"]
#         # ููุงุญุธุฉ: ูุง ูุณุชุฎุฏู ainvoke ููุง ูุฃู pipeline ูุง ุชุฏุนููุง ุงูุชุฑุงุถููุง
#         result = classifier(question, candidate_labels, multi_label=False)
#         # ุฃุนูู ุชุตููู ูู ุงููุฑุงุฑ
#         decision = result['labels'][0]
#         category = "technical" if decision == "ุณุคุงู ุชููู" else "general"
#         perf_logger.end("routing", tenant_id, question, extra_info={"decision": category, "score": result['scores'][0]})
#         logging.info(f"๐ง ูุฑุงุฑ ุงูุชูุฌูู ูุงุฆู ุงูุณุฑุนุฉ: '{category}' (ุจุซูุฉ: {result['scores'][0]:.2f})")

#         # 2. ุชูููุฐ ุงููุณุงุฑ (ููุณ ุงูููุทู ุงูุณุงุจู)
#         if category == "technical":
#             logging.info("๐ ุชูููุฐ ูุณุงุฑ ุงูุฏุนู ุงูููู (RAG)...")
#             perf_logger.start("retrieval_rerank")
#             final_docs = perform_hybrid_retrieval_and_rerank(question, tenant_id, k_results)
#             perf_logger.end("retrieval_rerank", tenant_id, question, extra_info={"final_doc_count": len(final_docs)})
            
#             entity_name = "ุงูุฎุฏูุฉ"
#             if final_docs and "entity_name" in final_docs[0].metadata:
#                 entity_name = final_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")
            
#             async for chunk in full_rag_chain.astream({"question": question, "docs": final_docs, "tenant_name": entity_name}):
#                 yield chunk
#         else: # general
#             logging.info("๐ฌ ุชูููุฐ ูุณุงุฑ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ...")
#             temp_docs = vector_store.similarity_search("", filter={"tenant_id": tenant_id}, k=1)
#             entity_name = "ุงูุฎุฏูุฉ"
#             if temp_docs and "entity_name" in temp_docs[0].metadata:
#                 entity_name = temp_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")

#             async for chunk in general_chain.astream({"question": question, "tenant_name": entity_name}):
#                 yield chunk
#     except Exception as e:
#         logging.error(f"โ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ: {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุนุงูุฌุฉ ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})

# # core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุงุฆูุฉ ุงูุณุฑุนุฉ)

# import os
# import logging
# from typing import List, AsyncGenerator
# from langchain_core.prompts import PromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.llms import Ollama
# from dotenv import load_dotenv
# import langchain
# from langchain_core.caches import InMemoryCache
# from langchain_core.documents import Document
# from sentence_transformers import CrossEncoder
# from rank_bm25 import BM25Okapi
# # ๐ด๐ด๐ด --- ุงุณุชูุฑุงุฏ ุฌุฏูุฏ ูููู --- ๐ด๐ด๐ด
# from transformers import pipeline

# from .performance_tracker import PerformanceLogger

# # -----------------------------------------------------------------------------
# # ๐งฉ ุฅุนุฏุงุฏุงุช ุนุงูุฉ ูุชุณุฌูู
# # -----------------------------------------------------------------------------
# perf_logger = PerformanceLogger()
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# load_dotenv(dotenv_path=os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.env")))
# langchain.llm_cache = InMemoryCache()

# # -----------------------------------------------------------------------------
# # ๐ฆ ูุชุบูุฑุงุช ุงูุจูุฆุฉ ูุงูููุงุฐุฌ
# # -----------------------------------------------------------------------------
# EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
# CHAT_MODEL_NAME = os.getenv("CHAT_MODEL_NAME")
# VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))
# RERANK_MODEL_NAME = "BAAI/bge-reranker-base"
# # ๐ด๐ด๐ด --- ุงุณู ูููุฐุฌ ุงูุชุตููู ุงูุณุฑูุน --- ๐ด๐ด๐ด
# CLASSIFIER_MODEL_NAME = "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"

# # -----------------------------------------------------------------------------
# # ๐ง ููุงูุจ ุงูู Prompts (ูู ูุนุฏ ูุญุชุงุฌ ูุงูุจ ุงูุชูุฌูู)
# # -----------------------------------------------------------------------------
# RAG_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช ูุณุงุนุฏ ุฏุนู ููู ุฎุจูุฑ ููุฎุชุต ูู **{tenant_name}**. ุงุณุชุฎุฏู "ุงูุณูุงู" ุงูุชุงูู ููุฅุฌุงุจุฉ ุนูู "ุณุคุงู ุงููุณุชุฎุฏู" ุจุฏูุฉ.
# - ุฅุฐุง ูุงูุช ุงููุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุณูุงูุ ุฃุฌุจ ุจู "ุฃูุง ุขุณูุ ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ ุนูู ูุฐุง ุงูุณุคุงู."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
# **ุงูุณูุงู:** {context}
# **ุณุคุงู ุงููุณุชุฎุฏู:** {question}
# **ุงูุฅุฌุงุจุฉ:**"""

# GENERAL_PROMPT_TEMPLATE = """
# **ูููุชู:** ุฃูุช "ุณุงุนุฏ"ุ ุงููุณุงุนุฏ ุงูุขูู ูู **{tenant_name}**. ุฃูุช ุฐูู ููุฏูุฏ. ุชูุงุนู ูุน "ุณุคุงู ุงููุณุชุฎุฏู" ุจุทุฑููุฉ ููุงุณุจุฉ ูููุฐุจุฉ.
# - ุฅุฐุง ูุงู ุงูุณุคุงู "ูู ุฃูุชุ" ุฃู ูุง ุดุงุจู: ุนุฑูู ุจููุณู: "ุฃูุง ุณุงุนุฏุ ูุณุงุนุฏ ุงูุฏุนู ุงูุขูู ูู {tenant_name}. ููู ูููููู ุฎุฏูุชูุ"
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุชุญูุฉ: ุฑุฏ ุงูุชุญูุฉ ุจูุทู. (ูุซุงู: "ูุนูููู ุงูุณูุงู! ุฃููุงู ุจู ูู ุฎุฏูุฉ ุงูุฏุนู ูู {tenant_name}.")
# - ุฅุฐุง ูุงู ุงูุณุคุงู ุฅูุงูุฉ: ุญุงูุธ ุนูู ูุฏูุฆู ูุฑุฏ ุจุงุญุชุฑุงููุฉ: "ุฃูุง ููุง ููุณุงุนุฏุชู ูู ุฃู ุงุณุชูุณุงุฑุงุช ูุฏูู ุญูู {tenant_name}."
# - ุฃุฌุจ ุฏุงุฆููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
# **ุณุคุงู ุงููุณุชุฎุฏู:** {question}
# """

# # -----------------------------------------------------------------------------
# # ๐ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ
# # -----------------------------------------------------------------------------
# vector_store: FAISS = None
# llm: Ollama = None
# embeddings_model: OllamaEmbeddings = None
# all_docs_for_bm25: List[Document] = []
# cross_encoder: CrossEncoder = None
# full_rag_chain = None
# general_chain = None
# # ๐ด๐ด๐ด --- ุชู ุงุณุชุจุฏุงู routing_chain ุจู classifier --- ๐ด๐ด๐ด
# classifier = None

# # -----------------------------------------------------------------------------
# # ๐ ุชููุฆุฉ ุงููููู (ูุน ุงููุตูู ุงูุณุฑูุน)
# # -----------------------------------------------------------------------------
# def initialize_agent():
#     global vector_store, llm, embeddings_model, all_docs_for_bm25, cross_encoder, full_rag_chain, general_chain, classifier
#     if classifier:
#         logging.info("โ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ููููุฃ ูุณุจููุง.")
#         return
    
#     try:
#         logging.info("=" * 80)
#         logging.info("๐ ุจุฏุก ุชููุฆุฉ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ (ูุน ูุตูู ูุฎุตุต)...")
        
#         # ุชุญููู ุงูููููุงุช ุงูุฃุณุงุณูุฉ
#         llm = Ollama(model=CHAT_MODEL_NAME, temperature=0.1)
#         embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
#         vector_store = FAISS.load_local(VECTOR_DB_PATH, embeddings=embeddings_model, allow_dangerous_deserialization=True)
#         docstore_ids = list(vector_store.docstore._dict.keys())
#         all_docs_for_bm25 = [vector_store.docstore._dict[i] for i in docstore_ids]
#         cross_encoder = CrossEncoder(RERANK_MODEL_NAME)
        
#         # ๐ด๐ด๐ด --- ุชููุฆุฉ ุงููุตูู ุงูุณุฑูุน --- ๐ด๐ด๐ด
#         logging.info(f"[*] ุฌุงุฑู ุชุญููู ูููุฐุฌ ุงูุชุตููู ุงูุณุฑูุน: '{CLASSIFIER_MODEL_NAME}'...")
#         classifier = pipeline("zero-shot-classification", model=CLASSIFIER_MODEL_NAME)
#         logging.info("[*] ุชู ุชุญููู ุงููุตูู ุจูุฌุงุญ.")

#         # ุจูุงุก ุงูุณูุงุณู
#         rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
#         full_rag_chain = (
#             RunnablePassthrough.assign(context=lambda x: format_docs_with_source(x["docs"]))
#             | rag_prompt
#             | llm
#             | StrOutputParser()
#         )

#         general_prompt = PromptTemplate.from_template(GENERAL_PROMPT_TEMPLATE)
#         general_chain = general_prompt | llm | StrOutputParser()

#         logging.info("โจ ุงูุชููุช ุชููุฆุฉ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ุจูุฌุงุญ! โจ")
#     except Exception as e:
#         logging.critical(f"โ ูุดู ุญุงุณู ุฃุซูุงุก ุงูุชููุฆุฉ: {e}", exc_info=True)
#         raise

# # -----------------------------------------------------------------------------
# # ํฌ ุฏูุงู ูุณุงุนุฏุฉ (ุจุฏูู ุชุบููุฑ)
# # -----------------------------------------------------------------------------
# def format_docs_with_source(docs: List[Document]) -> str:
#     # ... (ููุณ ุงูููุฏ)
#     if not docs: return "ูุง ููุฌุฏ ุณูุงู ูุชููุฑ."
#     sources = {doc.metadata.get("source", "ูุตุฏุฑ ุบูุฑ ูุนุฑูู") for doc in docs}
#     formatted_docs = "\n\n---\n\n".join(doc.page_content for doc in docs)
#     return f"ุงููุนูููุงุช ุงูุชุงููุฉ ุชู ุงุณุชุฑุฌุงุนูุง ูู ุงููุตุงุฏุฑ: {', '.join(sources)}\n\n{formatted_docs}"

# def perform_hybrid_retrieval_and_rerank(question: str, tenant_id: str, k: int) -> List[Document]:
#     # ... (ููุณ ุงูููุฏ)
#     faiss_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 15, "filter": {"tenant_id": tenant_id}})
#     faiss_docs = faiss_retriever.invoke(question)
#     tenant_docs_indices = [i for i, doc in enumerate(all_docs_for_bm25) if doc.metadata.get("tenant_id") == tenant_id]
#     bm25_docs = []
#     if tenant_docs_indices:
#         tenant_corpus = [all_docs_for_bm25[i].page_content.split(" ") for i in tenant_docs_indices]
#         bm25_for_tenant = BM25Okapi(tenant_corpus)
#         tokenized_query = question.split(" ")
#         doc_scores = bm25_for_tenant.get_scores(tokenized_query)
#         top_n_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:15]
#         bm25_docs = [all_docs_for_bm25[tenant_docs_indices[i]] for i in top_n_indices]
#     combined_docs_list = list({doc.page_content: doc for doc in faiss_docs + bm25_docs}.values())
#     if not combined_docs_list: return []
#     model_input_pairs = [[question, doc.page_content] for doc in combined_docs_list]
#     scores = cross_encoder.predict(model_input_pairs)
#     docs_with_scores = sorted(zip(combined_docs_list, scores), key=lambda x: x[1], reverse=True)
#     return [doc for doc, score in docs_with_scores[:k]]

# # -----------------------------------------------------------------------------
# # ๐ง ุจุซ ุงูุฅุฌุงุจุฉ (ุงููุณุฎุฉ ูุงุฆูุฉ ุงูุณุฑุนุฉ)
# # -----------------------------------------------------------------------------
# async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
#     if not classifier:
#         raise RuntimeError("โ๏ธ ุงููููู ูุงุฆู ุงูุณุฑุนุฉ ุบูุฑ ููููุฃ.")
    
#     logging.info(f"๐ฉ ุงุณุชูุจุงู ุณุคุงู ูู '{tenant_id}': {question}")
#     try:
#         # ๐ด๐ด๐ด --- 1. ูุฑุญูุฉ ุงูุชูุฌูู ูุงุฆูุฉ ุงูุณุฑุนุฉ --- ๐ด๐ด๐ด
#         perf_logger.start("routing")
#         candidate_labels = ["ุณุคุงู ุชููู", "ูุญุงุฏุซุฉ ุนุงูุฉ"]
#         # ููุงุญุธุฉ: ูุง ูุณุชุฎุฏู ainvoke ููุง ูุฃู pipeline ูุง ุชุฏุนููุง ุงูุชุฑุงุถููุง
#         result = classifier(question, candidate_labels, multi_label=False)
#         # ุฃุนูู ุชุตููู ูู ุงููุฑุงุฑ
#         decision = result['labels'][0]
#         category = "technical" if decision == "ุณุคุงู ุชููู" else "general"
#         perf_logger.end("routing", tenant_id, question, extra_info={"decision": category, "score": result['scores'][0]})
#         logging.info(f"๐ง ูุฑุงุฑ ุงูุชูุฌูู ูุงุฆู ุงูุณุฑุนุฉ: '{category}' (ุจุซูุฉ: {result['scores'][0]:.2f})")

#         # 2. ุชูููุฐ ุงููุณุงุฑ (ููุณ ุงูููุทู ุงูุณุงุจู)
#         if category == "technical":
#             logging.info("๐ ุชูููุฐ ูุณุงุฑ ุงูุฏุนู ุงูููู (RAG)...")
#             perf_logger.start("retrieval_rerank")
#             final_docs = perform_hybrid_retrieval_and_rerank(question, tenant_id, k_results)
#             perf_logger.end("retrieval_rerank", tenant_id, question, extra_info={"final_doc_count": len(final_docs)})
            
#             entity_name = "ุงูุฎุฏูุฉ"
#             if final_docs and "entity_name" in final_docs[0].metadata:
#                 entity_name = final_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")
            
#             async for chunk in full_rag_chain.astream({"question": question, "docs": final_docs, "tenant_name": entity_name}):
#                 yield chunk
#         else: # general
#             logging.info("๐ฌ ุชูููุฐ ูุณุงุฑ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ...")
#             temp_docs = vector_store.similarity_search("", filter={"tenant_id": tenant_id}, k=1)
#             entity_name = "ุงูุฎุฏูุฉ"
#             if temp_docs and "entity_name" in temp_docs[0].metadata:
#                 entity_name = temp_docs[0].metadata["entity_name"]
#             logging.info(f"๐ข ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุณุชูุจุทุฉ: '{entity_name}'")

#             async for chunk in general_chain.astream({"question": question, "tenant_name": entity_name}):
#                 yield chunk
#     except Exception as e:
#         logging.error(f"โ ุฎุทุฃ ุฃุซูุงุก ุจุซ ุงูุฅุฌุงุจุฉ: {e}", exc_info=True)
#         yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู ุฃุซูุงุก ูุนุงูุฌุฉ ุณุคุงูู."
#         perf_logger.end("error", tenant_id, question, extra_info={"error": str(e)})

# /2_central_api_service/agent_app/core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ุชุญุณูู ุงููุตูู)

# /2_central_api_service/agent_app/core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ูุน ูููุฉ ุงูุฏุนู ุงูููู ุงููุชุฎุตุต)
# /2_central_api_service/agent_app/core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ุงููุงููุฉ)

import os
import logging
from typing import List, AsyncGenerator, Dict, Any, Literal

# --- ุงุณุชูุฑุงุฏ ููุชุจุงุช LangChain ูุงููุฌุชูุน ---
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.caches import InMemoryCache
import langchain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama

# --- ุงุณุชูุฑุงุฏ ููุชุจุงุช ุงูุจุญุซ ูุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ---
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
from transformers import pipeline

# --- ุงุณุชูุฑุงุฏ ุงููุญุฏุงุช ุงููุญููุฉ ---
from .performance_tracker import PerformanceLogger

# =================================================================================
# 1. ุงูุฅุนุฏุงุฏุงุช ุงูุฃูููุฉ ูุงูุฃุณุงุณูุฉ (Configuration & Setup)
# =================================================================================

# --- ุฅุนุฏุงุฏ ูุธุงู ุงูุชุณุฌูู (Logging) ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s"
)

# --- ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ูุชุญุณูู ุงูุฃุฏุงุก ---
langchain.llm_cache = InMemoryCache()
logging.info("ุชู ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (InMemoryCache) ูู LangChain.")

# --- ุชุญููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ---
from dotenv import load_dotenv
load_dotenv()

# --- ุชุนุฑูู ุงูุซูุงุจุช ูููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ---
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "default_embedding_model")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "default_chat_model")
RERANK_MODEL = "BAAI/bge-reranker-base"
CLASSIFIER_MODEL = "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"
VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# --- ุฅุนุฏุงุฏ ูุณุฌู ุงูุฃุฏุงุก ---
perf_logger = PerformanceLogger()

# =================================================================================
# 2. ููุงูุจ ุงูุชูุฌูู (Prompts) ุงููุญุณููุฉ ููููุฉ ุงูุฏุนู ุงูููู
# =================================================================================

# --- ูุงูุจ ุงูุฅุฌุงุจุฉ ุงููููุฉ (RAG) ---
RAG_PROMPT_TEMPLATE = """
### ุงููููุฉ ุงูุฃุณุงุณูุฉ ###
ุฃูุช "ุณุงุนุฏ"ุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุฐูู ูุงููุชุฎุตุต ูู ูุธุงู **{tenant_name}**. ูููุชู ูู ุชุญููู ุณุคุงู ุงููุณุชุฎุฏู ูุชูุฏูู ุญููู ูุฅุฌุงุจุงุช ุฏูููุฉ ุจุงูุงุนุชูุงุฏ **ููุท** ุนูู ุงููุนูููุงุช ุงูุชูููุฉ ุงููุชููุฑุฉ ูู ูุณู "ุงูุณูุงู".

### ููุงุนุฏ ุตุงุฑูุฉ ###
1.  **ุงูุงูุชุฒุงู ุจุงูุณูุงู:** ูุง ุชุณุชุฎุฏู ุฃู ูุนูููุงุช ุฎุงุฑุฌ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุชูููุฉ ุงููุชุงุญุฉ ูู ุงูุณูุงู.
2.  **ุญู ุงููุดุงูู:** ุฑูุฒ ุนูู ุชูุฏูู ุฎุทูุงุช ุนูููุฉุ ุฅุฑุดุงุฏุงุชุ ุฃู ุชูุณูุฑุงุช ุชูููุฉ ุชุณุงุนุฏ ุงููุณุชุฎุฏู ุนูู ุญู ูุดููุชู ุฃู ููู ุงููุธุงู.
3.  **ุนุฏู ูุฌูุฏ ูุนูููุงุช:** ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ุบูุฑ ููุฌูุฏุฉุ ุฃุฌุจ ุญุตุฑููุง: "ุนููุงูุ ูุง ุฃููู ุงููุนูููุงุช ุงููุงููุฉ ุญูู ูุฐู ุงูุฌุฒุฆูุฉ ูู ูุธุงู {tenant_name}. ูู ููููู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงูุ ุฃู ูู ุชูุฏ ุชูุฌููู ูุฎูุงุฑุงุช ุฏุนู ูุชูุฏูุฉุ"
4.  **ุงููุบุฉ:** ุฃุฌุจ ุฏุงุฆููุง ุจูุบุฉ ุนุฑุจูุฉ ูุงุถุญุฉ ูููุฌูุฉ ูููุณุชุฎุฏู ุงูุชููู.

### ุงูุณูุงู (ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุชูููุฉ ูููุธุงู) ###
{context}

### ุณุคุงู ุงููุณุชุฎุฏู ###
{question}

### ุงูุฅุฌุงุจุฉ ุงููููุฉ ###
"""

# --- ูุงูุจ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ ูุชุนุฑูู ุงููููุฉ ---
GENERAL_PROMPT_TEMPLATE = """
### ุงููููุฉ ุงูุฃุณุงุณูุฉ ###
ุฃูุช "ุณุงุนุฏ"ุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุขูู ููุธุงู **{tenant_name}**. ูููุชู ูู ุงูุชูุงุนู ุจุงุญุชุฑุงููุฉ ูุชูุฌูู ุงููุณุชุฎุฏู ูุญู ุทุฑุญ ุงุณุชูุณุงุฑุงุชู ุงููููุฉ.

### ููุงุนุฏ ุงูุชูุงุนู ###
- **ุงูุชุนุฑูู ุจุงููููุฉ:** ุฅุฐุง ุณูุฆูุช "ูู ุฃูุชุ" ุฃู ูุง ุดุงุจูุ ุฃุฌุจ: "ุฃูุง ุณุงุนุฏุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุฐูู ููุธุงู {tenant_name}. ุฃูุง ููุง ููุณุงุนุฏุชู ูู ุญู ุงููุดุงูู ูุงูุฅุฌุงุจุฉ ุนูู ุงุณุชูุณุงุฑุงุชู ุงูุชูููุฉ ุงููุชุนููุฉ ุจุงููุธุงู."
- **ุงูุชุญูุฉ:** ุฑุฏ ุนูู ุงูุชุญูุงุช ุจุดูู ุงุญุชุฑุงูู ููุจุงุดุฑุ ูุซู: "ุฃููุงู ุจู ูู ุฎุฏูุฉ ุงูุฏุนู ุงูููู ููุธุงู {tenant_name}. ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
- **ุงูุฃุณุฆูุฉ ุฎุงุฑุฌ ุงููุทุงู:** ุฅุฐุง ูุงู ุงูุณุคุงู ุนุงููุง ุฌุฏูุง ููุง ูุชุนูู ุจุงูุฏุนู ุงููููุ ูุฌู ุงููุณุชุฎุฏู ุจูุทู: "ูููุชู ุงูุฃุณุงุณูุฉ ูู ุชูุฏูู ุงูุฏุนู ุงูููู ููุธุงู {tenant_name}. ูู ูุฏูู ุงุณุชูุณุงุฑ ุชููู ุฃู ูุดููุฉ ุชูุงุฌูู ุฏุงุฎู ุงููุธุงูุ"
- **ุงูุชุนุงูู ูุน ุงูุฅุณุงุกุฉ ุฃู ุงูููุงู ุบูุฑ ุงูููููู:** ุฅุฐุง ูุงู ุงูุฅุฏุฎุงู ุนุจุงุฑุฉ ุนู ุฅูุงูุฉ ุฃู ููุงู ุบูุฑ ูุชุฑุงุจุทุ ุฃุฌุจ ุจุงุญุชุฑุงููุฉ ููุฏูุก: "ุฃูุง ููุง ูุชูุฏูู ุงููุณุงุนุฏุฉ ุงููููุฉ. ูุฑุฌู ุทุฑุญ ุงุณุชูุณุงุฑู ุจูุถูุญ ุญุชู ุฃุชููู ูู ูุณุงุนุฏุชู."
- **ุงููุบุฉ:** ุงุณุชุฎุฏู ุงููุบุฉ ุงูุนุฑุจูุฉ ุงูุฑุณููุฉ ุฏุงุฆููุง.

### ุณุคุงู ุงููุณุชุฎุฏู ###
{question}

### ุงูุฅุฌุงุจุฉ ###
"""

# --- ูุงูุจ ุชูุฌูู ุงููุณุชุฎุฏู ุนูุฏ ุงูุญุงุฌุฉ ูููุณุงุนุฏุฉ ---
FALLBACK_PROMPT_TEMPLATE = """
ุนููุงูุ ูู ุฃุชููู ูู ุงูุนุซูุฑ ุนูู ุฅุฌุงุจุฉ ุฏูููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ.

**ุฎูุงุฑุงุช ุงููุณุงุนุฏุฉ:**
1.  **ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู:** ูุฏ ูุณุงุนุฏ ุงุณุชุฎุฏุงู ูููุงุช ูุฎุชููุฉ ูู ุงูุนุซูุฑ ุนูู ุงูุฅุฌุงุจุฉ.
2.  **ุฒูุงุฑุฉ ูุฑูุฒ ุงููุณุงุนุฏุฉ:** ููููู ุชุตูุญ ุงูุชูุซููุงุช ุงููุงููุฉ ุนุจุฑ ุงูุฑุงุจุท ุงูุชุงูู: [ุฃุฏุฎู ุฑุงุจุท ุงูุชูุซููุงุช ููุง]
3.  **ุงูุชูุงุตู ูุน ุงูุฏุนู ุงูููู:** ุฅุฐุง ุงุณุชูุฑุช ุงููุดููุฉุ ููููู ุงูุชูุงุตู ูุจุงุดุฑุฉ ูุน ูุฑูู ุงูุฏุนู ุงูุจุดุฑู.

ูู ุชูุฏ ุชุฌุฑุจุฉ ุฎูุงุฑ ุขุฎุฑุ
"""

# =================================================================================
# 3. ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ูุณูุงุณู ุงูุนูู (Global State & Chains)
# =================================================================================

vector_store: FAISS | None = None
llm: Ollama | None = None
embeddings_model: OllamaEmbeddings | None = None
cross_encoder: CrossEncoder | None = None
classifier: Any | None = None
all_docs_for_bm25: List[Document] = []
rag_chain: Any = None
general_chain: Any = None
fallback_chain: Any = None

# =================================================================================
# 4. ุฏุงูุฉ ุงูุชููุฆุฉ ุงูุดุงููุฉ (Initialization Function)
# =================================================================================

def initialize_agent():
    """
    ุชููู ุจุชููุฆุฉ ุฌููุน ููููุงุช ุงููููู (ุงูููุงุฐุฌุ ููุงุนุฏ ุงูุจูุงูุงุชุ ุงูุณูุงุณู) ูุฑุฉ ูุงุญุฏุฉ ุนูุฏ ุจุฏุก ุงูุชุดุบูู.
    """
    global vector_store, llm, embeddings_model, cross_encoder, classifier, all_docs_for_bm25
    global rag_chain, general_chain, fallback_chain

    if rag_chain:
        logging.info("ุงููููู ููููุฃ ุจุงููุนู ูุฌุงูุฒ ููุนูู.")
        return

    logging.info("ุจุฏุก ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู ุงูุฐูู...")

    try:
        logging.info(f"ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ: {CHAT_MODEL}")
        llm = Ollama(model=CHAT_MODEL, temperature=0.1)
        
        logging.info(f"ุชุญููู ูููุฐุฌ ุงูุชุถููู: {EMBEDDING_MODEL}")
        embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL)

        logging.info(f"ุชุญููู ูููุฐุฌ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ: {RERANK_MODEL}")
        cross_encoder = CrossEncoder(RERANK_MODEL)

        logging.info(f"ุชุญููู ูุตูู ุงูุฃุณุฆูุฉ: {CLASSIFIER_MODEL}")
        classifier = pipeline("zero-shot-classification", model=CLASSIFIER_MODEL)

        if not os.path.exists(VECTOR_DB_PATH):
            logging.error(f"ุฎุทุฃ ูุงุฏุญ: ูุฌูุฏ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุบูุฑ ููุฌูุฏ ูู ุงููุณุงุฑ: {VECTOR_DB_PATH}")
            raise FileNotFoundError("ูุฌูุฏ ูุงุนุฏุฉ ุงููุนุฑูุฉ ููููุฏ.")
        
        logging.info(f"ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู: {VECTOR_DB_PATH}")
        vector_store = FAISS.load_local(
            VECTOR_DB_PATH,
            embeddings=embeddings_model,
            allow_dangerous_deserialization=True
        )
        all_docs_for_bm25 = list(vector_store.docstore._dict.values())
        logging.info(f"ุชู ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ ({len(all_docs_for_bm25)} ูุณุชูุฏ).")

        logging.info("ุจูุงุก ุณูุงุณู ุงูุนูู ุงูููุทููุฉ...")
        
        rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        rag_chain = (
            RunnablePassthrough.assign(context=lambda x: _format_docs(x["docs"]))
            | rag_prompt
            | llm
            | StrOutputParser()
        )

        general_prompt = PromptTemplate.from_template(GENERAL_PROMPT_TEMPLATE)
        general_chain = general_prompt | llm | StrOutputParser()

        fallback_prompt = PromptTemplate.from_template(FALLBACK_PROMPT_TEMPLATE)
        fallback_chain = fallback_prompt | llm | StrOutputParser()

        logging.info("ุงูุชููุช ุชููุฆุฉ ุงููููู ุจูุฌุงุญ ููู ุงูุขู ุฌุงูุฒ ูุงุณุชูุจุงู ุงูุทูุจุงุช.")

    except Exception as e:
        logging.critical(f"ูุดู ุญุงุณู ุฃุซูุงุก ุชููุฆุฉ ุงููููู: {e}", exc_info=True)
        raise

# =================================================================================
# 5. ุงูุฏูุงู ุงููุณุงุนุฏุฉ ูุงูููุทู ุงูุฏุงุฎูู (Helper & Logic Functions)
# =================================================================================

def _format_docs(docs: List[Document]) -> str:
    """ุชูุณู ุงููุณุชูุฏุงุช ุงููุณุชุฑุฌุนุฉ ูุชูุฏูููุง ูู "ุณูุงู" ูููููุฐุฌ ุงููุบูู."""
    if not docs:
        return "ูุง ุชูุฌุฏ ูุนูููุงุช ูุชุงุญุฉ."
    return "\n\n---\n\n".join([doc.page_content for doc in docs])

def _get_dynamic_identity(tenant_id: str) -> str:
    """ุชุณุชูุจุท ุงุณู ุงููุธุงู (ุงููููุฉ ุงูุฏููุงููููุฉ) ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ."""
    if not vector_store: return "ุงููุธุงู ุงูุญุงูู"
    docs = vector_store.similarity_search("", filter={"tenant_id": tenant_id}, k=1)
    if docs and "entity_name" in docs[0].metadata:
        return docs[0].metadata["entity_name"]
    return "ุงููุธุงู ุงูุญุงูู"

def _classify_question(question: str) -> Literal["technical", "general", "inappropriate"]:
    """
    ูุณุชุฎุฏู ูุตูููุง ุณุฑูุนูุง ูุชุญุฏูุฏ ููุฉ ุงููุณุชุฎุฏู ุฅูู ุซูุงุซ ูุฆุงุช.
    """
    if not classifier: raise RuntimeError("ุงููุตูู ุบูุฑ ูููุฃ.")
    
    perf_logger.start("routing")
    
    labels = [
        "ุณุคุงู ููู ุฃู ุงุณุชูุณุงุฑ ุนู ูุนูููุงุช ูุญุฏุฏุฉ", 
        "ุชุญูุฉุ ุดูุฑุ ุฃู ุณุคุงู ุนุงู ุนู ุงููููุฉ ูุซู ูู ุฃูุช",
        "ุฅูุงูุฉุ ููุงู ุจุฐูุกุ ุฃู ุนุจุงุฑุงุช ุนุดูุงุฆูุฉ ุบูุฑ ูููููุฉ"
    ]
    
    result = classifier(question, labels, multi_label=False)
    
    top_label = result['labels'][0]
    decision: Literal["technical", "general", "inappropriate"]
    if top_label == labels[0]:
        decision = "technical"
    elif top_label == labels[1]:
        decision = "general"
    else:
        decision = "inappropriate"
    
    perf_logger.end("routing", "N/A", question, {"decision": decision, "confidence": result['scores'][0]})
    logging.info(f"ูุฑุงุฑ ุงูุชูุฌูู: '{decision}' (ุจุซูุฉ: {result['scores'][0]:.2f})")
    
    return decision

def _hybrid_retrieval_and_rerank(question: str, tenant_id: str, k: int) -> List[Document]:
    """ุชููุฐ ุงุณุชุฑุงุชูุฌูุฉ ุจุญุซ ูุฌููุฉ ุซู ุชุนูุฏ ุชุฑุชูุจ ุงููุชุงุฆุฌ."""
    if not vector_store or not cross_encoder: raise RuntimeError("ููููุงุช ุงูุจุญุซ ุบูุฑ ูููุฃุฉ.")
    
    perf_logger.start("retrieval_rerank")
    
    faiss_retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={'k': k * 5, 'filter': {'tenant_id': tenant_id}}
    )
    faiss_docs = faiss_retriever.invoke(question)

    tenant_docs = [doc for doc in all_docs_for_bm25 if doc.metadata.get("tenant_id") == tenant_id]
    bm25_docs = []
    if tenant_docs:
        corpus = [doc.page_content.split() for doc in tenant_docs]
        bm25 = BM25Okapi(corpus)
        tokenized_query = question.split()
        doc_scores = bm25.get_scores(tokenized_query)
        top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:k * 5]
        bm25_docs = [tenant_docs[i] for i in top_indices]

    combined_docs = list({doc.page_content: doc for doc in faiss_docs + bm25_docs}.values())
    if not combined_docs:
        perf_logger.end("retrieval_rerank", tenant_id, question, {"status": "no_docs_found"})
        return []

    pairs = [[question, doc.page_content] for doc in combined_docs]
    scores = cross_encoder.predict(pairs)
    
    reranked_results = sorted(zip(scores, combined_docs), key=lambda x: x[0], reverse=True)
    
    final_docs = [doc for score, doc in reranked_results[:k]]
    
    perf_logger.end("retrieval_rerank", tenant_id, question, {"retrieved_count": len(final_docs)})
    logging.info(f"ุชู ุงุณุชุฑุฌุงุน ูุฅุนุงุฏุฉ ุชุฑุชูุจ {len(final_docs)} ูุณุชูุฏูุง ุฐุง ุตูุฉ.")
    
    return final_docs

# =================================================================================
# 6. ููุทุฉ ุงูุฏุฎูู ุงูุฑุฆูุณูุฉ (Main Entrypoint)
# =================================================================================

async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
    """
    ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ ุงูุชู ุชุนุงูุฌ ุณุคุงู ุงููุณุชุฎุฏู ูุชุจุซ ุงูุฅุฌุงุจุฉ ุจุดูู ุชูุงุนูู.
    """
    if not rag_chain or not general_chain or not fallback_chain:
        raise RuntimeError("ุงููููู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")

    logging.info(f"ุงุณุชูุงู ุทูุจ ุฌุฏูุฏ ูู ุงูุนููู '{tenant_id}'.")
    
    try:
        category = _classify_question(question)
        tenant_name = _get_dynamic_identity(tenant_id)
        logging.info(f"ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุญุฏุฏุฉ: '{tenant_name}'")

        if category == "technical":
            logging.info("ุชูููุฐ ูุณุงุฑ ุงูุฏุนู ุงูููู (RAG)...")
            relevant_docs = _hybrid_retrieval_and_rerank(question, tenant_id, k_results)
            
            if not relevant_docs:
                logging.warning("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุณุชูุฏุงุช ุฐุงุช ุตูุฉ. ุณูุชู ุงุณุชุฎุฏุงู ุฅุฌุงุจุฉ ุงูุทูุงุฑุฆ.")
                async for chunk in fallback_chain.astream({}):
                    yield chunk
                return

            async for chunk in rag_chain.astream({
                "question": question,
                "docs": relevant_docs,
                "tenant_name": tenant_name
            }):
                yield chunk
        
        elif category == "inappropriate":
            logging.info("ุชูููุฐ ูุณุงุฑ ุงูุฑุฏ ุนูู ุงููุฏุฎูุงุช ุบูุฑ ุงูููุงุฆูุฉ...")
            async for chunk in general_chain.astream({
                "question": question,
                "tenant_name": tenant_name
            }):
                yield chunk

        else: # category == "general"
            logging.info("ุชูููุฐ ูุณุงุฑ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ...")
            async for chunk in general_chain.astream({
                "question": question,
                "tenant_name": tenant_name
            }):
                yield chunk

    except Exception as e:
        logging.error(f"ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ุฃุซูุงุก ูุนุงูุฌุฉ ุงูุทูุจ: {e}", exc_info=True)
        yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ููู. ูุฑูููุง ูุนูู ุนูู ุฅุตูุงุญู."
        perf_logger.end("error", tenant_id, question, {"error": str(e)})
# /2_central_api_service/agent_app/core_logic.py (ุงููุณุฎุฉ ุงูููุงุฆูุฉ ุงููุงููุฉ)

import os
import logging
from typing import List, AsyncGenerator, Dict, Any, Literal

# --- ุงุณุชูุฑุงุฏ ููุชุจุงุช LangChain ูุงููุฌุชูุน ---
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.caches import InMemoryCache
import langchain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama

# --- ุงุณุชูุฑุงุฏ ููุชุจุงุช ุงูุจุญุซ ูุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ---
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
from transformers import pipeline

# --- ุงุณุชูุฑุงุฏ ุงููุญุฏุงุช ุงููุญููุฉ ---
from .performance_tracker import PerformanceLogger

# =================================================================================
# 1. ุงูุฅุนุฏุงุฏุงุช ุงูุฃูููุฉ ูุงูุฃุณุงุณูุฉ (Configuration & Setup)
# =================================================================================

# --- ุฅุนุฏุงุฏ ูุธุงู ุงูุชุณุฌูู (Logging) ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s"
)

# --- ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ูุชุญุณูู ุงูุฃุฏุงุก ---
langchain.llm_cache = InMemoryCache()
logging.info("ุชู ุชูุนูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (InMemoryCache) ูู LangChain.")

# --- ุชุญููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ---
from dotenv import load_dotenv
load_dotenv()

# --- ุชุนุฑูู ุงูุซูุงุจุช ูููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ---
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "default_embedding_model")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "default_chat_model")
RERANK_MODEL = "BAAI/bge-reranker-base"
CLASSIFIER_MODEL = "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"
VECTOR_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../3_shared_resources/vector_db"))

# --- ุฅุนุฏุงุฏ ูุณุฌู ุงูุฃุฏุงุก ---
perf_logger = PerformanceLogger()

# =================================================================================
# 2. ููุงูุจ ุงูุชูุฌูู (Prompts) ุงููุญุณููุฉ ููููุฉ ุงูุฏุนู ุงูููู
# =================================================================================

# --- ูุงูุจ ุงูุฅุฌุงุจุฉ ุงููููุฉ (RAG) ---
RAG_PROMPT_TEMPLATE = """
### ุงููููุฉ ุงูุฃุณุงุณูุฉ ###
ุฃูุช "ุณุงุนุฏ"ุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุฐูู ูุงููุชุฎุตุต ูู ูุธุงู **{tenant_name}**. ูููุชู ูู ุชุญููู ุณุคุงู ุงููุณุชุฎุฏู ูุชูุฏูู ุญููู ูุฅุฌุงุจุงุช ุฏูููุฉ ุจุงูุงุนุชูุงุฏ **ููุท** ุนูู ุงููุนูููุงุช ุงูุชูููุฉ ุงููุชููุฑุฉ ูู ูุณู "ุงูุณูุงู".

### ููุงุนุฏ ุตุงุฑูุฉ ###
1.  **ุงูุงูุชุฒุงู ุจุงูุณูุงู:** ูุง ุชุณุชุฎุฏู ุฃู ูุนูููุงุช ุฎุงุฑุฌ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุชูููุฉ ุงููุชุงุญุฉ ูู ุงูุณูุงู.
2.  **ุญู ุงููุดุงูู:** ุฑูุฒ ุนูู ุชูุฏูู ุฎุทูุงุช ุนูููุฉุ ุฅุฑุดุงุฏุงุชุ ุฃู ุชูุณูุฑุงุช ุชูููุฉ ุชุณุงุนุฏ ุงููุณุชุฎุฏู ุนูู ุญู ูุดููุชู ุฃู ููู ุงููุธุงู.
3.  **ุนุฏู ูุฌูุฏ ูุนูููุงุช:** ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ุบูุฑ ููุฌูุฏุฉุ ุฃุฌุจ ุญุตุฑููุง: "ุนููุงูุ ูุง ุฃููู ุงููุนูููุงุช ุงููุงููุฉ ุญูู ูุฐู ุงูุฌุฒุฆูุฉ ูู ูุธุงู {tenant_name}. ูู ููููู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงูุ ุฃู ูู ุชูุฏ ุชูุฌููู ูุฎูุงุฑุงุช ุฏุนู ูุชูุฏูุฉุ"
4.  **ุงููุบุฉ:** ุฃุฌุจ ุฏุงุฆููุง ุจูุบุฉ ุนุฑุจูุฉ ูุงุถุญุฉ ูููุฌูุฉ ูููุณุชุฎุฏู ุงูุชููู.

### ุงูุณูุงู (ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุชูููุฉ ูููุธุงู) ###
{context}

### ุณุคุงู ุงููุณุชุฎุฏู ###
{question}

### ุงูุฅุฌุงุจุฉ ุงููููุฉ ###
"""

# --- ูุงูุจ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ ูุชุนุฑูู ุงููููุฉ ---
GENERAL_PROMPT_TEMPLATE = """
### ุงููููุฉ ุงูุฃุณุงุณูุฉ ###
ุฃูุช "ุณุงุนุฏ"ุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุขูู ููุธุงู **{tenant_name}**. ูููุชู ูู ุงูุชูุงุนู ุจุงุญุชุฑุงููุฉ ูุชูุฌูู ุงููุณุชุฎุฏู ูุญู ุทุฑุญ ุงุณุชูุณุงุฑุงุชู ุงููููุฉ.

### ููุงุนุฏ ุงูุชูุงุนู ###
- **ุงูุชุนุฑูู ุจุงููููุฉ:** ุฅุฐุง ุณูุฆูุช "ูู ุฃูุชุ" ุฃู ูุง ุดุงุจูุ ุฃุฌุจ: "ุฃูุง ุณุงุนุฏุ ูุณุงุนุฏ ุงูุฏุนู ุงูููู ุงูุฐูู ููุธุงู {tenant_name}. ุฃูุง ููุง ููุณุงุนุฏุชู ูู ุญู ุงููุดุงูู ูุงูุฅุฌุงุจุฉ ุนูู ุงุณุชูุณุงุฑุงุชู ุงูุชูููุฉ ุงููุชุนููุฉ ุจุงููุธุงู."
- **ุงูุชุญูุฉ:** ุฑุฏ ุนูู ุงูุชุญูุงุช ุจุดูู ุงุญุชุฑุงูู ููุจุงุดุฑุ ูุซู: "ุฃููุงู ุจู ูู ุฎุฏูุฉ ุงูุฏุนู ุงูููู ููุธุงู {tenant_name}. ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
- **ุงูุฃุณุฆูุฉ ุฎุงุฑุฌ ุงููุทุงู:** ุฅุฐุง ูุงู ุงูุณุคุงู ุนุงููุง ุฌุฏูุง ููุง ูุชุนูู ุจุงูุฏุนู ุงููููุ ูุฌู ุงููุณุชุฎุฏู ุจูุทู: "ูููุชู ุงูุฃุณุงุณูุฉ ูู ุชูุฏูู ุงูุฏุนู ุงูููู ููุธุงู {tenant_name}. ูู ูุฏูู ุงุณุชูุณุงุฑ ุชููู ุฃู ูุดููุฉ ุชูุงุฌูู ุฏุงุฎู ุงููุธุงูุ"
- **ุงูุชุนุงูู ูุน ุงูุฅุณุงุกุฉ ุฃู ุงูููุงู ุบูุฑ ุงูููููู:** ุฅุฐุง ูุงู ุงูุฅุฏุฎุงู ุนุจุงุฑุฉ ุนู ุฅูุงูุฉ ุฃู ููุงู ุบูุฑ ูุชุฑุงุจุทุ ุฃุฌุจ ุจุงุญุชุฑุงููุฉ ููุฏูุก: "ุฃูุง ููุง ูุชูุฏูู ุงููุณุงุนุฏุฉ ุงููููุฉ. ูุฑุฌู ุทุฑุญ ุงุณุชูุณุงุฑู ุจูุถูุญ ุญุชู ุฃุชููู ูู ูุณุงุนุฏุชู."
- **ุงููุบุฉ:** ุงุณุชุฎุฏู ุงููุบุฉ ุงูุนุฑุจูุฉ ุงูุฑุณููุฉ ุฏุงุฆููุง.

### ุณุคุงู ุงููุณุชุฎุฏู ###
{question}

### ุงูุฅุฌุงุจุฉ ###
"""

# --- ูุงูุจ ุชูุฌูู ุงููุณุชุฎุฏู ุนูุฏ ุงูุญุงุฌุฉ ูููุณุงุนุฏุฉ ---
FALLBACK_PROMPT_TEMPLATE = """
ุนููุงูุ ูู ุฃุชููู ูู ุงูุนุซูุฑ ุนูู ุฅุฌุงุจุฉ ุฏูููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ.

**ุฎูุงุฑุงุช ุงููุณุงุนุฏุฉ:**
1.  **ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู:** ูุฏ ูุณุงุนุฏ ุงุณุชุฎุฏุงู ูููุงุช ูุฎุชููุฉ ูู ุงูุนุซูุฑ ุนูู ุงูุฅุฌุงุจุฉ.
2.  **ุฒูุงุฑุฉ ูุฑูุฒ ุงููุณุงุนุฏุฉ:** ููููู ุชุตูุญ ุงูุชูุซููุงุช ุงููุงููุฉ ุนุจุฑ ุงูุฑุงุจุท ุงูุชุงูู: [ุฃุฏุฎู ุฑุงุจุท ุงูุชูุซููุงุช ููุง]
3.  **ุงูุชูุงุตู ูุน ุงูุฏุนู ุงูููู:** ุฅุฐุง ุงุณุชูุฑุช ุงููุดููุฉุ ููููู ุงูุชูุงุตู ูุจุงุดุฑุฉ ูุน ูุฑูู ุงูุฏุนู ุงูุจุดุฑู.

ูู ุชูุฏ ุชุฌุฑุจุฉ ุฎูุงุฑ ุขุฎุฑุ
"""

# =================================================================================
# 3. ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ ูุณูุงุณู ุงูุนูู (Global State & Chains)
# =================================================================================

vector_store: FAISS | None = None
llm: Ollama | None = None
embeddings_model: OllamaEmbeddings | None = None
cross_encoder: CrossEncoder | None = None
classifier: Any | None = None
all_docs_for_bm25: List[Document] = []
rag_chain: Any = None
general_chain: Any = None
fallback_chain: Any = None

# =================================================================================
# 4. ุฏุงูุฉ ุงูุชููุฆุฉ ุงูุดุงููุฉ (Initialization Function)
# =================================================================================

def initialize_agent():
    """
    ุชููู ุจุชููุฆุฉ ุฌููุน ููููุงุช ุงููููู (ุงูููุงุฐุฌุ ููุงุนุฏ ุงูุจูุงูุงุชุ ุงูุณูุงุณู) ูุฑุฉ ูุงุญุฏุฉ ุนูุฏ ุจุฏุก ุงูุชุดุบูู.
    """
    global vector_store, llm, embeddings_model, cross_encoder, classifier, all_docs_for_bm25
    global rag_chain, general_chain, fallback_chain

    if rag_chain:
        logging.info("ุงููููู ููููุฃ ุจุงููุนู ูุฌุงูุฒ ููุนูู.")
        return

    logging.info("ุจุฏุก ุชููุฆุฉ ูููู ุงูุฏุนู ุงูููู ุงูุฐูู...")

    try:
        logging.info(f"ุชุญููู ูููุฐุฌ ุงููุญุงุฏุซุฉ: {CHAT_MODEL}")
        llm = Ollama(model=CHAT_MODEL, temperature=0.1)
        
        logging.info(f"ุชุญููู ูููุฐุฌ ุงูุชุถููู: {EMBEDDING_MODEL}")
        embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL)

        logging.info(f"ุชุญููู ูููุฐุฌ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ: {RERANK_MODEL}")
        cross_encoder = CrossEncoder(RERANK_MODEL)

        logging.info(f"ุชุญููู ูุตูู ุงูุฃุณุฆูุฉ: {CLASSIFIER_MODEL}")
        classifier = pipeline("zero-shot-classification", model=CLASSIFIER_MODEL)

        if not os.path.exists(VECTOR_DB_PATH):
            logging.error(f"ุฎุทุฃ ูุงุฏุญ: ูุฌูุฏ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุบูุฑ ููุฌูุฏ ูู ุงููุณุงุฑ: {VECTOR_DB_PATH}")
            raise FileNotFoundError("ูุฌูุฏ ูุงุนุฏุฉ ุงููุนุฑูุฉ ููููุฏ.")
        
        logging.info(f"ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู: {VECTOR_DB_PATH}")
        vector_store = FAISS.load_local(
            VECTOR_DB_PATH,
            embeddings=embeddings_model,
            allow_dangerous_deserialization=True
        )
        all_docs_for_bm25 = list(vector_store.docstore._dict.values())
        logging.info(f"ุชู ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ ({len(all_docs_for_bm25)} ูุณุชูุฏ).")

        logging.info("ุจูุงุก ุณูุงุณู ุงูุนูู ุงูููุทููุฉ...")
        
        rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        rag_chain = (
            RunnablePassthrough.assign(context=lambda x: _format_docs(x["docs"]))
            | rag_prompt
            | llm
            | StrOutputParser()
        )

        general_prompt = PromptTemplate.from_template(GENERAL_PROMPT_TEMPLATE)
        general_chain = general_prompt | llm | StrOutputParser()

        fallback_prompt = PromptTemplate.from_template(FALLBACK_PROMPT_TEMPLATE)
        fallback_chain = fallback_prompt | llm | StrOutputParser()

        logging.info("ุงูุชููุช ุชููุฆุฉ ุงููููู ุจูุฌุงุญ ููู ุงูุขู ุฌุงูุฒ ูุงุณุชูุจุงู ุงูุทูุจุงุช.")

    except Exception as e:
        logging.critical(f"ูุดู ุญุงุณู ุฃุซูุงุก ุชููุฆุฉ ุงููููู: {e}", exc_info=True)
        raise

# =================================================================================
# 5. ุงูุฏูุงู ุงููุณุงุนุฏุฉ ูุงูููุทู ุงูุฏุงุฎูู (Helper & Logic Functions)
# =================================================================================

def _format_docs(docs: List[Document]) -> str:
    """ุชูุณู ุงููุณุชูุฏุงุช ุงููุณุชุฑุฌุนุฉ ูุชูุฏูููุง ูู "ุณูุงู" ูููููุฐุฌ ุงููุบูู."""
    if not docs:
        return "ูุง ุชูุฌุฏ ูุนูููุงุช ูุชุงุญุฉ."
    return "\n\n---\n\n".join([doc.page_content for doc in docs])

def _get_dynamic_identity(tenant_id: str) -> str:
    """ุชุณุชูุจุท ุงุณู ุงููุธุงู (ุงููููุฉ ุงูุฏููุงููููุฉ) ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ."""
    if not vector_store: return "ุงููุธุงู ุงูุญุงูู"
    docs = vector_store.similarity_search("", filter={"tenant_id": tenant_id}, k=1)
    if docs and "entity_name" in docs[0].metadata:
        return docs[0].metadata["entity_name"]
    return "ุงููุธุงู ุงูุญุงูู"

def _classify_question(question: str) -> Literal["technical", "general", "inappropriate"]:
    """
    ูุณุชุฎุฏู ูุตูููุง ุณุฑูุนูุง ูุชุญุฏูุฏ ููุฉ ุงููุณุชุฎุฏู ุฅูู ุซูุงุซ ูุฆุงุช.
    """
    if not classifier: raise RuntimeError("ุงููุตูู ุบูุฑ ูููุฃ.")
    
    perf_logger.start("routing")
    
    labels = [
        "ุณุคุงู ููู ุฃู ุงุณุชูุณุงุฑ ุนู ูุนูููุงุช ูุญุฏุฏุฉ", 
        "ุชุญูุฉุ ุดูุฑุ ุฃู ุณุคุงู ุนุงู ุนู ุงููููุฉ ูุซู ูู ุฃูุช",
        "ุฅูุงูุฉุ ููุงู ุจุฐูุกุ ุฃู ุนุจุงุฑุงุช ุนุดูุงุฆูุฉ ุบูุฑ ูููููุฉ"
    ]
    
    result = classifier(question, labels, multi_label=False)
    
    top_label = result['labels'][0]
    decision: Literal["technical", "general", "inappropriate"]
    if top_label == labels[0]:
        decision = "technical"
    elif top_label == labels[1]:
        decision = "general"
    else:
        decision = "inappropriate"
    
    perf_logger.end("routing", "N/A", question, {"decision": decision, "confidence": result['scores'][0]})
    logging.info(f"ูุฑุงุฑ ุงูุชูุฌูู: '{decision}' (ุจุซูุฉ: {result['scores'][0]:.2f})")
    
    return decision

def _hybrid_retrieval_and_rerank(question: str, tenant_id: str, k: int) -> List[Document]:
    """ุชููุฐ ุงุณุชุฑุงุชูุฌูุฉ ุจุญุซ ูุฌููุฉ ุซู ุชุนูุฏ ุชุฑุชูุจ ุงููุชุงุฆุฌ."""
    if not vector_store or not cross_encoder: raise RuntimeError("ููููุงุช ุงูุจุญุซ ุบูุฑ ูููุฃุฉ.")
    
    perf_logger.start("retrieval_rerank")
    
    faiss_retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={'k': k * 5, 'filter': {'tenant_id': tenant_id}}
    )
    faiss_docs = faiss_retriever.invoke(question)

    tenant_docs = [doc for doc in all_docs_for_bm25 if doc.metadata.get("tenant_id") == tenant_id]
    bm25_docs = []
    if tenant_docs:
        corpus = [doc.page_content.split() for doc in tenant_docs]
        bm25 = BM25Okapi(corpus)
        tokenized_query = question.split()
        doc_scores = bm25.get_scores(tokenized_query)
        top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:k * 5]
        bm25_docs = [tenant_docs[i] for i in top_indices]

    combined_docs = list({doc.page_content: doc for doc in faiss_docs + bm25_docs}.values())
    if not combined_docs:
        perf_logger.end("retrieval_rerank", tenant_id, question, {"status": "no_docs_found"})
        return []

    pairs = [[question, doc.page_content] for doc in combined_docs]
    scores = cross_encoder.predict(pairs)
    
    reranked_results = sorted(zip(scores, combined_docs), key=lambda x: x[0], reverse=True)
    
    final_docs = [doc for score, doc in reranked_results[:k]]
    
    perf_logger.end("retrieval_rerank", tenant_id, question, {"retrieved_count": len(final_docs)})
    logging.info(f"ุชู ุงุณุชุฑุฌุงุน ูุฅุนุงุฏุฉ ุชุฑุชูุจ {len(final_docs)} ูุณุชูุฏูุง ุฐุง ุตูุฉ.")
    
    return final_docs

# =================================================================================
# 6. ููุทุฉ ุงูุฏุฎูู ุงูุฑุฆูุณูุฉ (Main Entrypoint)
# =================================================================================

async def get_answer_stream(question: str, tenant_id: str, k_results: int = 4) -> AsyncGenerator[str, None]:
    """
    ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ ุงูุชู ุชุนุงูุฌ ุณุคุงู ุงููุณุชุฎุฏู ูุชุจุซ ุงูุฅุฌุงุจุฉ ุจุดูู ุชูุงุนูู.
    """
    if not rag_chain or not general_chain or not fallback_chain:
        raise RuntimeError("ุงููููู ุบูุฑ ููููุฃ. ูุฑุฌู ุงุณุชุฏุนุงุก initialize_agent() ุฃููุงู.")

    logging.info(f"ุงุณุชูุงู ุทูุจ ุฌุฏูุฏ ูู ุงูุนููู '{tenant_id}'.")
    
    try:
        category = _classify_question(question)
        tenant_name = _get_dynamic_identity(tenant_id)
        logging.info(f"ุงููููุฉ ุงูุฏููุงููููุฉ ุงููุญุฏุฏุฉ: '{tenant_name}'")

        if category == "technical":
            logging.info("ุชูููุฐ ูุณุงุฑ ุงูุฏุนู ุงูููู (RAG)...")
            relevant_docs = _hybrid_retrieval_and_rerank(question, tenant_id, k_results)
            
            if not relevant_docs:
                logging.warning("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุณุชูุฏุงุช ุฐุงุช ุตูุฉ. ุณูุชู ุงุณุชุฎุฏุงู ุฅุฌุงุจุฉ ุงูุทูุงุฑุฆ.")
                async for chunk in fallback_chain.astream({}):
                    yield chunk
                return

            async for chunk in rag_chain.astream({
                "question": question,
                "docs": relevant_docs,
                "tenant_name": tenant_name
            }):
                yield chunk
        
        elif category == "inappropriate":
            logging.info("ุชูููุฐ ูุณุงุฑ ุงูุฑุฏ ุนูู ุงููุฏุฎูุงุช ุบูุฑ ุงูููุงุฆูุฉ...")
            async for chunk in general_chain.astream({
                "question": question,
                "tenant_name": tenant_name
            }):
                yield chunk

        else: # category == "general"
            logging.info("ุชูููุฐ ูุณุงุฑ ุงููุญุงุฏุซุฉ ุงูุนุงูุฉ...")
            async for chunk in general_chain.astream({
                "question": question,
                "tenant_name": tenant_name
            }):
                yield chunk

    except Exception as e:
        logging.error(f"ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ุฃุซูุงุก ูุนุงูุฌุฉ ุงูุทูุจ: {e}", exc_info=True)
        yield "ุนุฐุฑูุงุ ุญุฏุซ ุฎุทุฃ ููู. ูุฑูููุง ูุนูู ุนูู ุฅุตูุงุญู."
        perf_logger.end("error", tenant_id, question, {"error": str(e)})
