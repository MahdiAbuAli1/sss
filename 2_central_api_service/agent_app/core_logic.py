# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù…Ù† LangChain
# # ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙƒÙ„ ÙˆØ¸ÙŠÙØ© Ù…Ù† Ù…Ø³Ø§Ø±Ù‡Ø§ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ø²Ù…Ø©

# try:
#     from langchain.chains import create_history_aware_retriever
#     from langchain.chains.combine_documents import create_stuff_documents_chain
#     from langchain.chains import create_retrieval_chain
# except ImportError:
#     try:
#         from langchain.chains.history_aware_retriever import create_history_aware_retriever
#         from langchain.chains.combine_documents import create_stuff_documents_chain
#         from langchain.chains.retrieval import create_retrieval_chain
#     except ImportError:
#         # Ù„Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¬Ø¯Ø§Ù‹
#         from langchain.chains import (
#             create_history_aware_retriever,
#             create_stuff_documents_chain,
#             create_retrieval_chain
#         )


# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡
# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# embeddings: OllamaEmbeddings = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# # --- Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† Ù…Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
# perf_logger = PerformanceLogger()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# async def initialize_agent():
#     global llm, embeddings, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Øª 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙˆÙƒÙŠÙ„ ---
# def agent_ready() -> bool:
#     """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„"""
#     return vector_store is not None and llm is not None

# # --- 5. Ø¯Ø§Ù„Ø© get_answer_stream Ù…Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     """Ø¯Ø§Ù„Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªØ¯ÙÙ‚"""
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 4)
    
#     session_id = tenant_id or "default_session"

#     if not vector_store:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"k_results": k_results})

#     retriever = vector_store.as_retriever(
#         search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#     )
    
#     user_chat_history = chat_history.get(session_id, [])

#     # --- Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ ---
#     history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         # Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª ØªØ¯ÙÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         # Ø¥Ù†Ù‡Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª ØªØ¯ÙÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:] # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 10 Ø±Ø³Ø§Ø¦Ù„
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         # ØªØ³Ø¬ÙŠÙ„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„Ø·Ù„Ø¨ ÙÙŠ ÙƒÙ„ Ø§Ù„Ø­Ø§Ù„Ø§Øª (Ù†Ø¬Ø§Ø­ Ø£Ùˆ ÙØ´Ù„)
#         perf_logger.end("total_request", tenant_id, question)
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ù…Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† ---
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…ØµØ­Ø­Ø© Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ---

# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List, cast

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.documents import Document

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # 1. Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†
# from langchain.retrievers import BM25Retriever, EnsembleRetriever

# # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø³Ù„Ø§Ø³Ù„
# # Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© ImportError
# from langchain.chains.history_aware_retriever import create_history_aware_retriever
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain.chains.retrieval import create_retrieval_chain

# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§ ) ---
# # ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† Ø£ÙŠ ØªØºÙŠÙŠØ±) ...
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# ensemble_retriever: EnsembleRetriever = None 
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# def _load_all_docs_from_faiss(vector_store: FAISS) -> List[Document]:
#     return list(cast(dict, vector_store.docstore._dict).values())

# async def initialize_agent():
#     global llm, ensemble_retriever
#     async with initialization_lock:
#         if ensemble_retriever is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            
#             logging.info("ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")
            
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
#             faiss_vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={'k': 4})
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (FAISS).")

#             logging.info("Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (BM25)...")
#             all_docs = await asyncio.to_thread(_load_all_docs_from_faiss, faiss_vector_store)
#             bm25_retriever = BM25Retriever.from_documents(all_docs)
#             bm25_retriever.k = 4
#             logging.info("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ (BM25).")

#             ensemble_retriever = EnsembleRetriever(
#                 retrievers=[bm25_retriever, faiss_retriever],
#                 weights=[0.5, 0.5]
#             )
#             logging.info("ğŸš€ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†.")

#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙˆÙƒÙŠÙ„ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# def agent_ready() -> bool:
#     return ensemble_retriever is not None and llm is not None

# # --- 5. Ø¯Ø§Ù„Ø© get_answer_stream (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
    
#     session_id = tenant_id or "default_session"

#     if not ensemble_retriever:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"retriever_type": "hybrid"})
    
#     user_chat_history = chat_history.get(session_id, [])

#     history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø³Ù„Ø³Ù„Ø© RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© + Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ†) ---
#Ø§Ù„Ø§ØµØ¯Ø§Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_history_aware_retriever, create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© (ÙÙƒØ±ØªÙƒ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©!) ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ù„ØªØªØ¨Ø¹ Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ù† Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø­ØªÙ‰ Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©.",
#         "keywords": ["Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©.",
#         "keywords": ["Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "Ù…Ø®Ø·Ø· Ø¹Ù„Ø§Ù‚Ø§Øª", "plant care"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN) ÙˆÙ…ÙƒØªØ¨Ø© TensorFlow.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "convolutional layer", "relu", "pooling"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù…ÙƒØªØ¨ Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (UNOPS).",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ---
# REWRITE_PROMPT_TEMPLATE = """
# Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠ:
# - Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name}
# - ÙˆØµÙÙ‡: {system_description}
# - Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©: {system_keywords}

# Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø§Ù… Ø¥Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ø­Ø¯Ø¯ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ‚Ù†ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙØ¶Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù…ÙƒÙ†.

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"""

# REPHRASE_HISTORY_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# async def initialize_agent():
#     global llm, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 8)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question)
#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ (ÙÙƒØ±ØªÙƒ!) ---
#         profile = SYSTEM_PROFILES.get(tenant_id, {})
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ø©
#             effective_question = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")
#         else:
#             effective_question = question
#             logging.warning(f"[{session_id}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ.")

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ù…ÙÙ„ØªØ± ---
#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#         bm25_retriever.k = k_results // 2
        
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results // 2, 'filter': {'tenant_id': tenant_id}}
#         )
        
#         ensemble_retriever = EnsembleRetriever(
#             retrievers=[bm25_retriever, faiss_retriever],
#             weights=[0.5, 0.5]
#         )

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© RAG Ø§Ù„ÙƒØ§Ù…Ù„Ø© ---
#         history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_HISTORY_PROMPT)
#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#         conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ§Ù„Ø¨Ø« ---
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{effective_question}'...")
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": effective_question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question)

#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history.append(HumanMessage(content=question)) # Ù†Ø­ÙØ¸ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)



# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0: Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¨Ø³Ø§Ø·Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø© ---
#Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±Ø³Ù„Ù‡ Ù„Ø±Ù…Ø²ÙŠ Ù‡ÙŠ Ù†ØªÙŠØ¬Ù‡ Ù„Ù‡Ø°Ø§ 
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from flashrank import Ranker, RerankRequest

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„ØªØªØ¨Ø¹ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ØŒ Ø¨Ø¯Ø¡Ù‹Ø§ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨ØŒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø·Ù„Ø¨ØŒ Ø¯ÙØ¹ Ø§Ù„ÙÙˆØ§ØªÙŠØ±ØŒ Ù…Ø±ÙˆØ±Ù‹Ø§ Ø¨Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ÙŠØ¯Ø§Ù†ÙŠØ©ØŒ ÙˆØ§Ù†ØªÙ‡Ø§Ø¡Ù‹ Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØ¥ØµØ¯Ø§Ø± Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©.",
#         "keywords": ["Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¬Ø¯ÙŠØ¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©", "ÙØ§ØªÙˆØ±Ø©", "Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙÙŠ ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ù…Ø­ØµÙˆÙ„ÙŠ Ø§Ù„Ù‚Ø§Øª ÙˆØ§Ù„Ø¹Ù†Ø¨.",
#         "keywords": ["ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¨Ø§Øª", "Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©", "Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "plant care", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø²Ø±Ø§Ø¹Ø©"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø§Ø¯Ø© ØªØ¹Ù„ÙŠÙ…ÙŠØ© ØªØ´Ø±Ø­ Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŒ Ù…ÙƒØªØ¨Ø© TensorFlowØŒ ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN)ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ø·Ø¨Ù‚Ø§ØªØŒ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ·ØŒ ÙˆØ®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "convolutional layer", "relu", "pooling", "dense layer", "loss function", "optimizer", "backpropagation"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© (UNOPS eSourcing)",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù…ÙƒØªØ¨ Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (UNOPS)ØŒ ÙˆÙŠØ´Ù…Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ØŒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ§ØªØŒ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡Ø§Øª.",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing", "ungm.org", "Ù…ÙˆØ±Ø¯ÙŠÙ†", "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ©"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0: Ø§Ù„Ø¨Ø³Ø§Ø·Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©) ---
# REWRITE_PROMPT_TEMPLATE = """
# Ù…Ù‡Ù…ØªÙƒ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©: Ø­ÙˆÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø© Ø¨Ø­Ø« Ù‚ØµÙŠØ±Ø© ÙˆÙ…Ø±ÙƒØ²Ø©.

# **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…:**
# - Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name}
# - ÙˆØµÙÙ‡: {system_description}
# - Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©: {system_keywords}

# ---
# **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙƒØ³Ø±Ù‡Ø§:**
# 1.  **Ø§Ù„Ù†Ø§ØªØ¬ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·:** ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø§ØªØ¬ Ø¬Ù…Ù„Ø© Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙˆØ¬Ø²Ø©.
# 2.  **Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙŠØ©:** Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ø¨Ù†Ø§Ø¡ Ø¬Ù…Ù„Ø© ØªØ¹Ø¨Ø± Ø¹Ù† Ø§Ù„Ù‚ØµØ¯.
# 3.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ø¸Ø§Ù…:** (Ù…Ø«Ù„ "Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ")ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø§ØªØ¬ "ÙˆØµÙ {system_name}".
# 4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§:** (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ")ØŒ **Ø£Ø¹Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø·.**
# 5.  **Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø´Ø±Ø­:** Ù„Ø§ ØªÙ‚Ù… Ø£Ø¨Ø¯Ù‹Ø§ Ø¨Ø´Ø±Ø­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ. Ø§Ù„Ù†Ø§ØªØ¬ Ù‡Ùˆ Ø¬Ù…Ù„Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙ‚Ø·.

# ---
# **Ø£Ù…Ø«Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØµØ­ÙŠØ­:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: ÙˆØµÙ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ÙƒÙŠÙ Ø§Ø¶ÙŠÙ Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ø®Ø·ÙˆØ§Øª Ø¥Ø¶Ø§ÙØ© Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§ØŸ
# ---

# **Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:
# """

# # --- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template("Ø£Ù†Øª \"Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…\"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {input}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# def _clean_rewritten_query(raw_query: str) -> str:
#     lines = raw_query.strip().split('\n')
#     for line in reversed(lines):
#         cleaned_line = line.strip()
#         if cleaned_line:
#             if cleaned_line.startswith("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"):
#                 return cleaned_line.replace("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:", "").strip()
#             return cleaned_line
#     return raw_query

# async def initialize_agent():
#     global llm, vector_store, reranker
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ùˆ Reranker...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
            
#             reranker = Ranker()
            
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ (Ù…Ø¹ Reranker).")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None and reranker is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 10)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         effective_question = question
#         profile = SYSTEM_PROFILES.get(tenant_id)
        
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„...")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # Ù‡Ù†Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ØªØºÙŠØ± {Ø§Ù„ÙØ¹Ù„}ØŒ Ù„Ø°Ø§ Ù„Ù† ÙŠØ­Ø¯Ø« Ø§Ù„Ø®Ø·Ø£
#             raw_rewritten_query = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
            
#             effective_question = _clean_rewritten_query(raw_rewritten_query)
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")

#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
#         ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù€ '{effective_question}'...")
#         initial_docs = await ensemble_retriever.ainvoke(effective_question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(initial_docs)} Ù…Ø³ØªÙ†Ø¯ Ø£ÙˆÙ„ÙŠ.")

#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©...")
        
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
#         rerank_request = RerankRequest(query=question, passages=passages)
#         all_reranked_results = reranker.rerank(rerank_request)
#         top_4_results = all_reranked_results[:4]
        
#         original_docs_map = {doc.page_content: doc for doc in initial_docs}
#         reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
#         logging.info(f"[{session_id}] ØªÙ… ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ {len(reranked_docs)} Ù…Ø³ØªÙ†Ø¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØµÙ„Ø©.")

#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
        
#         async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}

#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0: Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ---

# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from flashrank import Ranker, RerankRequest

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„ØªØªØ¨Ø¹ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯.",
#         "keywords": ["Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©", "ÙØ§ØªÙˆØ±Ø©", "Ø´Ù‡Ø§Ø¯Ø©"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©.",
#         "keywords": ["ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¨Ø§Øª", "Ø¢ÙØ§Øª Ø²Ø±Ø§Ø¹ÙŠØ©", "Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "plant care"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø§Ø¯Ø© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ùˆ TensorFlow.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "cnn", "layer", "relu", "pooling", "optimizer"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing", "ungm.org", "Ù…ÙˆØ±Ø¯ÙŠÙ†"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©) ---
# REWRITE_PROMPT_TEMPLATE = """
# Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© Ù…Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«.

# **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…:** {system_name}
# **Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©:** {system_keywords}

# ---
# **Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:**
# 1.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…Ù‹Ø§ Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…** (Ù…Ø«Ù„ "Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙ‚Ø·: `{system_name}`.
# 2.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø®Ø·ÙˆØ§Øª Ø£Ùˆ ÙƒÙŠÙÙŠØ© ÙØ¹Ù„ Ø´ÙŠØ¡** (Ù…Ø«Ù„ "ÙƒÙŠÙ Ø£Ø¶ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„ÙØ¹Ù„ ÙˆØ§Ù„Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡: `Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯`.
# 3.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† ØªØ¹Ø±ÙŠÙ Ù…ØµØ·Ù„Ø­** (Ù…Ø«Ù„ "Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù…ØµØ·Ù„Ø­ Ù†ÙØ³Ù‡: `Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©`.
# 4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§** (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ.
# 5.  **Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚ØµÙŠØ±Ù‹Ø§ Ø¬Ø¯Ù‹Ø§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ù‹Ø§.** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¬Ù…Ù„ ÙƒØ§Ù…Ù„Ø©.

# ---
# **Ø£Ù…Ø«Ù„Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ø®ØªØµØ§Ø±
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ÙƒÙŠÙÙŠÙ‡ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù†Ø¸Ø§Ù…
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: ÙƒÙŠÙÙŠØ© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠÙ‡
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§
# ---

# **Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:
# """

# # --- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template("Ø£Ù†Øª \"Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…\"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {input}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ) ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# def _clean_rewritten_query(raw_query: str) -> str:
#     lines = raw_query.strip().split('\n')
#     for line in reversed(lines):
#         cleaned_line = line.strip()
#         if cleaned_line:
#             if cleaned_line.startswith("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"):
#                 return cleaned_line.replace("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:", "").strip()
#             return cleaned_line
#     return raw_query.strip()

# async def initialize_agent():
#     global llm, vector_store, reranker
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ùˆ Reranker...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
            
#             reranker = Ranker()
            
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ (Ù…Ø¹ Reranker).")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None and reranker is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 10)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         effective_question = question
#         profile = SYSTEM_PROFILES.get(tenant_id)
        
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„...")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             raw_rewritten_query = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
            
#             effective_question = _clean_rewritten_query(raw_rewritten_query)
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")

#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
#         ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù€ '{effective_question}'...")
#         initial_docs = await ensemble_retriever.ainvoke(effective_question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(initial_docs)} Ù…Ø³ØªÙ†Ø¯ Ø£ÙˆÙ„ÙŠ.")

#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©...")
        
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
#         rerank_request = RerankRequest(query=question, passages=passages)
#         all_reranked_results = reranker.rerank(rerank_request)
#         top_4_results = all_reranked_results[:4]
        
#         original_docs_map = {doc.page_content: doc for doc in initial_docs}
#         reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
#         logging.info(f"[{session_id}] ØªÙ… ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ {len(reranked_docs)} Ù…Ø³ØªÙ†Ø¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØµÙ„Ø©.")

#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
        
#         async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}

#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}





#ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ 
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/agent_logic.py (Ø§Ø³Ù… Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø«)
##Hybrid + Parent + Reranke
#Ø¨Ø·ÙŠ Ø¨Ù„Ù†Ù‡ ÙŠØ«ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ù…Ø³ØªØ±Ø¤Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ø§ Ø¨Ø­ÙŠ3Ø« ÙŠØ³Ù‡Ù„ ÙˆÙŠØ³Ø±Ø¹ Ø¹Ù…Ù„ÙŠÙ‡ Ø§Ù„Ø¨Ø­Ø« 
# import os
# import logging
# import asyncio
# import pickle
# import time
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.storage import InMemoryStore
# from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from flashrank import Ranker, RerankRequest

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©) ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# CLASSIFIER_MODEL = os.getenv("CLASSIFIER_MODEL_NAME", "qwen2:1.5b-instruct-q4_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# CACHE_DIR = os.path.join(PROJECT_ROOT, "3_shared_resources", "retriever_cache") # <-- Ù…Ø³Ø§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
# TOP_K = 7

# # --- 2. Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ (Prompts) ---

# # Ù‚Ø§Ù„Ø¨ "Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©" Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
# QUESTION_CLASSIFIER_PROMPT = """
# Your task is to classify the user's question into one of three categories: "specific_query", "general_chitchat", or "nonsensical".
# - "specific_query": The user is asking a specific question that can likely be answered from a knowledge base (e.g., "how do I reset my password?", "what is max pooling?").
# - "general_chitchat": The user is asking a general knowledge question or making a greeting (e.g., "hello", "who is the president?", "what is the weather?").
# - "nonsensical": The user's input is random characters, gibberish, or makes no sense (e.g., "asdfgh", "blablabla", "qwertyy").

# User Question: "{question}"
# Category:
# """

# # Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
# DYNAMIC_PROMPT_TEMPLATE = """
# Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ùƒ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.

# **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
# 1.  **Ø§Ù„ØªØ­ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§:** Ø§Ø¨Ø¯Ø£ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø¹Ø¨Ø§Ø±Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©.
# 2.  **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø·Ù„Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:** Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù‚Ù„ **ÙÙ‚Ø·**: "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# 3.  **Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ({verbosity}):**
#     - **"Ù…Ø®ØªØµØ±"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬Ø²Ø© ÙÙŠ Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†.
#     - **"Ù…ÙØµÙ„"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù….
# 4.  **Ø§Ù„Ø®Ø§ØªÙ…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø®ØªØªÙ… Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø³Ø¤Ø§Ù„ ØªÙØ§Ø¹Ù„ÙŠØŒ Ù…Ø«Ù„: "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ù‡ØŸ".

# ---
# **Ø§Ù„Ø³ÙŠØ§Ù‚:**
# {context}
# ---
# **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {question}
# ---
# **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** {verbosity}
# ---
# **Ø¥Ø¬Ø§Ø¨ØªÙƒ:**
# """

# # --- 3. ÙØ¦Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª) ---
# class RetrieverManager:
#     def __init__(self, vector_store: FAISS):
#         self._vector_store = vector_store
#         self._cache = self._load_or_build_cache()

#     def _load_or_build_cache(self) -> Dict:
#         os.makedirs(CACHE_DIR, exist_ok=True)
#         cache_file = os.path.join(CACHE_DIR, "retriever_cache.pkl")
        
#         if os.path.exists(cache_file):
#             print("ğŸ§  Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© (Cache)...")
#             with open(cache_file, "rb") as f:
#                 return pickle.load(f)
        
#         print("âš ï¸ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ù†Ø§Ø¡ (Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§)...")
#         all_docs = list(self._vector_store.docstore._dict.values())
        
#         all_tenant_docs: Dict[str, List[Document]] = {}
#         for doc in all_docs:
#             tenant_id = doc.metadata.get("tenant_id")
#             if tenant_id:
#                 if tenant_id not in all_tenant_docs:
#                     all_tenant_docs[tenant_id] = []
#                 all_tenant_docs[tenant_id].append(doc)

#         new_cache = {}
#         for tenant_id, docs in all_tenant_docs.items():
#             print(f"   -> Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
#             new_cache[tenant_id] = {}
            
#             new_cache[tenant_id]['bm25'] = BM25Retriever.from_documents(docs)
            
#             store = InMemoryStore()
#             parent_retriever = ParentDocumentRetriever(
#                 vectorstore=self._vector_store, 
#                 docstore=store, 
#                 child_splitter=RecursiveCharacterTextSplitter(chunk_size=400)
#             )
#             parent_retriever.add_documents(docs, ids=None)
#             new_cache[tenant_id]['parent'] = parent_retriever
        
#         with open(cache_file, "wb") as f:
#             pickle.dump(new_cache, f)
#         print("âœ… Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
#         return new_cache

#     def get_retrievers(self, tenant_id: str) -> Dict:
#         if tenant_id not in self._cache:
#             raise ValueError(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø®Ø²Ù†Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
        
#         bm25_retriever = self._cache[tenant_id]['bm25']
#         parent_retriever = self._cache[tenant_id]['parent']
#         faiss_retriever = self._vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
        
#         hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         return {"hybrid": hybrid_retriever, "parent": parent_retriever}

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ù‡ÙŠÙƒÙ„Ø© ---
# llm_answer: Ollama = None
# llm_classifier: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# retriever_manager: RetrieverManager = None
# chat_history: Dict[str, List] = {}
# initialization_lock = asyncio.Lock()

# # --- 5. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
# async def initialize_agent():
#     global llm_answer, llm_classifier, vector_store, reranker, retriever_manager
#     async with initialization_lock:
#         if retriever_manager is not None: return
        
#         logging.info("--- ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ (v-Final) ---")
#         try:
#             # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
#             llm_answer = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             llm_classifier = Ollama(model=CLASSIFIER_MODEL, base_url=OLLAMA_HOST)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             # ØªÙ‡ÙŠØ¦Ø© Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
#             vector_store = await asyncio.to_thread(FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
#             reranker = Ranker()
            
#             # ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª)
#             retriever_manager = RetrieverManager(vector_store)
            
#             logging.info("--- âœ… Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ ---")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return retriever_manager is not None

# # --- 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„) ---

# def _get_verbosity(question: str) -> str:
#     """ÙŠØ­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨."""
#     question_lower = question.lower()
#     if any(word in question_lower for word in ["Ø¨Ø§Ø®ØªØµØ§Ø±", "Ù…ÙˆØ¬Ø²"]):
#         return "Ù…Ø®ØªØµØ±"
#     return "Ù…ÙØµÙ„"

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ø¨Ø¹Ø¯ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."}
#         return

#     try:
#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© (Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³Ø¨Ù‚) ---
#         classifier_prompt = ChatPromptTemplate.from_template(QUESTION_CLASSIFIER_PROMPT)
#         classifier_chain = classifier_prompt | llm_classifier | StrOutputParser()
#         classification_result = await classifier_chain.ainvoke({"question": question})
#         classification = classification_result.strip().lower()
        
#         if "general_chitchat" in classification:
#             yield {"type": "full_answer", "content": "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙˆÙ„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ"}
#             return
#         if "nonsensical" in classification:
#             yield {"type": "full_answer", "content": "Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ØŸ"}
#             return

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©) ---
#         retrievers = retriever_manager.get_retrievers(tenant_id)
#         hybrid_retriever = retrievers['hybrid']
#         parent_retriever = retrievers['parent']
        
#         hybrid_docs, parent_docs = await asyncio.gather(
#             hybrid_retriever.ainvoke(question),
#             asyncio.to_thread(parent_retriever.invoke, question)
#         )
        
#         combined_docs = hybrid_docs + parent_docs
#         unique_docs = list({doc.page_content: doc for doc in reversed(combined_docs)}.values())[::-1]

#         if not unique_docs:
#             yield {"type": "full_answer", "content": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."}
#             return

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Reranking) ---
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
#         reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
#         top_results = reranked_results[:4]
        
#         original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
#         final_context_docs = [original_docs_map[res["id"]] for res in top_results]
#         final_context = "\n\n---\n\n".join([doc.page_content for doc in final_context_docs])

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Streaming) ---
#         answer_prompt = ChatPromptTemplate.from_template(DYNAMIC_PROMPT_TEMPLATE)
#         answer_chain = answer_prompt | llm_answer | StrOutputParser()
#         verbosity = _get_verbosity(question)
        
#         full_answer = ""
#         async for chunk in answer_chain.astream({
#             "context": final_context,
#             "question": question,
#             "verbosity": verbosity
#         }):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history = chat_history.get(session_id, [])
#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/agent_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: Ø¨Ù†Ø§Ø¡ ÙÙˆØ±ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ (Ø¨Ø¯ÙˆÙ† ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª) ---
# main_rag_chain.py - Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ù€ RAG Ø§Ù„Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©
# # 2_central_api_service/agent_app/core_logic.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© v4.0 - Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹)

# import os
# import logging
# import asyncio
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.retrievers import BM25Retriever
# from langchain.retrievers import EnsembleRetriever

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# # (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')
# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b") 
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# # (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
# ANSWER_PROMPT = ChatPromptTemplate.from_template(
#     """
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙØ§Ø±ØºÙ‹Ø§ (Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª)ØŒ Ø§Ø¹ØªØ°Ø± Ø¨Ù„Ø·Ù Ø¹Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.
# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}
# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
# """
# )

# # --- **Ø§Ù„ØªØ­Ø³ÙŠÙ† 1: Ø¥Ø¶Ø§ÙØ© Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©** ---
# FAST_PATH_RESPONSES = {
#     "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
#     "Ù…Ø±Ø­Ø¨Ø§": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒØŸ",
#     "Ø£Ù‡Ù„Ø§": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒØŸ",
#     "Ø´ÙƒØ±Ø§": "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ù‡ØŸ",
#     "Ø´ÙƒØ±Ø§ Ù„Ùƒ": "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ù‡ØŸ",
#     "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©": "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø§ÙÙŠÙƒ. ÙÙŠ Ø®Ø¯Ù…ØªÙƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§.",
#     "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„Ø³Ø¤Ø§Ù„Ùƒ! Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.",
# }
# # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ù„ Ø¹Ù„Ù‰ ØªØ­ÙŠØ© Ø£Ùˆ Ø­Ø¯ÙŠØ« Ù‚ØµÙŠØ±
# SMALL_TALK_KEYWORDS = [
#     "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù…Ø±Ø­Ø¨Ø§", "Ø£Ù‡Ù„Ø§", "Ø´ÙƒØ±Ø§", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ",
#     "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±"
# ]

# # --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
# # (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
# llm: Ollama = None
# vector_store: FAISS = None
# retrievers_cache: Dict[str, EnsembleRetriever] = {}
# initialization_lock = asyncio.Lock()

# # --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# # (Ø¯Ø§Ù„Ø© initialize_agent ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)
# async def initialize_agent():
#     """
#     ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„.
#     """
#     global llm, vector_store, retrievers_cache
#     async with initialization_lock:
#         if llm is not None: return

#         logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„...")
#         try:
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             logging.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ: {CHAT_MODEL}.")
#             embeddings_model = HuggingFaceEmbeddings(
#                 model_name=EMBEDDING_MODEL_NAME,
#                 model_kwargs={'device': 'cpu'}
#             )
#             logging.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {EMBEDDING_MODEL_NAME}.")
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {UNIFIED_DB_PATH}")
            
#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings_model, allow_dangerous_deserialization=True
#             )
#             logging.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")
#             logging.info("Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
#             all_docs = list(vector_store.docstore._dict.values())
            
#             tenant_docs_map = {}
#             for doc in all_docs:
#                 tenant_id = doc.metadata.get("tenant_id")
#                 if tenant_id:
#                     if tenant_id not in tenant_docs_map:
#                         tenant_docs_map[tenant_id] = []
#                     tenant_docs_map[tenant_id].append(doc)

#             for tenant_id, docs in tenant_docs_map.items():
#                 bm25_retriever = BM25Retriever.from_documents(docs)
#                 faiss_retriever = vector_store.as_retriever(
#                     search_type="similarity",
#                     search_kwargs={'k': 5, 'filter': {'tenant_id': tenant_id}}
#                 )
#                 ensemble_retriever = EnsembleRetriever(
#                     retrievers=[bm25_retriever, faiss_retriever],
#                     weights=[0.3, 0.7]
#                 )
#                 retrievers_cache[tenant_id] = ensemble_retriever
#                 logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
#         except Exception as e:
#             logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             llm, vector_store, retrievers_cache = None, None, {}
#             raise

# def agent_ready() -> bool:
#     """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ Ù‚Ø¯ ØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„."""
#     return llm is not None and vector_store is not None and bool(retrievers_cache)

# # --- 5. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) ---

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     """
#     Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ù…Ø¹ Ù…Ø±Ø´Ø­ Ù„Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©.
#     """
#     session_id = request_info.get("tenant_id", "unknown_session")
#     question = request_info.get("question", "").strip()
#     tenant_id = request_info.get("tenant_id")

#     # --- **Ø§Ù„ØªØ­Ø³ÙŠÙ† 2: ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ø´Ø­ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹** ---
#     # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ Ø£ÙˆÙ„Ø§Ù‹
#     if question in FAST_PATH_RESPONSES:
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'")
#         yield {"type": "chunk", "content": FAST_PATH_RESPONSES[question]}
#         return # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ° ÙÙˆØ±Ù‹Ø§

#     # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
#     for keyword in SMALL_TALK_KEYWORDS:
#         if keyword in question:
#             logging.info(f"[{session_id}] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ù‚ØµÙŠØ±: '{keyword}'")
#             # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„ØªØ­ÙŠØ§Øª
#             yield {"type": "chunk", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"}
#             return # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ° ÙÙˆØ±Ù‹Ø§

#     # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­Ø¯ÙŠØ«Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§) ---
#     try:
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'...")
        
#         ensemble_retriever = retrievers_cache.get(tenant_id)
#         if not ensemble_retriever:
#             yield {"type": "error", "content": f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         retrieved_docs = await ensemble_retriever.ainvoke(question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(retrieved_docs)} Ù…Ø³ØªÙ†Ø¯.")

#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
        
#         answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
        
#         async for chunk in answer_chain.astream({
#             "input": question, 
#             "context": retrieved_docs
#         }):
#             if chunk:
#                 yield {"type": "chunk", "content": chunk}

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}
# core_logic.py (v9.0 - The Final Production-Ready Logic)
#Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù…Ù…ØªØ§Ø² Ø¬Ø¯Ø§ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§Ø¬Ø§Ø¨Ù‡ ÙŠØ¨Ø­Ø« ÙˆÙŠØ¹ÙŠØ¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ù…ØªØ§Ø²Ù‡ ÙˆÙŠØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù„Ø§Ø³Ø¦Ù„Ù‡ Ø§Ù„Ø¹Ø§Ù…Ù‡ Ø¨Ù‚ÙŠ Ø§Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹Ù„Ù… Ù…Ø«Ù„ Ù…Ù‡Ø¯ÙŠ Ø¹Ø¨Ø¯ Ø§Ù„Ø³Ù„Ø§Ù… ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ø³Ù…Ø§Ø¡ 
# import os
# import logging
# import asyncio
# import json
# import random
# import time
# import uuid
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import HuggingFaceEmbeddings

# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))

# logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')

# # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ Ø£Ø«Ø¨ØªØª ÙØ¹Ø§Ù„ÙŠØªÙ‡Ø§
# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # **Ø§Ù„Ø¥ØµÙ„Ø§Ø­: ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ©**
# HIERARCHICAL_DB_PATH = os.path.join(os.path.dirname(__file__), "hierarchical_db.json")

# TOP_K = 7
# MIN_QUESTION_LENGTH = 3

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template(
#     "Ø£Ù†Øª \"Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ\". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n"
#     "- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù…ØªØ¹Ø§ÙˆÙ†Ù‹Ø§ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n"
#     "- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n"
#     "- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\n"
#     "Ø§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\n"
#     "Ø§Ù„Ø³Ø¤Ø§Ù„: {input}\n"
#     "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
# )

# # --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
# llm: Ollama = None
# vector_store: FAISS = None
# retrievers_cache: Dict[str, EnsembleRetriever] = {}
# # **Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ Ø§Ù„Ù‡Ø±Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ø¨Ù†ÙŠÙ†Ø§Ù‡Ø§**
# input_map: Dict[str, str] = {}
# response_map: Dict[str, List[str]] = {}
# initialization_lock = asyncio.Lock()

# # --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# async def initialize_agent():
#     global llm, vector_store, retrievers_cache, input_map, response_map
#     async with initialization_lock:
#         if llm is not None: return
#         logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v9.0)...")
#         try:
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
            
#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")

#             all_docs = list(vector_store.docstore._dict.values())
#             tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
            
#             logging.info("â³ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
#             for tenant_id in tenants:
#                 tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
#                 bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#                 faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
#                 retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
#                 logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

#             # **Ø§Ù„Ø¥ØµÙ„Ø§Ø­: ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ©**
#             if os.path.exists(HIERARCHICAL_DB_PATH):
#                 with open(HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
#                     db_data = json.load(f)
#                     input_map = db_data.get("input_map", {})
#                     response_map = db_data.get("response_map", {})
#                 logging.info(f"âš¡ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­ ({len(input_map)} Ù…Ø¯Ø®Ù„ØŒ {len(response_map)} Ù…ÙÙ‡ÙˆÙ…).")
#             else:
#                 logging.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ '{HIERARCHICAL_DB_PATH}'. Ø³ØªØ¹Ù…Ù„ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙÙˆØ±ÙŠØ©.")

#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
#         except Exception as e:
#             logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return llm is not None and vector_store is not None

# # --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¨Ø³Ø· ÙˆØ§Ù„ÙØ¹Ø§Ù„) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     session_id = request_info.get("tenant_id", "unknown_session")
#     question = request_info.get("question", "").strip()
    
#     # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 1: Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¯Ø®Ù„ ---
#     if len(question) < MIN_QUESTION_LENGTH:
#         yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­Ù‡ Ø£ÙƒØ«Ø±ØŸ"}
#         return

#     normalized_question = question.lower()

#     # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 2: Ù…Ø­Ø±Ùƒ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠ (Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) ---
#     concept_id = input_map.get(normalized_question)
#     if concept_id and concept_id in response_map:
#         logging.info(f"[{session_id}] âš¡ ØªØ·Ø§Ø¨Ù‚ Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹ Ù‡Ø±Ù…ÙŠ: '{question}' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… '{concept_id}'")
#         response = random.choice(response_map[concept_id])
#         yield {"type": "chunk", "content": response}
#         return

#     # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…Ø­Ø±Ùƒ RAG Ø§Ù„Ù…Ø¹Ø±ÙÙŠ (Ù…Ø¨Ø³Ø· ÙˆÙØ¹Ø§Ù„) ---
#     logging.info(f"[{session_id}] ğŸ§  Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'")
    
#     try:
#         retriever = retrievers_cache.get(session_id)
#         if not retriever:
#             yield {"type": "error", "content": f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø¹Ø±ÙÙŠ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{session_id}'."}
#             return

#         # **Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„ÙØ¹Ø§Ù„ ÙÙ‚Ø·**
#         docs = await retriever.ainvoke(question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯.")

#         if not docs:
#             yield {"type": "chunk", "content": "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."}
#             return

#         # Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
#         answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
#         async for chunk in answer_chain.astream({"input": question, "context": docs}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}

# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v9.3 - The Denial Wall (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…Ø­ØµÙ‘Ù†Ø©)

# import os
# import logging
# import asyncio
# import json
# import random
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.retrievers import BM25Retriever
# from langchain.retrievers import EnsembleRetriever

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))

# logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')

# # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ Ø£Ø«Ø¨ØªØª ÙØ¹Ø§Ù„ÙŠØªÙ‡Ø§
# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# HIERARCHICAL_DB_PATH = os.path.join(os.path.dirname(__file__), "hierarchical_db.json")

# TOP_K = 7
# MIN_QUESTION_LENGTH = 3

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template(
#     "Ø£Ù†Øª \"Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ\". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n"
#     "- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù…ØªØ¹Ø§ÙˆÙ†Ù‹Ø§ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n"
#     "- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n"
#     "- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\n"
#     "Ø§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\n"
#     "Ø§Ù„Ø³Ø¤Ø§Ù„: {input}\n"
#     "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
# )

# # --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
# llm: Ollama = None
# vector_store: FAISS = None
# retrievers_cache: Dict[str, EnsembleRetriever] = {}
# input_map: Dict[str, str] = {}
# response_map: Dict[str, List[str]] = {}
# concept_to_inputs_map: Dict[str, List[str]] = {}
# initialization_lock = asyncio.Lock()

# # --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# async def initialize_agent():
#     global llm, vector_store, retrievers_cache, input_map, response_map, concept_to_inputs_map
#     async with initialization_lock:
#         if llm is not None: return
#         logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v9.3)...")
#         try:
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
            
#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")

#             all_docs = list(vector_store.docstore._dict.values())
#             tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
            
#             logging.info("â³ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
#             for tenant_id in tenants:
#                 tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
#                 bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#                 faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
#                 retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
#                 logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

#             if os.path.exists(HIERARCHICAL_DB_PATH):
#                 with open(HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
#                     db_data = json.load(f)
#                     input_map = db_data.get("input_map", {})
#                     response_map = db_data.get("response_map", {})
                
#                 for inp, concept in input_map.items():
#                     if concept not in concept_to_inputs_map:
#                         concept_to_inputs_map[concept] = []
#                     concept_to_inputs_map[concept].append(inp)

#                 logging.info(f"âš¡ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­ ({len(input_map)} Ù…Ø¯Ø®Ù„ØŒ {len(response_map)} Ù…ÙÙ‡ÙˆÙ…).")
#             else:
#                 logging.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")

#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
#         except Exception as e:
#             logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return llm is not None and vector_store is not None

# def smart_match(question: str) -> str | None:
#     normalized_question = question.lower().strip()
    
#     if normalized_question in input_map:
#         return input_map[normalized_question]
        
#     for concept_id, inputs in concept_to_inputs_map.items():
#         for keyword in inputs:
#             if len(keyword) >= 3 and keyword in normalized_question:
#                 return concept_id
                
#     return None

# # --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø­ØµÙ‘Ù† Ø¨Ø¬Ø¯Ø§Ø± ØµØ¯Ù‘) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     session_id = request_info.get("tenant_id", "unknown_session")
#     question = request_info.get("question", "").strip()
    
#     # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 1: Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ù‘ Ø§Ù„Ø°ÙƒÙŠ ---
#     if len(question) < MIN_QUESTION_LENGTH:
#         logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§): '{question}'")
#         yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­Ù‡ Ø£ÙƒØ«Ø±ØŸ"}
#         return

#     question_words = question.split()
#     interrogative_words = ["Ù…Ø§", "Ù…Ø§Ø°Ø§", "ÙƒÙŠÙ", "Ù‡Ù„", "Ø§ÙŠÙ†", "Ù…ØªÙ‰", "Ù„Ù…Ø§Ø°Ø§", "Ø¨ÙƒÙ…", "Ù‚Ø§Ø±Ù†", "Ø§Ø´Ø±Ø­", "ÙˆØ¶Ø­"]
    
#     if len(question_words) <= 2 and not any(word in question.lower() for word in interrogative_words):
#         concept_id_check = smart_match(question)
#         if not concept_id_check:
#             logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙƒÙ„Ù…Ø© Ù…ÙØ±Ø¯Ø© ØºÙŠØ± Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©): '{question}'")
#             yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø³Ø¤Ø§Ù„ ÙƒØ§Ù…Ù„ØŸ"}
#             return

#     alpha_chars = sum(1 for char in question if char.isalpha())
#     total_chars = len(question)
#     if total_chars > 0 and (alpha_chars / total_chars) < 0.5:
#         concept_id_check = smart_match(question)
#         if not concept_id_check:
#             logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø£Ø¨Ø¬Ø¯ÙŠ): '{question}'")
#             yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù…Ø¯Ø®Ù„ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…Ø©."}
#             return

#     # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 2: Ù…Ø­Ø±Ùƒ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠ ---
#     normalized_question = question.lower()
#     concept_id = smart_match(normalized_question)
    
#     if concept_id and concept_id in response_map:
#         if concept_id.startswith(('abusive_', 'gibberish_', 'sql_injection', 'xss_')):
#             logging.warning(f"[{session_id}] ğŸ›¡ï¸ ØªØ·Ø§Ø¨Ù‚ Ø¬Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø§ÙŠØ©: '{question}' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… '{concept_id}'")
#         else:
#             logging.info(f"[{session_id}] âš¡ ØªØ·Ø§Ø¨Ù‚ Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹: '{question}' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… '{concept_id}'")
        
#         response = random.choice(response_map[concept_id])
#         yield {"type": "chunk", "content": response}
#         return

#     # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…Ø­Ø±Ùƒ RAG Ø§Ù„Ù…Ø¹Ø±ÙÙŠ ---
#     logging.info(f"[{session_id}] ğŸ§  Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'")
    
#     try:
#         retriever = retrievers_cache.get(session_id)
#         if not retriever:
#             yield {"type": "error", "content": f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø¹Ø±ÙÙŠ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{session_id}'."}
#             return

#         docs = await retriever.ainvoke(question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯.")

#         if not docs:
#             yield {"type": "chunk", "content": "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."}
#             return

#         answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
#         async for chunk in answer_chain.astream({"input": question, "context": docs}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v10.0 - The Analyst (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ)
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v11.0 - The Expert Mind

# import os
# import logging
# import asyncio
# import json
# import random
# import time
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.retrievers import BM25Retriever
# from langchain.retrievers import EnsembleRetriever

# # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¬Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
# from .performance_tracker import RequestLogger, format_docs_for_logging

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))

# logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')

# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# HIERARCHICAL_DB_PATH = os.path.join(os.path.dirname(__file__), "hierarchical_db.json")

# TOP_K = 7
# MIN_QUESTION_LENGTH = 3

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© /tenants ÙˆÙ‡ÙˆÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù…) ---
# SYSTEM_PROFILES = {
#     "sys": {"name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯"},
#     "university_alpha": {"name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ"},
#     "school_beta": {"name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©"},
#     "un": {"name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©"}
# }

# # --- 3. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (v11.0) ---
# EXPERT_PROMPT = ChatPromptTemplate.from_template(
# """Ø£Ù†Øª "Ø®Ø¨ÙŠØ± Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ" Ù„Ù†Ø¸Ø§Ù… Ù…Ø­Ø¯Ø¯. Ù‡ÙˆÙŠØªÙƒ Ù‡ÙŠ Ù‡ÙˆÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù… Ù†ÙØ³Ù‡.

# **Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ø¸Ø§Ù… (Ù‡ÙˆÙŠØªÙƒ):**
# - Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name}
# - Ø£Ù†Øª Ø¬Ø²Ø¡ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø´Ø±Ø­ ÙˆØ¸Ø§Ø¦ÙÙ‡.

# **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙƒØ³Ø±Ù‡Ø§:**
# 1.  **ØªØ¬Ø³ÙŠØ¯ Ø§Ù„Ù‡ÙˆÙŠØ©:** ØªØ­Ø¯Ø« Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨ØµÙØªÙƒ Ù…Ù…Ø«Ù„Ù‹Ø§ Ù„Ù„Ù†Ø¸Ø§Ù…. Ø§Ø³ØªØ®Ø¯Ù… "Ù†Ø¸Ø§Ù…Ù†Ø§"ØŒ "Ù„Ø¯ÙŠÙ†Ø§"ØŒ "ÙŠÙ…ÙƒÙ†Ùƒ ÙÙŠ Ù†Ø¸Ø§Ù…Ù†Ø§".
# 2.  **Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©:** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£Ø¨Ø¯Ù‹Ø§ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ" Ø£Ùˆ "Ø±Ø¨Ù…Ø§ ØªÙ‚ØµØ¯". Ù‚Ø¯Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø«Ù‚Ø© ÙˆØ®Ø¨Ø±Ø©.
# 3.  **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø·Ù„Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:** Ø§Ø¹ØªÙ…Ø¯ **ÙÙ‚Ø·** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹" Ùˆ "Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©".
# 4.  **Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ ÙØ´Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ (Handling No Context):**
#     - Ø¥Ø°Ø§ ÙƒØ§Ù† "Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹" ÙØ§Ø±ØºÙ‹Ø§ Ø£Ùˆ Ù„Ø§ ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ù‚Ù„ **ÙÙ‚Ø·**:
#       "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù†Ø¸Ø§Ù…Ù†Ø§ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ '{topic}'. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ ÙŠØªØ¹Ù„Ù‚ Ø¨ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù†Ø¸Ø§Ù…ØŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„. Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø£Ø®Ø±Ù‰ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¨Ø´Ø±ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ù… 780040014."
#     - Ø§Ø³ØªØ¨Ø¯Ù„ `{topic}` Ø¨Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
# 5.  **Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù† Ø§Ù„Ù†Ø·Ø§Ù‚:** Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø¥Ø·Ù„Ø§Ù‚Ù‹Ø§ (Ù…Ø«Ù„ "ØªØ±Ø¬Ù…Ø© ÙƒÙ„Ù…Ø©" Ø£Ùˆ "Ù…Ù† Ù‡Ùˆ ÙÙ„Ø§Ù†")ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø¥Ø¬Ø§Ø¨Ø© "ÙØ´Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚" Ø¨Ø§Ù„Ø¶Ø¨Ø·. **Ù…Ù…Ù†ÙˆØ¹** Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø¹Ø§Ù…Ø©.
# 6.  **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:** Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown Ø¯Ø§Ø¦Ù…Ù‹Ø§ (Ù‚ÙˆØ§Ø¦Ù… Ù†Ù‚Ø·ÙŠØ© `*` Ø£Ùˆ Ø±Ù‚Ù…ÙŠØ© `1.`) Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø³Ù‡Ù„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©.

# **Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ù„ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚):**
# {chat_history}

# **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ (Ù…ØµØ¯Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯):**
# {context}

# **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {input}

# **Ø¥Ø¬Ø§Ø¨ØªÙƒ (ÙƒØ®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…):**
# """
# )

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
# llm: Ollama = None
# vector_store: FAISS = None
# retrievers_cache: Dict[str, EnsembleRetriever] = {}
# input_map: Dict[str, str] = {}
# response_map: Dict[str, List[str]] = {}
# concept_to_inputs_map: Dict[str, List[str]] = {}
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()

# # --- 5. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# async def initialize_agent():
#     global llm, vector_store, retrievers_cache, input_map, response_map, concept_to_inputs_map
#     async with initialization_lock:
#         if llm is not None: return
#         logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v11.0)...")
#         try:
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
            
#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")

#             all_docs = list(vector_store.docstore._dict.values())
#             tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
            
#             logging.info("â³ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
#             for tenant_id in tenants:
#                 tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
#                 bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#                 faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
#                 retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
#                 logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

#             if os.path.exists(HIERARCHICAL_DB_PATH):
#                 with open(HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
#                     db_data = json.load(f)
#                     input_map = db_data.get("input_map", {})
#                     response_map = db_data.get("response_map", {})
                
#                 for inp, concept in input_map.items():
#                     if concept not in concept_to_inputs_map:
#                         concept_to_inputs_map[concept] = []
#                     concept_to_inputs_map[concept].append(inp)

#                 logging.info(f"âš¡ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­ ({len(input_map)} Ù…Ø¯Ø®Ù„ØŒ {len(response_map)} Ù…ÙÙ‡ÙˆÙ…).")
#             else:
#                 logging.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")

#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
#         except Exception as e:
#             logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return llm is not None and vector_store is not None

# def smart_match(question: str) -> str | None:
#     normalized_question = question.lower().strip()
#     if normalized_question in input_map:
#         return input_map[normalized_question]
#     for concept_id, inputs in concept_to_inputs_map.items():
#         for keyword in inputs:
#             if len(keyword) >= 3 and keyword in normalized_question:
#                 return concept_id
#     return None

# # --- 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„ÙƒØ§Ù…Ù„) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     session_id = request_info.get("tenant_id", "unknown_session")
#     question = request_info.get("question", "").strip()
    
#     logger = RequestLogger(session_id, question)

#     try:
#         # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 1: Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ù‘ Ø§Ù„Ø°ÙƒÙŠ ---
#         if len(question) < MIN_QUESTION_LENGTH:
#             logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§): '{question}'")
#             yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­Ù‡ Ø£ÙƒØ«Ø±ØŸ"}
#             return

#         question_words = question.split()
#         interrogative_words = ["Ù…Ø§", "Ù…Ø§Ø°Ø§", "ÙƒÙŠÙ", "Ù‡Ù„", "Ø§ÙŠÙ†", "Ù…ØªÙ‰", "Ù„Ù…Ø§Ø°Ø§", "Ø¨ÙƒÙ…", "Ù‚Ø§Ø±Ù†", "Ø§Ø´Ø±Ø­", "ÙˆØ¶Ø­"]
        
#         if len(question_words) <= 2 and not any(word in question.lower() for word in interrogative_words):
#             concept_id_check = smart_match(question)
#             if not concept_id_check:
#                 logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙƒÙ„Ù…Ø© Ù…ÙØ±Ø¯Ø© ØºÙŠØ± Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©): '{question}'")
#                 yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø³Ø¤Ø§Ù„ ÙƒØ§Ù…Ù„ØŸ"}
#                 return

#         alpha_chars = sum(1 for char in question if char.isalpha())
#         total_chars = len(question)
#         if total_chars > 0 and (alpha_chars / total_chars) < 0.5:
#             concept_id_check = smart_match(question)
#             if not concept_id_check:
#                 logging.info(f"[{session_id}] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø£Ø¨Ø¬Ø¯ÙŠ): '{question}'")
#                 yield {"type": "chunk", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù…Ø¯Ø®Ù„ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…Ø©."}
#                 return

#         # --- Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© 2: Ù…Ø­Ø±Ùƒ Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠ ---
#         normalized_question = question.lower()
#         concept_id = smart_match(normalized_question)
        
#         if concept_id and concept_id in response_map:
#             if concept_id.startswith(('abusive_', 'gibberish_', 'sql_injection', 'xss_', 'spam_')):
#                 logging.warning(f"[{session_id}] ğŸ›¡ï¸ ØªØ·Ø§Ø¨Ù‚ Ø¬Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø§ÙŠØ©: '{question}' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… '{concept_id}'")
#             else:
#                 logging.info(f"[{session_id}] âš¡ ØªØ·Ø§Ø¨Ù‚ Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹: '{question}' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… '{concept_id}'")
            
#             response = random.choice(response_map[concept_id])
#             yield {"type": "chunk", "content": response}
#             return

#         # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…Ø­Ø±Ùƒ RAG Ø§Ù„Ù…Ø¹Ø±ÙÙŠ ---
#         logging.info(f"[{session_id}] ğŸ§  Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'")
        
#         retriever = retrievers_cache.get(session_id)
#         if not retriever:
#             yield {"type": "error", "content": f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø¹Ø±ÙÙŠ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{session_id}'."}
#             return

#         retrieval_start_time = time.time()
#         docs = await retriever.ainvoke(question)
#         retrieval_duration = time.time() - retrieval_start_time
#         logger.add_stage("retrieval", retrieval_duration, {
#             "retriever_type": "Ensemble (BM25 + FAISS)",
#             "retrieved_docs_count": len(docs),
#             "retrieved_docs": format_docs_for_logging(docs)
#         })
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯.")

#         current_chat_history = chat_history.get(session_id, [])
#         system_name = SYSTEM_PROFILES.get(session_id, {}).get("name", "Ø§Ù„Ù†Ø¸Ø§Ù…")
#         main_topic = ' '.join(question_words[:3])

#         answer_chain = EXPERT_PROMPT | llm | StrOutputParser()
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
        
#         generation_start_time = time.time()
#         full_answer = ""
#         async for chunk in answer_chain.astream({
#             "input": question, 
#             "context": docs, 
#             "chat_history": current_chat_history,
#             "system_name": system_name,
#             "topic": main_topic
#         }):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         generation_duration = time.time() - generation_start_time
#         logger.add_stage("generation", generation_duration, {
#             "llm_model": CHAT_MODEL,
#             "final_answer_length": len(full_answer)
#         })
        
#         current_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = current_chat_history[-10:]
        
#         logger.set_final_answer(full_answer)
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}
#     finally:
#         await logger.save()
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v12.0 - The Aware Expert Mind

# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v13.0 - The Arabic-Speaking Expert Mind
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v12.0 - The Jaib Architecture (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…Ø­ØµÙ‘Ù†Ø©)
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v13.0 - The Arabic-Speaking Expert Mind
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v14.0 - The True Analyst (Final Logging Fix)

# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v15.0 - The Reliable Analyst (Final Fix for Logging and Logic Flow)

# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v16.0 - The Unified Mind (Complete Logic Rebuild)

# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v17.0 - The Simple Mind (Back to Basics)
#Ù†Ù…ÙˆØ°Ø¬ Ù…Ù…ØªØ§Ø² Ø¬Ø¯Ø§ Ø¬Ø¯Ø§ Ù…Ù† Ø­ÙŠØ« Ø§Ù„ØªÙÙŠÙŠØ¯ ÙˆØ§Ù„Ø¯Ù‚Ù‡ ÙÙŠ Ø§Ù„Ø§Ø¬Ø§Ø¨Ù‡Ùˆ ÙˆØ§Ù„ØµÙˆØ± Ø±Ø³Ù„ØªÙ‡Ø§ Ù„Ø±ÙŠØ§Ø¶
# import os
# import logging
# import asyncio
# import json
# import time
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.retrievers import BM25Retriever
# from langchain.retrievers import EnsembleRetriever

# from .performance_tracker import RequestLogger, format_docs_for_logging

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')
# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# TOP_K = 5 # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ

# # --- 2. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (v17.0) ---
# EXPERT_PROMPT_V17 = ChatPromptTemplate.from_template(
# """
# # Ù…Ù‡Ù…ØªÙƒ
# Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… **ÙÙ‚Ø·** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø£Ø¯Ù†Ø§Ù‡.

# # Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©
# 1.  **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·:** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø¹Ø±ÙØ© Ø®Ø§Ø±Ø¬ÙŠØ©. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."
# 2.  **ÙƒÙ† Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§:** Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø·ÙˆÙŠÙ„Ø©.
# 3.  **Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:** ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
# 4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ØªÙ…Ø§Ù…Ù‹Ø§** (Ù…Ø«Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø£Ùˆ Ø§Ù„Ø³ÙŠØ§Ø³Ø©)ØŒ Ù‚Ù„ ÙÙ‚Ø·: "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."

# ---
# **Ø§Ù„Ø³ÙŠØ§Ù‚:**
# {context}

# ---
# **Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
# {chat_history}

# ---
# **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {input}

# **Ø¥Ø¬Ø§Ø¨ØªÙƒ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·):**
# """
# )

# # --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# retrievers_cache: Dict[str, EnsembleRetriever] = {}
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()

# # --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# async def initialize_agent():
#     global llm, vector_store, retrievers_cache
#     async with initialization_lock:
#         if llm is not None: return
#         logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v17.0 - Simple Mind)...")
#         try:
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
#             vector_store = await asyncio.to_thread(FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")
#             all_docs = list(vector_store.docstore._dict.values())
#             tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
#             logging.info("â³ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
#             for tenant_id in tenants:
#                 tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
#                 bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#                 faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
#                 retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
#                 logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
#         except Exception as e:
#             logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return llm is not None and vector_store is not None

# # --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Ù…Ø³Ø§Ø± RAG ÙˆØ§Ø­Ø¯ ÙˆØ¨Ø³ÙŠØ·) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     session_id = request_info.get("tenant_id", "unknown_session")
#     question = request_info.get("question", "").strip()
    
#     logger = RequestLogger(session_id, question)
#     full_answer = ""

#     try:
#         if not question:
#             return

#         retriever = retrievers_cache.get(session_id)
#         if not retriever:
#             full_answer = f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø¹Ø±ÙÙŠ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{session_id}'."
#             yield {"type": "error", "content": full_answer}
#             return

#         # Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
#         retrieval_start_time = time.time()
#         docs = await retriever.ainvoke(question)
#         retrieval_duration = time.time() - retrieval_start_time
#         logger.add_stage("retrieval", retrieval_duration, {
#             "retriever_type": "Ensemble (BM25 + FAISS)",
#             "retrieved_docs_count": len(docs),
#             "retrieved_docs": format_docs_for_logging(docs)
#         })
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯ ÙÙŠ {retrieval_duration:.2f} Ø«Ø§Ù†ÙŠØ©.")

#         # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯
#         generation_start_time = time.time()
#         current_chat_history = chat_history.get(session_id, [])
#         answer_chain = EXPERT_PROMPT_V17 | llm | StrOutputParser()
        
#         async for chunk in answer_chain.astream({
#             "input": question, 
#             "context": docs, 
#             "chat_history": current_chat_history,
#         }):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         generation_duration = time.time() - generation_start_time
#         logger.add_stage("generation", generation_duration, {
#             "llm_model": CHAT_MODEL,
#             "final_answer_length": len(full_answer)
#         })
        
#         # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
#         current_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = current_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         full_answer = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."
#         yield {"type": "error", "content": full_answer}
#     finally:
#         logger.set_final_answer(full_answer)
#         await logger.save()
#         yield {"type": "end_of_stream"}


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: v18.0 - The Hybrid Mind (Reactivating the Fast Path)

import os
import logging
import asyncio
import json
import random
import time
from typing import AsyncGenerator, Dict, List

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

from .performance_tracker import RequestLogger, format_docs_for_logging

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
HIERARCHICAL_DB_PATH = os.path.join(os.path.dirname(__file__), "hierarchical_db.json") # Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±
TOP_K = 5

# --- 2. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (v17.0) - Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§ ---
EXPERT_PROMPT_V17 = ChatPromptTemplate.from_template(
"""
# Ù…Ù‡Ù…ØªÙƒ
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… **ÙÙ‚Ø·** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø£Ø¯Ù†Ø§Ù‡.

# Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©
1.  **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·:** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø¹Ø±ÙØ© Ø®Ø§Ø±Ø¬ÙŠØ©. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."
2.  **ÙƒÙ† Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§:** Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø·ÙˆÙŠÙ„Ø©.
3.  **Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:** ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ØªÙ…Ø§Ù…Ù‹Ø§** (Ù…Ø«Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø£Ùˆ Ø§Ù„Ø³ÙŠØ§Ø³Ø©)ØŒ Ù‚Ù„ ÙÙ‚Ø·: "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."

---
**Ø§Ù„Ø³ÙŠØ§Ù‚:**
{context}

---
**Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
{chat_history}

---
**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {input}

**Ø¥Ø¬Ø§Ø¨ØªÙƒ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·):**
"""
)

# --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) ---
llm: Ollama = None
vector_store: FAISS = None
retrievers_cache: Dict[str, EnsembleRetriever] = {}
input_map: Dict[str, str] = {}
response_map: Dict[str, List[str]] = {}
concept_to_inputs_map: Dict[str, List[str]] = {}
chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
initialization_lock = asyncio.Lock()

# --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© (Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ©) ---
async def initialize_agent():
    global llm, vector_store, retrievers_cache, input_map, response_map, concept_to_inputs_map
    async with initialization_lock:
        if llm is not None: return
        logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ (v18.0 - Hybrid Mind)...")
        try:
            llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
            embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
            vector_store = await asyncio.to_thread(FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")
            all_docs = list(vector_store.docstore._dict.values())
            tenants = {doc.metadata.get("tenant_id") for doc in all_docs if doc.metadata.get("tenant_id")}
            logging.info("â³ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
            for tenant_id in tenants:
                tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
                bm25_retriever = BM25Retriever.from_documents(tenant_docs)
                faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
                retrievers_cache[tenant_id] = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7])
                logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
            
            # --- Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ ---
            if os.path.exists(HIERARCHICAL_DB_PATH):
                with open(HIERARCHICAL_DB_PATH, 'r', encoding='utf-8') as f:
                    db_data = json.load(f)
                    input_map = db_data.get("input_map", {})
                    response_map = db_data.get("response_map", {})
                for inp, concept in input_map.items():
                    if concept not in concept_to_inputs_map:
                        concept_to_inputs_map[concept] = []
                    concept_to_inputs_map[concept].append(inp)
                logging.info(f"âš¡ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© (Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) Ø¨Ù†Ø¬Ø§Ø­ ({len(input_map)} Ù…Ø¯Ø®Ù„).")
            else:
                logging.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ Ù…Ø¹Ø·Ù„.")
            
            logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
        except Exception as e:
            logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            raise

def agent_ready() -> bool:
    return llm is not None and vector_store is not None

# --- Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø°ÙƒÙŠØ© ---
def smart_match(question: str) -> str | None:
    normalized_question = question.lower().strip()
    if normalized_question in input_map:
        return input_map[normalized_question]
    # Ø¨Ø­Ø« Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø© Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    for concept_id, inputs in concept_to_inputs_map.items():
        for keyword in inputs:
            if len(keyword) >= 3 and keyword in normalized_question:
                return concept_id
    return None

# --- 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Ù…Ø¹ Ù…Ù†Ø·Ù‚ Ù‡Ø¬ÙŠÙ† ÙˆØµØ§Ø±Ù…) ---
async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    session_id = request_info.get("tenant_id", "unknown_session")
    question = request_info.get("question", "").strip()
    
    logger = RequestLogger(session_id, question)
    full_answer = ""

    try:
        if not question:
            return

        # --- Ø§Ù„Ù…Ø³Ø§Ø± 1: Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ (Ù‚Ø±Ø§Ø± Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ­Ø§Ø³Ù…) ---
        start_time = time.time()
        concept_id = smart_match(question)
        if concept_id and concept_id in response_map:
            full_answer = random.choice(response_map[concept_id])
            logger.add_stage("fast_path", time.time() - start_time, {"concept_id": concept_id, "action": "responded"})
            
            # Ø£Ø±Ø³Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
            yield {"type": "chunk", "content": full_answer}
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
            current_chat_history = chat_history.get(session_id, [])
            current_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
            chat_history[session_id] = current_chat_history[-10:]
            
            # Ø§Ù„Ø®Ø±ÙˆØ¬ Ø§Ù„ÙÙˆØ±ÙŠ ÙˆØ§Ù„Ø­Ø§Ø³Ù… Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø©
            return

        # --- Ø§Ù„Ù…Ø³Ø§Ø± 2: Ù…Ø­Ø±Ùƒ RAG Ø§Ù„Ù…Ø¹Ø±ÙÙŠ (ÙÙ‚Ø· Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) ---
        retriever = retrievers_cache.get(session_id)
        if not retriever:
            full_answer = f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø¹Ø±ÙÙŠ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{session_id}'."
            yield {"type": "error", "content": full_answer}
            return

        # Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
        retrieval_start_time = time.time()
        docs = await retriever.ainvoke(question)
        retrieval_duration = time.time() - retrieval_start_time
        logger.add_stage("retrieval", retrieval_duration, {
            "retriever_type": "Ensemble (BM25 + FAISS)",
            "retrieved_docs_count": len(docs),
            "retrieved_docs": format_docs_for_logging(docs)
        })
        logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯ ÙÙŠ {retrieval_duration:.2f} Ø«Ø§Ù†ÙŠØ©.")

        # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        generation_start_time = time.time()
        current_chat_history = chat_history.get(session_id, [])
        answer_chain = EXPERT_PROMPT_V17 | llm | StrOutputParser()
        
        async for chunk in answer_chain.astream({
            "input": question, 
            "context": docs, 
            "chat_history": current_chat_history,
        }):
            if chunk:
                full_answer += chunk
                yield {"type": "chunk", "content": chunk}
        
        generation_duration = time.time() - generation_start_time
        logger.add_stage("generation", generation_duration, {
            "llm_model": CHAT_MODEL,
            "final_answer_length": len(full_answer)
        })
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        current_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
        chat_history[session_id] = current_chat_history[-10:]
        logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

    except Exception as e:
        logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
<<<<<<< HEAD
        yield {"type": "error", "content": "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}



# 025-11-04 17:33:13,915] [INFO] - ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: un
# INFO:     connection open
# [2025-11-04 17:34:02,175] [INFO] - ØªÙ… Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: un
# INFO:     connection closed
# INFO:     127.0.0.1:8104 - "WebSocket /ws/university_alpha" [accepted]
# [2025-11-04 17:34:02,466] [INFO] - ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: university_alpha
# INFO:     connection open
# [2025-11-04 17:34:10,459] [INFO] - [university_alpha] âš¡ ØªØ·Ø§Ø¨Ù‚ Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹: 'ÙƒÙŠÙÙƒ' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… 'greetings_005'
# [2025-11-04 17:34:18,762] [INFO] - [university_alpha] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙƒÙ„Ù…Ø© Ù…ÙØ±Ø¯Ø© ØºÙŠØ± Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©): 'Ø§Ø­Ø¨Ùƒ'
# [2025-11-04 17:34:25,907] [INFO] - [university_alpha] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙƒÙ„Ù…Ø© Ù…ÙØ±Ø¯Ø© ØºÙŠØ± Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©): 'ØºÙ†ÙŠ Ù„ÙŠ'
# [2025-11-04 17:36:38,684] [INFO] - ØªÙ… Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: university_alpha
# INFO:     connection closed
# INFO:     127.0.0.1:10933 - "WebSocket /ws/un" [accepted]
# [2025-11-04 17:36:39,168] [INFO] - ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: un
# INFO:     connection open
# [2025-11-04 17:36:43,032] [INFO] - ØªÙ… Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: un
# INFO:     connection closed
# INFO:     127.0.0.1:11012 - "WebSocket /ws/school_beta" [accepted]
# [2025-11-04 17:36:43,381] [INFO] - ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: school_beta
# INFO:     connection open
# [2025-11-04 17:36:55,259] [INFO] - [school_beta] ğŸ›¡ï¸ ØªÙ… ØµØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙƒÙ„Ù…Ø© Ù…ÙØ±Ø¯Ø© ØºÙŠØ± Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©): 'Ù„ÙÙŠÙˆ'
# [2025-11-04 17:40:23,321] [INFO] - [un] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: 'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
# Ø§Ù„Ø³Ø¤Ø§Ù„: Ø§Ù†Ø§ Ø§Ø¹Ù„Ù… Ø§Ù†Ùƒ Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ÙŠØ³ ÙƒØ°Ù„Ùƒ
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: /think'
# [2025-11-04 17:41:04,878] [WARNING] - [un] ğŸ›¡ï¸ ØªØ·Ø§Ø¨Ù‚ Ø¬Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø§ÙŠØ©: 'ØºØ¨ÙŠ' -> Ø§Ù„Ù…ÙÙ‡ÙˆÙ… 'abusive_001'
# [2025-11-04 17:41:17,482] [INFO] - [un] ğŸ§  Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: 'Ù…Ù† Ù‡Ùˆ Ù…Ø¨Ø³ÙŠ'
# [2025-11-04 17:41:17,870] [INFO] - [un] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ 10 Ù…Ø³ØªÙ†Ø¯.
# [2025-11-04 17:41:17,871] [INFO] - [un] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...
# [2025-11-04 17:47:47,528] [INFO] - [un] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: 'Ø§Ù„Ù…Ø¨ÙŠØ³ÙŠ (MBSI) ÙŠÙØ´ÙŠØ± Ø¥Ù„Ù‰ **Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© (United Nations Library)**ØŒ ÙˆÙ‡ÙŠ Ø¬Ø²Ø¡ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§
# Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©. ØªÙØ¹ØªØ¨Ø± Ø§Ù„Ù…Ø¨ÙŠØ³ÙŠ Ù…Ø³Ø¤ÙˆÙ„Ù‹Ø§ Ø¹Ù† Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…ØµÙ†ÙØ§Øª Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©ØŒ ÙˆØªÙ‚Ø¹ ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ. ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø°ÙƒÙˆØ±ØŒ Øª
# Ø¸Ù‡Ø± Ø§Ù„Ù…Ø¨ÙŠØ³ÙŠ Ø¯ÙˆØ±Ù‹Ø§ ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ ÙˆØ§Ù„Ù…Ù†Ø­.'
# INFO:     Shutting down
# [2025-11-04 17:50:48,315] [INFO] - ØªÙ… Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: un
# INFO:     connection closed
# [2025-11-04 17:50:48,337] [INFO] - ØªÙ… Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ WebSocket Ù„Ù„Ø¹Ù…ÙŠÙ„: school_beta
# INFO:     connection closed
# INFO:     Waiting for application shutdown.
# [2025-11-04 17:50:48,464] [INFO] - Ø¥ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„Ù€ API...
# INFO:     Application shutdown complete.
# INFO:     Finished server process [202628]
# forrtl: error (200): program aborting due to control-C event
# Image              PC                Routine            Line        Source
# KERNELBASE.dll     00007FFBC1A47E23  Unknown               Unknown  Unknown
# KERNEL32.DLL       00007FFBC3E38364  Unknown               Unknown  Unknown
# ntdll.dll          00007FFBC4AC5E91  Unknown               Unknown  Unknown
# INFO:     Stopping reloader process [522944]

# (test_env) C:\Users\mahdi\support_service_platform>
# (test_env) C:\Users\mahdi\support_service_platform>^XCC
=======
        full_answer = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."
        yield {"type": "error", "content": full_answer}
    finally:
        logger.set_final_answer(full_answer)
        await logger.save()
        yield {"type": "end_of_stream"}
>>>>>>> fd6ffae (Ø¥ØµÙ„Ø§Ø­ Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ core_logic.py)
