# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù…Ù† LangChain
# # ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙƒÙ„ ÙˆØ¸ÙŠÙØ© Ù…Ù† Ù…Ø³Ø§Ø±Ù‡Ø§ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ø²Ù…Ø©

# try:
#     from langchain.chains import create_history_aware_retriever
#     from langchain.chains.combine_documents import create_stuff_documents_chain
#     from langchain.chains import create_retrieval_chain
# except ImportError:
#     try:
#         from langchain.chains.history_aware_retriever import create_history_aware_retriever
#         from langchain.chains.combine_documents import create_stuff_documents_chain
#         from langchain.chains.retrieval import create_retrieval_chain
#     except ImportError:
#         # Ù„Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¬Ø¯Ø§Ù‹
#         from langchain.chains import (
#             create_history_aware_retriever,
#             create_stuff_documents_chain,
#             create_retrieval_chain
#         )


# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡
# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# embeddings: OllamaEmbeddings = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# # --- Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† Ù…Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
# perf_logger = PerformanceLogger()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# async def initialize_agent():
#     global llm, embeddings, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Øª 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙˆÙƒÙŠÙ„ ---
# def agent_ready() -> bool:
#     """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„"""
#     return vector_store is not None and llm is not None

# # --- 5. Ø¯Ø§Ù„Ø© get_answer_stream Ù…Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     """Ø¯Ø§Ù„Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªØ¯ÙÙ‚"""
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 4)
    
#     session_id = tenant_id or "default_session"

#     if not vector_store:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"k_results": k_results})

#     retriever = vector_store.as_retriever(
#         search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#     )
    
#     user_chat_history = chat_history.get(session_id, [])

#     # --- Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ ---
#     history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         # Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª ØªØ¯ÙÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         # Ø¥Ù†Ù‡Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª ØªØ¯ÙÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:] # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 10 Ø±Ø³Ø§Ø¦Ù„
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         # ØªØ³Ø¬ÙŠÙ„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„Ø·Ù„Ø¨ ÙÙŠ ÙƒÙ„ Ø§Ù„Ø­Ø§Ù„Ø§Øª (Ù†Ø¬Ø§Ø­ Ø£Ùˆ ÙØ´Ù„)
#         perf_logger.end("total_request", tenant_id, question)
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ù…Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ† ---
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…ØµØ­Ø­Ø© Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ---

# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List, cast

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.documents import Document

# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# # --- Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# # 1. Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†
# from langchain.retrievers import BM25Retriever, EnsembleRetriever

# # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø³Ù„Ø§Ø³Ù„
# # Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© ImportError
# from langchain.chains.history_aware_retriever import create_history_aware_retriever
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain.chains.retrieval import create_retrieval_chain

# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# # --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§ ) ---
# # ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† Ø£ÙŠ ØªØºÙŠÙŠØ±) ...
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# ensemble_retriever: EnsembleRetriever = None 
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# def _load_all_docs_from_faiss(vector_store: FAISS) -> List[Document]:
#     return list(cast(dict, vector_store.docstore._dict).values())

# async def initialize_agent():
#     global llm, ensemble_retriever
#     async with initialization_lock:
#         if ensemble_retriever is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            
#             logging.info("ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")
            
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
#             faiss_vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={'k': 4})
#             logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (FAISS).")

#             logging.info("Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (BM25)...")
#             all_docs = await asyncio.to_thread(_load_all_docs_from_faiss, faiss_vector_store)
#             bm25_retriever = BM25Retriever.from_documents(all_docs)
#             bm25_retriever.k = 4
#             logging.info("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ (BM25).")

#             ensemble_retriever = EnsembleRetriever(
#                 retrievers=[bm25_retriever, faiss_retriever],
#                 weights=[0.5, 0.5]
#             )
#             logging.info("ğŸš€ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†.")

#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙˆÙƒÙŠÙ„ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# def agent_ready() -> bool:
#     return ensemble_retriever is not None and llm is not None

# # --- 5. Ø¯Ø§Ù„Ø© get_answer_stream (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
    
#     session_id = tenant_id or "default_session"

#     if not ensemble_retriever:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question, {"retriever_type": "hybrid"})
    
#     user_chat_history = chat_history.get(session_id, [])

#     history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø³Ù„Ø³Ù„Ø© RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© + Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ†) ---
#Ø§Ù„Ø§ØµØ¯Ø§Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_history_aware_retriever, create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© (ÙÙƒØ±ØªÙƒ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©!) ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ù„ØªØªØ¨Ø¹ Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ù† Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø­ØªÙ‰ Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©.",
#         "keywords": ["Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©.",
#         "keywords": ["Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "Ù…Ø®Ø·Ø· Ø¹Ù„Ø§Ù‚Ø§Øª", "plant care"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN) ÙˆÙ…ÙƒØªØ¨Ø© TensorFlow.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "convolutional layer", "relu", "pooling"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù…ÙƒØªØ¨ Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (UNOPS).",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ---
# REWRITE_PROMPT_TEMPLATE = """
# Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠ:
# - Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name}
# - ÙˆØµÙÙ‡: {system_description}
# - Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©: {system_keywords}

# Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø§Ù… Ø¥Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ø­Ø¯Ø¯ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ‚Ù†ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙØ¶Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù…ÙƒÙ†.

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"""

# REPHRASE_HISTORY_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# async def initialize_agent():
#     global llm, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 8)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     perf_logger.start("total_request", tenant_id, question)
#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ (ÙÙƒØ±ØªÙƒ!) ---
#         profile = SYSTEM_PROFILES.get(tenant_id, {})
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'.")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ø©
#             effective_question = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")
#         else:
#             effective_question = question
#             logging.warning(f"[{session_id}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ.")

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ù…ÙÙ„ØªØ± ---
#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs)
#         bm25_retriever.k = k_results // 2
        
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results // 2, 'filter': {'tenant_id': tenant_id}}
#         )
        
#         ensemble_retriever = EnsembleRetriever(
#             retrievers=[bm25_retriever, faiss_retriever],
#             weights=[0.5, 0.5]
#         )

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© RAG Ø§Ù„ÙƒØ§Ù…Ù„Ø© ---
#         history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_HISTORY_PROMPT)
#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#         conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ§Ù„Ø¨Ø« ---
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{effective_question}'...")
#         full_answer = ""
#         perf_logger.start("llm_stream_generation", tenant_id, question)

#         async for chunk in conversational_rag_chain.astream({"input": effective_question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         perf_logger.end("llm_stream_generation", tenant_id, question)

#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history.append(HumanMessage(content=question)) # Ù†Ø­ÙØ¸ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
#     finally:
#         perf_logger.end("total_request", tenant_id, question)



# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0: Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¨Ø³Ø§Ø·Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø© ---
#Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±Ø³Ù„Ù‡ Ù„Ø±Ù…Ø²ÙŠ Ù‡ÙŠ Ù†ØªÙŠØ¬Ù‡ Ù„Ù‡Ø°Ø§ 
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains import create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from flashrank import Ranker, RerankRequest

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„ØªØªØ¨Ø¹ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ØŒ Ø¨Ø¯Ø¡Ù‹Ø§ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨ØŒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø·Ù„Ø¨ØŒ Ø¯ÙØ¹ Ø§Ù„ÙÙˆØ§ØªÙŠØ±ØŒ Ù…Ø±ÙˆØ±Ù‹Ø§ Ø¨Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ÙŠØ¯Ø§Ù†ÙŠØ©ØŒ ÙˆØ§Ù†ØªÙ‡Ø§Ø¡Ù‹ Ø¨Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØ¥ØµØ¯Ø§Ø± Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©.",
#         "keywords": ["Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¬Ø¯ÙŠØ¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©", "ÙØ§ØªÙˆØ±Ø©", "Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙÙŠ ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ù…Ø­ØµÙˆÙ„ÙŠ Ø§Ù„Ù‚Ø§Øª ÙˆØ§Ù„Ø¹Ù†Ø¨.",
#         "keywords": ["ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¨Ø§Øª", "Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©", "Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "plant care", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø²Ø±Ø§Ø¹Ø©"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø§Ø¯Ø© ØªØ¹Ù„ÙŠÙ…ÙŠØ© ØªØ´Ø±Ø­ Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŒ Ù…ÙƒØªØ¨Ø© TensorFlowØŒ ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN)ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ø·Ø¨Ù‚Ø§ØªØŒ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ·ØŒ ÙˆØ®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "convolutional layer", "relu", "pooling", "dense layer", "loss function", "optimizer", "backpropagation"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© (UNOPS eSourcing)",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù…ÙƒØªØ¨ Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (UNOPS)ØŒ ÙˆÙŠØ´Ù…Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ØŒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ§ØªØŒ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡Ø§Øª.",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing", "ungm.org", "Ù…ÙˆØ±Ø¯ÙŠÙ†", "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ©"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0: Ø§Ù„Ø¨Ø³Ø§Ø·Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©) ---
# REWRITE_PROMPT_TEMPLATE = """
# Ù…Ù‡Ù…ØªÙƒ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©: Ø­ÙˆÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø© Ø¨Ø­Ø« Ù‚ØµÙŠØ±Ø© ÙˆÙ…Ø±ÙƒØ²Ø©.

# **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…:**
# - Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name}
# - ÙˆØµÙÙ‡: {system_description}
# - Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©: {system_keywords}

# ---
# **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙƒØ³Ø±Ù‡Ø§:**
# 1.  **Ø§Ù„Ù†Ø§ØªØ¬ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·:** ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø§ØªØ¬ Ø¬Ù…Ù„Ø© Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙˆØ¬Ø²Ø©.
# 2.  **Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙŠØ©:** Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ø¨Ù†Ø§Ø¡ Ø¬Ù…Ù„Ø© ØªØ¹Ø¨Ø± Ø¹Ù† Ø§Ù„Ù‚ØµØ¯.
# 3.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ø¸Ø§Ù…:** (Ù…Ø«Ù„ "Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ")ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ø§ØªØ¬ "ÙˆØµÙ {system_name}".
# 4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§:** (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ")ØŒ **Ø£Ø¹Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø·.**
# 5.  **Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø´Ø±Ø­:** Ù„Ø§ ØªÙ‚Ù… Ø£Ø¨Ø¯Ù‹Ø§ Ø¨Ø´Ø±Ø­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ. Ø§Ù„Ù†Ø§ØªØ¬ Ù‡Ùˆ Ø¬Ù…Ù„Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙ‚Ø·.

# ---
# **Ø£Ù…Ø«Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØµØ­ÙŠØ­:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: ÙˆØµÙ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ÙƒÙŠÙ Ø§Ø¶ÙŠÙ Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ø®Ø·ÙˆØ§Øª Ø¥Ø¶Ø§ÙØ© Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§ØŸ
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§ØŸ
# ---

# **Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:
# """

# # --- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template("Ø£Ù†Øª \"Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…\"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {input}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# def _clean_rewritten_query(raw_query: str) -> str:
#     lines = raw_query.strip().split('\n')
#     for line in reversed(lines):
#         cleaned_line = line.strip()
#         if cleaned_line:
#             if cleaned_line.startswith("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"):
#                 return cleaned_line.replace("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:", "").strip()
#             return cleaned_line
#     return raw_query

# async def initialize_agent():
#     global llm, vector_store, reranker
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ùˆ Reranker...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
            
#             reranker = Ranker()
            
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ (Ù…Ø¹ Reranker).")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None and reranker is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 10)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         effective_question = question
#         profile = SYSTEM_PROFILES.get(tenant_id)
        
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„...")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             # Ù‡Ù†Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ØªØºÙŠØ± {Ø§Ù„ÙØ¹Ù„}ØŒ Ù„Ø°Ø§ Ù„Ù† ÙŠØ­Ø¯Ø« Ø§Ù„Ø®Ø·Ø£
#             raw_rewritten_query = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
            
#             effective_question = _clean_rewritten_query(raw_rewritten_query)
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")

#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
#         ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù€ '{effective_question}'...")
#         initial_docs = await ensemble_retriever.ainvoke(effective_question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(initial_docs)} Ù…Ø³ØªÙ†Ø¯ Ø£ÙˆÙ„ÙŠ.")

#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©...")
        
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
#         rerank_request = RerankRequest(query=question, passages=passages)
#         all_reranked_results = reranker.rerank(rerank_request)
#         top_4_results = all_reranked_results[:4]
        
#         original_docs_map = {doc.page_content: doc for doc in initial_docs}
#         reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
#         logging.info(f"[{session_id}] ØªÙ… ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ {len(reranked_docs)} Ù…Ø³ØªÙ†Ø¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØµÙ„Ø©.")

#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
        
#         async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}

#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}


# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0: Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ---

# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.chains.combine_documents import create_stuff_documents_chain

# from flashrank import Ranker, RerankRequest

# from .performance_tracker import PerformanceLogger

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù„Ù„Ø£Ù†Ø¸Ù…Ø© ---
# SYSTEM_PROFILES = {
#     "sys": {
#         "name": "Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
#         "description": "Ù†Ø¸Ø§Ù… Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù„ØªØªØ¨Ø¹ Ø±Ø­Ù„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯.",
#         "keywords": ["Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø·Ù„Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯", "Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ­Ù‚Ù‚", "Ø¯Ø±Ø§Ø³Ø© Ù…ÙƒØªØ¨ÙŠØ©", "Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©", "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØµØ­ÙŠØ­ÙŠØ©", "ÙØ§ØªÙˆØ±Ø©", "Ø´Ù‡Ø§Ø¯Ø©"]
#     },
#     "university_alpha": {
#         "name": "ØªØ·Ø¨ÙŠÙ‚ Plant Care",
#         "description": "ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒÙŠ Ù„ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©.",
#         "keywords": ["ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¨Ø§Øª", "Ø¢ÙØ§Øª Ø²Ø±Ø§Ø¹ÙŠØ©", "Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ¸ÙŠÙÙŠØ©", "Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…", "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…", "plant care"]
#     },
#     "school_beta": {
#         "name": "Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
#         "description": "Ù…Ø§Ø¯Ø© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ùˆ TensorFlow.",
#         "keywords": ["Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©", "tensorflow", "cnn", "layer", "relu", "pooling", "optimizer"]
#     },
#     "un": {
#         "name": "Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©",
#         "description": "Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯ÙŠÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.",
#         "keywords": ["Ù…Ù†Ø§Ù‚ØµØ§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "Ø¹Ø·Ø§Ø¡Ø§Øª", "unops", "esourcing", "ungm.org", "Ù…ÙˆØ±Ø¯ÙŠÙ†"]
#     }
# }

# # --- 3. Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©) ---
# REWRITE_PROMPT_TEMPLATE = """
# Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ© Ù…Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«.

# **Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…:** {system_name}
# **Ù…ØµØ·Ù„Ø­Ø§Øª Ù‡Ø§Ù…Ø©:** {system_keywords}

# ---
# **Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:**
# 1.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…Ù‹Ø§ Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…** (Ù…Ø«Ù„ "Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ø³Ù… Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙ‚Ø·: `{system_name}`.
# 2.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø®Ø·ÙˆØ§Øª Ø£Ùˆ ÙƒÙŠÙÙŠØ© ÙØ¹Ù„ Ø´ÙŠØ¡** (Ù…Ø«Ù„ "ÙƒÙŠÙ Ø£Ø¶ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù…ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„ÙØ¹Ù„ ÙˆØ§Ù„Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡: `Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯`.
# 3.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† ØªØ¹Ø±ÙŠÙ Ù…ØµØ·Ù„Ø­** (Ù…Ø«Ù„ "Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù…ØµØ·Ù„Ø­ Ù†ÙØ³Ù‡: `Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©`.
# 4.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§** (Ù…Ø«Ù„ "Ù…Ù† Ù‡Ùˆ Ù…ÙŠØ³ÙŠØŸ")ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ.
# 5.  **Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚ØµÙŠØ±Ù‹Ø§ Ø¬Ø¯Ù‹Ø§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ù‹Ø§.** Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¬Ù…Ù„ ÙƒØ§Ù…Ù„Ø©.

# ---
# **Ø£Ù…Ø«Ù„Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ø®ØªØµØ§Ø±
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ÙƒÙŠÙÙŠÙ‡ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù†Ø¸Ø§Ù…
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: ÙƒÙŠÙÙŠØ© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ø§Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠÙ‡
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§
# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: Ù…Ù† Ù‡ÙŠ Ø¬ÙˆØ±Ø¬ÙŠÙ†Ø§
# ---

# **Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**

# Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:
# """

# # --- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# ANSWER_PROMPT = ChatPromptTemplate.from_template("Ø£Ù†Øª \"Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…\"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ \"Ø§Ù„Ø³ÙŠØ§Ù‚\" Ø§Ù„Ù…Ù‚Ø¯Ù….\n- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….\n- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: \"Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {input}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
# initialization_lock = asyncio.Lock()
# perf_logger = PerformanceLogger()

# # --- 5. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø¹Ø¸Ù…Ù‡Ø§ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ) ---

# def _load_all_docs_from_faiss(vs: FAISS) -> List[Document]:
#     return list(vs.docstore._dict.values())

# def _clean_rewritten_query(raw_query: str) -> str:
#     lines = raw_query.strip().split('\n')
#     for line in reversed(lines):
#         cleaned_line = line.strip()
#         if cleaned_line:
#             if cleaned_line.startswith("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:"):
#                 return cleaned_line.replace("Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†:", "").strip()
#             return cleaned_line
#     return raw_query.strip()

# async def initialize_agent():
#     global llm, vector_store, reranker
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ùˆ Reranker...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
            
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
            
#             reranker = Ranker()
            
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ (Ù…Ø¹ Reranker).")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return vector_store is not None and llm is not None and reranker is not None

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     k_results = request_info.get("k_results", 10)
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     user_chat_history = chat_history.get(session_id, [])

#     try:
#         effective_question = question
#         profile = SYSTEM_PROFILES.get(tenant_id)
        
#         if profile:
#             logging.info(f"[{session_id}] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø´Ø®ØµÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„...")
#             rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_PROMPT_TEMPLATE)
#             rewriter_chain = rewrite_prompt | llm | StrOutputParser()
            
#             raw_rewritten_query = await rewriter_chain.ainvoke({
#                 "system_name": profile.get("name", ""),
#                 "system_description": profile.get("description", ""),
#                 "system_keywords": ", ".join(profile.get("keywords", [])),
#                 "question": question
#             })
            
#             effective_question = _clean_rewritten_query(raw_rewritten_query)
#             logging.info(f"[{session_id}] Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: '{question}' -> Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†: '{effective_question}'")

#         all_docs = _load_all_docs_from_faiss(vector_store)
#         tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]

#         if not tenant_docs:
#             yield {"type": "error", "content": f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
#             return

#         bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=k_results)
#         faiss_retriever = vector_store.as_retriever(
#             search_kwargs={'k': k_results, 'filter': {'tenant_id': tenant_id}}
#         )
#         ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù€ '{effective_question}'...")
#         initial_docs = await ensemble_retriever.ainvoke(effective_question)
#         logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(initial_docs)} Ù…Ø³ØªÙ†Ø¯ Ø£ÙˆÙ„ÙŠ.")

#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©...")
        
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(initial_docs)]
        
#         rerank_request = RerankRequest(query=question, passages=passages)
#         all_reranked_results = reranker.rerank(rerank_request)
#         top_4_results = all_reranked_results[:4]
        
#         original_docs_map = {doc.page_content: doc for doc in initial_docs}
#         reranked_docs = [original_docs_map[res["text"]] for res in top_4_results if res["text"] in original_docs_map]
        
#         logging.info(f"[{session_id}] ØªÙ… ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ {len(reranked_docs)} Ù…Ø³ØªÙ†Ø¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØµÙ„Ø©.")

#         document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
        
#         logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
#         full_answer = ""
        
#         async for chunk in document_chain.astream({"input": question, "context": reranked_docs, "chat_history": user_chat_history}):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}

#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}





#ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ 
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/agent_logic.py (Ø§Ø³Ù… Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø«)
##Hybrid + Parent + Reranke
#Ø¨Ø·ÙŠ Ø¨Ù„Ù†Ù‡ ÙŠØ«ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ù…Ø³ØªØ±Ø¤Ø¬Ø¹Ø§Øª Ù…Ø³Ø¨Ù‚Ø§ Ø¨Ø­ÙŠ3Ø« ÙŠØ³Ù‡Ù„ ÙˆÙŠØ³Ø±Ø¹ Ø¹Ù…Ù„ÙŠÙ‡ Ø§Ù„Ø¨Ø­Ø« 
# import os
# import logging
# import asyncio
# import pickle
# import time
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_community.llms import Ollama
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.retrievers import BM25Retriever, EnsembleRetriever
# from langchain.storage import InMemoryStore
# from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from flashrank import Ranker, RerankRequest

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©) ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# CLASSIFIER_MODEL = os.getenv("CLASSIFIER_MODEL_NAME", "qwen2:1.5b-instruct-q4_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
# CACHE_DIR = os.path.join(PROJECT_ROOT, "3_shared_resources", "retriever_cache") # <-- Ù…Ø³Ø§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
# TOP_K = 7

# # --- 2. Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ (Prompts) ---

# # Ù‚Ø§Ù„Ø¨ "Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©" Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
# QUESTION_CLASSIFIER_PROMPT = """
# Your task is to classify the user's question into one of three categories: "specific_query", "general_chitchat", or "nonsensical".
# - "specific_query": The user is asking a specific question that can likely be answered from a knowledge base (e.g., "how do I reset my password?", "what is max pooling?").
# - "general_chitchat": The user is asking a general knowledge question or making a greeting (e.g., "hello", "who is the president?", "what is the weather?").
# - "nonsensical": The user's input is random characters, gibberish, or makes no sense (e.g., "asdfgh", "blablabla", "qwertyy").

# User Question: "{question}"
# Category:
# """

# # Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
# DYNAMIC_PROMPT_TEMPLATE = """
# Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ùƒ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.

# **Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
# 1.  **Ø§Ù„ØªØ­ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§:** Ø§Ø¨Ø¯Ø£ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø¹Ø¨Ø§Ø±Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©.
# 2.  **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø·Ù„Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:** Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù‚Ù„ **ÙÙ‚Ø·**: "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# 3.  **Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ({verbosity}):**
#     - **"Ù…Ø®ØªØµØ±"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬Ø²Ø© ÙÙŠ Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†.
#     - **"Ù…ÙØµÙ„"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù….
# 4.  **Ø§Ù„Ø®Ø§ØªÙ…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø®ØªØªÙ… Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø³Ø¤Ø§Ù„ ØªÙØ§Ø¹Ù„ÙŠØŒ Ù…Ø«Ù„: "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ù‡ØŸ".

# ---
# **Ø§Ù„Ø³ÙŠØ§Ù‚:**
# {context}
# ---
# **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {question}
# ---
# **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** {verbosity}
# ---
# **Ø¥Ø¬Ø§Ø¨ØªÙƒ:**
# """

# # --- 3. ÙØ¦Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª) ---
# class RetrieverManager:
#     def __init__(self, vector_store: FAISS):
#         self._vector_store = vector_store
#         self._cache = self._load_or_build_cache()

#     def _load_or_build_cache(self) -> Dict:
#         os.makedirs(CACHE_DIR, exist_ok=True)
#         cache_file = os.path.join(CACHE_DIR, "retriever_cache.pkl")
        
#         if os.path.exists(cache_file):
#             print("ğŸ§  Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© (Cache)...")
#             with open(cache_file, "rb") as f:
#                 return pickle.load(f)
        
#         print("âš ï¸ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ù†Ø§Ø¡ (Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§)...")
#         all_docs = list(self._vector_store.docstore._dict.values())
        
#         all_tenant_docs: Dict[str, List[Document]] = {}
#         for doc in all_docs:
#             tenant_id = doc.metadata.get("tenant_id")
#             if tenant_id:
#                 if tenant_id not in all_tenant_docs:
#                     all_tenant_docs[tenant_id] = []
#                 all_tenant_docs[tenant_id].append(doc)

#         new_cache = {}
#         for tenant_id, docs in all_tenant_docs.items():
#             print(f"   -> Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
#             new_cache[tenant_id] = {}
            
#             new_cache[tenant_id]['bm25'] = BM25Retriever.from_documents(docs)
            
#             store = InMemoryStore()
#             parent_retriever = ParentDocumentRetriever(
#                 vectorstore=self._vector_store, 
#                 docstore=store, 
#                 child_splitter=RecursiveCharacterTextSplitter(chunk_size=400)
#             )
#             parent_retriever.add_documents(docs, ids=None)
#             new_cache[tenant_id]['parent'] = parent_retriever
        
#         with open(cache_file, "wb") as f:
#             pickle.dump(new_cache, f)
#         print("âœ… Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
#         return new_cache

#     def get_retrievers(self, tenant_id: str) -> Dict:
#         if tenant_id not in self._cache:
#             raise ValueError(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø®Ø²Ù†Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
        
#         bm25_retriever = self._cache[tenant_id]['bm25']
#         parent_retriever = self._cache[tenant_id]['parent']
#         faiss_retriever = self._vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
        
#         hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        
#         return {"hybrid": hybrid_retriever, "parent": parent_retriever}

# # --- 4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ù‡ÙŠÙƒÙ„Ø© ---
# llm_answer: Ollama = None
# llm_classifier: Ollama = None
# vector_store: FAISS = None
# reranker: Ranker = None
# retriever_manager: RetrieverManager = None
# chat_history: Dict[str, List] = {}
# initialization_lock = asyncio.Lock()

# # --- 5. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
# async def initialize_agent():
#     global llm_answer, llm_classifier, vector_store, reranker, retriever_manager
#     async with initialization_lock:
#         if retriever_manager is not None: return
        
#         logging.info("--- ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ (v-Final) ---")
#         try:
#             # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
#             llm_answer = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             llm_classifier = Ollama(model=CLASSIFIER_MODEL, base_url=OLLAMA_HOST)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             # ØªÙ‡ÙŠØ¦Ø© Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
#             vector_store = await asyncio.to_thread(FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
#             reranker = Ranker()
            
#             # ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª)
#             retriever_manager = RetrieverManager(vector_store)
            
#             logging.info("--- âœ… Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ ---")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# def agent_ready() -> bool:
#     return retriever_manager is not None

# # --- 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„) ---

# def _get_verbosity(question: str) -> str:
#     """ÙŠØ­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨."""
#     question_lower = question.lower()
#     if any(word in question_lower for word in ["Ø¨Ø§Ø®ØªØµØ§Ø±", "Ù…ÙˆØ¬Ø²"]):
#         return "Ù…Ø®ØªØµØ±"
#     return "Ù…ÙØµÙ„"

# async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
#     question = request_info.get("question", "")
#     tenant_id = request_info.get("tenant_id", "default_session")
#     session_id = tenant_id or "default_session"

#     if not agent_ready():
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ø¨Ø¹Ø¯ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."}
#         return

#     try:
#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© (Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³Ø¨Ù‚) ---
#         classifier_prompt = ChatPromptTemplate.from_template(QUESTION_CLASSIFIER_PROMPT)
#         classifier_chain = classifier_prompt | llm_classifier | StrOutputParser()
#         classification_result = await classifier_chain.ainvoke({"question": question})
#         classification = classification_result.strip().lower()
        
#         if "general_chitchat" in classification:
#             yield {"type": "full_answer", "content": "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙˆÙ„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ"}
#             return
#         if "nonsensical" in classification:
#             yield {"type": "full_answer", "content": "Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ØŸ"}
#             return

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©) ---
#         retrievers = retriever_manager.get_retrievers(tenant_id)
#         hybrid_retriever = retrievers['hybrid']
#         parent_retriever = retrievers['parent']
        
#         hybrid_docs, parent_docs = await asyncio.gather(
#             hybrid_retriever.ainvoke(question),
#             asyncio.to_thread(parent_retriever.invoke, question)
#         )
        
#         combined_docs = hybrid_docs + parent_docs
#         unique_docs = list({doc.page_content: doc for doc in reversed(combined_docs)}.values())[::-1]

#         if not unique_docs:
#             yield {"type": "full_answer", "content": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."}
#             return

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Reranking) ---
#         passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
#         reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
#         top_results = reranked_results[:4]
        
#         original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
#         final_context_docs = [original_docs_map[res["id"]] for res in top_results]
#         final_context = "\n\n---\n\n".join([doc.page_content for doc in final_context_docs])

#         # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Streaming) ---
#         answer_prompt = ChatPromptTemplate.from_template(DYNAMIC_PROMPT_TEMPLATE)
#         answer_chain = answer_prompt | llm_answer | StrOutputParser()
#         verbosity = _get_verbosity(question)
        
#         full_answer = ""
#         async for chunk in answer_chain.astream({
#             "context": final_context,
#             "question": question,
#             "verbosity": verbosity
#         }):
#             if chunk:
#                 full_answer += chunk
#                 yield {"type": "chunk", "content": chunk}
        
#         # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         user_chat_history = chat_history.get(session_id, [])
#         user_chat_history.extend([HumanMessage(content=question), AIMessage(content=full_answer)])
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")

#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}
# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/agent_logic.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: Ø¨Ù†Ø§Ø¡ ÙÙˆØ±ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ (Ø¨Ø¯ÙˆÙ† ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª) ---
# main_rag_chain.py - Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ù€ RAG Ø§Ù„Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©
# 2_central_api_service/agent_app/core_logic.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© v4.0 - Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹)

import os
import logging
import asyncio
from typing import AsyncGenerator, Dict, List

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] - %(message)s')
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b") 
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
ANSWER_PROMPT = ChatPromptTemplate.from_template(
    """
Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙØ§Ø±ØºÙ‹Ø§ (Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª)ØŒ Ø§Ø¹ØªØ°Ø± Ø¨Ù„Ø·Ù Ø¹Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.
- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.
Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}
Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""
)

# --- **Ø§Ù„ØªØ­Ø³ÙŠÙ† 1: Ø¥Ø¶Ø§ÙØ© Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©** ---
FAST_PATH_RESPONSES = {
    "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
    "Ù…Ø±Ø­Ø¨Ø§": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒØŸ",
    "Ø£Ù‡Ù„Ø§": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒØŸ",
    "Ø´ÙƒØ±Ø§": "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ù‡ØŸ",
    "Ø´ÙƒØ±Ø§ Ù„Ùƒ": "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ù‡ØŸ",
    "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©": "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø§ÙÙŠÙƒ. ÙÙŠ Ø®Ø¯Ù…ØªÙƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§.",
    "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„Ø³Ø¤Ø§Ù„Ùƒ! Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.",
}
# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ù„ Ø¹Ù„Ù‰ ØªØ­ÙŠØ© Ø£Ùˆ Ø­Ø¯ÙŠØ« Ù‚ØµÙŠØ±
SMALL_TALK_KEYWORDS = [
    "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù…Ø±Ø­Ø¨Ø§", "Ø£Ù‡Ù„Ø§", "Ø´ÙƒØ±Ø§", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ",
    "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±"
]

# --- 3. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Cache) ---
# (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ)
llm: Ollama = None
vector_store: FAISS = None
retrievers_cache: Dict[str, EnsembleRetriever] = {}
initialization_lock = asyncio.Lock()

# --- 4. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ---
# (Ø¯Ø§Ù„Ø© initialize_agent ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)
async def initialize_agent():
    """
    ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„.
    """
    global llm, vector_store, retrievers_cache
    async with initialization_lock:
        if llm is not None: return

        logging.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„...")
        try:
            llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.0)
            logging.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ: {CHAT_MODEL}.")
            embeddings_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={'device': 'cpu'}
            )
            logging.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {EMBEDDING_MODEL_NAME}.")
            if not os.path.isdir(UNIFIED_DB_PATH):
                raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {UNIFIED_DB_PATH}")
            
            vector_store = await asyncio.to_thread(
                FAISS.load_local, UNIFIED_DB_PATH, embeddings_model, allow_dangerous_deserialization=True
            )
            logging.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­.")
            logging.info("Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„...")
            all_docs = list(vector_store.docstore._dict.values())
            
            tenant_docs_map = {}
            for doc in all_docs:
                tenant_id = doc.metadata.get("tenant_id")
                if tenant_id:
                    if tenant_id not in tenant_docs_map:
                        tenant_docs_map[tenant_id] = []
                    tenant_docs_map[tenant_id].append(doc)

            for tenant_id, docs in tenant_docs_map.items():
                bm25_retriever = BM25Retriever.from_documents(docs)
                faiss_retriever = vector_store.as_retriever(
                    search_type="similarity",
                    search_kwargs={'k': 5, 'filter': {'tenant_id': tenant_id}}
                )
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever],
                    weights=[0.3, 0.7]
                )
                retrievers_cache[tenant_id] = ensemble_retriever
                logging.info(f"  -> ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")

            logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨ÙƒØ§Ù…Ù„ Ø·Ø§Ù‚ØªÙ‡.")
        except Exception as e:
            logging.critical(f"âŒ ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            llm, vector_store, retrievers_cache = None, None, {}
            raise

def agent_ready() -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ Ù‚Ø¯ ØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„."""
    return llm is not None and vector_store is not None and bool(retrievers_cache)

# --- 5. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹) ---

async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ù…Ø¹ Ù…Ø±Ø´Ø­ Ù„Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©.
    """
    session_id = request_info.get("tenant_id", "unknown_session")
    question = request_info.get("question", "").strip()
    tenant_id = request_info.get("tenant_id")

    # --- **Ø§Ù„ØªØ­Ø³ÙŠÙ† 2: ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ø´Ø­ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹** ---
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ Ø£ÙˆÙ„Ø§Ù‹
    if question in FAST_PATH_RESPONSES:
        logging.info(f"[{session_id}] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'")
        yield {"type": "chunk", "content": FAST_PATH_RESPONSES[question]}
        return # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ° ÙÙˆØ±Ù‹Ø§

    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    for keyword in SMALL_TALK_KEYWORDS:
        if keyword in question:
            logging.info(f"[{session_id}] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ù‚ØµÙŠØ±: '{keyword}'")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„ØªØ­ÙŠØ§Øª
            yield {"type": "chunk", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"}
            return # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ° ÙÙˆØ±Ù‹Ø§

    # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­Ø¯ÙŠØ«Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§) ---
    try:
        logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ (RAG) Ù„Ù„Ø³Ø¤Ø§Ù„: '{question}'...")
        
        ensemble_retriever = retrievers_cache.get(tenant_id)
        if not ensemble_retriever:
            yield {"type": "error", "content": f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ù‡ÙŠØ£ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."}
            return

        retrieved_docs = await ensemble_retriever.ainvoke(question)
        logging.info(f"[{session_id}] ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(retrieved_docs)} Ù…Ø³ØªÙ†Ø¯.")

        logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
        
        answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
        
        async for chunk in answer_chain.astream({
            "input": question, 
            "context": retrieved_docs
        }):
            if chunk:
                yield {"type": "chunk", "content": chunk}

    except Exception as e:
        logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG: {e}", exc_info=True)
        yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."}
