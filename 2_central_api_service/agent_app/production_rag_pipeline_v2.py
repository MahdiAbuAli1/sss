# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/production_rag_pipeline_v2.py
# --- Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ "Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø±Ù†" ---
#Ù„Ø­ÙƒÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ v2
#Ø§Ù„ÙƒÙˆØ¯ v2 ÙŠÙ†ØªØ¬ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆÙ„ÙƒÙ†Ù‡ ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ø¨Ø³Ø¨Ø¨ Ø¨Ø·Ø¦Ù‡ Ø§Ù„Ø´Ø¯ÙŠØ¯. Ù„Ù‚Ø¯ Ù†Ø¬Ø­Ù†Ø§ ÙÙŠ Ø¨Ù†Ø§Ø¡ "Ø¹Ù‚Ù„ Ø°ÙƒÙŠ" ÙˆÙ„ÙƒÙ†Ù‡ "Ø¹Ù‚Ù„ Ø¨Ø·ÙŠØ¡".
#
import asyncio
import os
from typing import List, Dict
from dotenv import load_dotenv

# --- (Ø¬Ù…ÙŠØ¹ Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ---
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.storage import InMemoryStore
from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from flashrank import Ranker, RerankRequest

# --- (Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ---
load_dotenv()
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
TOP_K = 7

# --- 3. Ù‚Ø§Ù„Ø¨ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ (v2) ---
DYNAMIC_PROMPT_TEMPLATE = """
Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ùƒ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.

**Ø´Ø®ØµÙŠØªÙƒ:**
- **Ø®Ø¨ÙŠØ± ÙˆÙ…ÙˆØ«ÙˆÙ‚:** ÙˆØ§Ø«Ù‚ Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ ÙˆØ¯Ù‚ÙŠÙ‚. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø§Øª.
- **Ù…Ø³Ø§Ø¹Ø¯ ÙˆÙ…Ø±Ù†:** Ù‡Ø¯ÙÙƒ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙŠ ÙŠÙØ¶Ù„Ù‡Ø§.

**Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙƒØ³Ø±Ù‡Ø§:**
1.  **Ø§Ù„ØªØ­ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§:** Ø§Ø¨Ø¯Ø£ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø¹Ø¨Ø§Ø±Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© (Ù…Ø«Ø§Ù„: "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ!"ØŒ "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ØŒ Ø¨Ø®ØµÙˆØµ Ø³Ø¤Ø§Ù„Ùƒ...").
2.  **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø·Ù„Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:**
    - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø£Ø¬Ø¨ Ø¹Ù„ÙŠÙ‡Ø§.
    - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© **ØªÙ…Ø§Ù…Ù‹Ø§** ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ„ **ÙÙ‚Ø·**: "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
3.  **Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ({verbosity}):**
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ùˆ **"Ù…Ø®ØªØµØ±"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…ÙˆØ¬Ø²Ø© ÙÙŠ Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†.
    - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ùˆ **"Ù…ÙØµÙ„"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø·ÙŠØ© Ø£Ùˆ Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø£Ùˆ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©.
4.  **Ø§Ù„Ø§Ø®ØªØµØ§Ø±:** Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø¨Ø¯Ù‹Ø§ ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚" Ø£Ùˆ "ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª".
5.  **Ø§Ù„Ø®Ø§ØªÙ…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø®ØªØªÙ… Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø³Ø¤Ø§Ù„ ØªÙØ§Ø¹Ù„ÙŠØŒ Ù…Ø«Ù„: "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ù‡ØŸ".

---
**Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ù…ØµØ¯Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„ÙˆØ­ÙŠØ¯):**
{context}
---
**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**
{question}
---
**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
{verbosity}
---
**Ø¥Ø¬Ø§Ø¨ØªÙƒ (Ù…Ø¹ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„ØªØ­ÙŠØ©ØŒ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ØŒ ÙˆØ§Ù„Ø®Ø§ØªÙ…Ø©):**
"""

# --- 4. Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© (Ù…Ø¹ Ù…Ù†Ø·Ù‚ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙØµÙŠÙ„) ---
class RAGPipeline:
    def __init__(self):
        # ... (Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ...
        print("--- ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ RAG Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ (v2) ---")
        self.llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        self.vector_store = FAISS.load_local(UNIFIED_DB_PATH, self.embeddings, allow_dangerous_deserialization=True)
        self.reranker = Ranker()
        
        all_docs = list(self.vector_store.docstore._dict.values())
        self.all_tenant_docs = {}
        for doc in all_docs:
            tenant_id = doc.metadata.get("tenant_id")
            if tenant_id:
                if tenant_id not in self.all_tenant_docs:
                    self.all_tenant_docs[tenant_id] = []
                self.all_tenant_docs[tenant_id].append(doc)
        
        self.final_prompt = ChatPromptTemplate.from_template(DYNAMIC_PROMPT_TEMPLATE)
        self.answer_chain = self.final_prompt | self.llm | StrOutputParser()
        print("--- âœ… Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¬Ø§Ù‡Ø² ---")

    def _get_verbosity(self, question: str) -> str:
        """ÙŠØ­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„."""
        question_lower = question.lower()
        if any(word in question_lower for word in ["Ø¨Ø§Ø®ØªØµØ§Ø±", "Ù…ÙˆØ¬Ø²", "Ù‡Ù„ ÙŠÙ…ÙƒÙ†"]):
            return "Ù…Ø®ØªØµØ±"
        # "Ø§Ø´Ø±Ø­"ØŒ "Ø¨Ø§Ù„ØªÙØµÙŠÙ„"ØŒ "Ù…Ø§ Ù‡Ùˆ" Ø³ØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return "Ù…ÙØµÙ„"

    async def get_answer(self, question: str, tenant_id: str) -> str:
        print(f"\n[>>] ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}': '{question}'")

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„" ---
        # ... (ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ...
        print("[1/2] ğŸ” ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„...")
        tenant_docs = self.all_tenant_docs.get(tenant_id)
        if not tenant_docs: return "Ø®Ø·Ø£: Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„."
        faiss_retriever = self.vector_store.as_retriever(search_type="similarity", search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
        bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=TOP_K)
        ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])
        store = InMemoryStore()
        parent_document_retriever = ParentDocumentRetriever(vectorstore=self.vector_store, docstore=store, child_splitter=RecursiveCharacterTextSplitter(chunk_size=400))
        parent_document_retriever.add_documents(tenant_docs, ids=None)
        hybrid_docs = await ensemble_retriever.ainvoke(question)
        parent_docs = await asyncio.to_thread(parent_document_retriever.invoke, question)
        combined_initial_docs = hybrid_docs + parent_docs
        unique_docs_map = {doc.page_content: doc for doc in reversed(combined_initial_docs)}
        unique_docs = list(unique_docs_map.values())[::-1]
        if not unique_docs: return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø©."
        passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
        reranked_results = self.reranker.rerank(RerankRequest(query=question, passages=passages))
        top_results = reranked_results[:4]
        original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
        final_context_docs = [original_docs_map[res["id"]] for res in top_results]
        final_context = "\n\n---\n\n".join([doc.page_content for doc in final_context_docs])
        
        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ---
        verbosity = self._get_verbosity(question)
        print(f"[2/2] ğŸ§  ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„: {verbosity})...")
        
        final_answer = await self.answer_chain.ainvoke({
            "context": final_context,
            "question": question,
            "verbosity": verbosity  # ØªÙ…Ø±ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù„Ø¨
        })
        
        return final_answer

# --- 5. Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© (Ù…Ø¹ Ø£Ø³Ø¦Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±ÙˆÙ†Ø©) ---
async def main():
    pipeline = RAGPipeline()

    # --- Ø£Ø³Ø¦Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø­ÙˆØ§Ø±ÙŠ ---
    test_cases = [
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØµÙ„ (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        {"tenant_id": "school_beta", "question": "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…Ø§ Ù‡ÙŠ Ø·Ø¨Ù‚Ø© Ø§Ù„Ù€ pooling ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø®ØªØµØ±Ø©
        {"tenant_id": "school_beta", "question": "Ø¨Ø§Ø®ØªØµØ§Ø±ØŒ Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø·Ø¨Ù‚Ø© Ø§Ù„Ù€ poolingØŸ"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª (Ø§Ù„Ù‡Ù„ÙˆØ³Ø©)
        {"tenant_id": "un", "question": "Ù…Ø§ Ù‡Ùˆ Ø³Ø¹Ø± Ø³Ù‡Ù… Ø´Ø±ÙƒØ© Ø£Ø¨Ù„ Ø§Ù„ÙŠÙˆÙ…ØŸ"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø¤Ø§Ù„ Ù…Ø±ÙƒØ¨ (ÙŠØªØ·Ù„Ø¨ ØªÙØµÙŠÙ„)
        {"tenant_id": "university_alpha", "question": "Ù…Ø§ Ù‡ÙŠ Ø£Ù‡Ø¯Ø§Ù Ù…Ø´Ø±ÙˆØ¹ Plant Care ÙˆÙƒÙŠÙ ÙŠØ®ØªÙ„Ù Ø¹Ù† ØªØ·Ø¨ÙŠÙ‚ PlantixØŸ"}
    ]

    for case in test_cases:
        answer = await pipeline.get_answer(question=case["question"], tenant_id=case["tenant_id"])
        print("\n" + "="*30 + " ğŸ’¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸ’¬ " + "="*30)
        print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {case['question']}")
        print(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n{answer}")
        print("="*86)

if __name__ == "__main__":
    asyncio.run(main())
