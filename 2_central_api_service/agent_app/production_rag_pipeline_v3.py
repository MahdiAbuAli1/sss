# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/production_rag_pipeline_v3.py
# --- Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³Ø¨Ù‚ ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ---

import asyncio
import os
import time
import re

from typing import Dict, List
from dotenv import load_dotenv

# --- 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ---
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.storage import InMemoryStore
from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from flashrank import Ranker, RerankRequest

# --- 2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
load_dotenv()
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
CLASSIFIER_MODEL = os.getenv("CLASSIFIER_MODEL_NAME", "qwen2:1.5b-instruct-q4_K_M") # Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ± ÙˆØ³Ø±ÙŠØ¹ Ù„Ù„ØªØµÙ†ÙŠÙ
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
TOP_K = 7

# --- 3. Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ (Prompts) ---

# Ù‚Ø§Ù„Ø¨ "Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©" Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
QUESTION_CLASSIFIER_PROMPT = """
Your task is to classify the user's question into one of three categories: "specific_query", "general_chitchat", or "nonsensical".
- "specific_query": The user is asking a specific question that can likely be answered from a knowledge base (e.g., "how do I reset my password?", "what is max pooling?").
- "general_chitchat": The user is asking a general knowledge question or making a greeting (e.g., "hello", "who is the president?", "what is the weather?").
- "nonsensical": The user's input is random characters, gibberish, or makes no sense (e.g., "asdfgh", "blablabla", "qwertyy").

User Question: "{question}"
Category:
"""

# Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ (Ù„Ø§ ØªØºÙŠÙŠØ±)
DYNAMIC_PROMPT_TEMPLATE = """
Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ"... 
(Ù†ÙØ³ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ù…Ù† v2)
"""

# --- 4. ÙØ¦Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª) ---
class RetrieverManager:
    """
    ÙØ¦Ø© Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¥Ø¯Ø§Ø±Ø© ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ø¤Ù‚Øª Ù„ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙƒÙ„ÙØ©.
    """
    def __init__(self, vector_store: FAISS, all_tenant_docs: Dict[str, List[Document]]):
        self._vector_store = vector_store
        self._all_tenant_docs = all_tenant_docs
        self._retriever_cache: Dict[str, Dict[str, any]] = {}
        print("ğŸ§  Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©...")
        self._build_cache()

    def _build_cache(self):
        """ÙŠÙ‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ ÙˆØªØ®Ø²ÙŠÙ† ÙƒØ§Ø¦Ù†Ø§Øª BM25 Ùˆ ParentDocument Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„."""
        for tenant_id, docs in self._all_tenant_docs.items():
            if tenant_id not in self._retriever_cache:
                self._retriever_cache[tenant_id] = {}
            
            # ØªØ®Ø²ÙŠÙ† BM25Retriever
            self._retriever_cache[tenant_id]['bm25'] = BM25Retriever.from_documents(docs, k=TOP_K)
            
            # ØªØ®Ø²ÙŠÙ† ParentDocumentRetriever
            store = InMemoryStore()
            parent_retriever = ParentDocumentRetriever(
                vectorstore=self._vector_store, 
                docstore=store, 
                child_splitter=RecursiveCharacterTextSplitter(chunk_size=400)
            )
            parent_retriever.add_documents(docs, ids=None)
            self._retriever_cache[tenant_id]['parent'] = parent_retriever
        print(f"âœ… Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª: ØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù„Ù€ {len(self._retriever_cache)} Ø¹Ù…ÙŠÙ„.")

    def get_retrievers(self, tenant_id: str) -> Dict[str, any]:
        """
        ÙŠÙØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯.
        """
        if tenant_id not in self._retriever_cache:
            raise ValueError(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ø®Ø²Ù†Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
            
        # Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø§Ù„ØªÙŠ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ„ØªØ±Ø© (Ø³Ø±ÙŠØ¹Ø© ÙˆÙ„Ø§ ØªØ­ØªØ§Ø¬ Ù„ØªØ®Ø²ÙŠÙ†)
        faiss_retriever = self._vector_store.as_retriever(
            search_type="similarity", 
            search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}}
        )
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        bm25_retriever = self._retriever_cache[tenant_id]['bm25']
        parent_retriever = self._retriever_cache[tenant_id]['parent']
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ (Ø³Ø±ÙŠØ¹)
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever], 
            weights=[0.5, 0.5]
        )
        
        return {
            "hybrid": ensemble_retriever,
            "parent": parent_retriever
        }

# --- 5. Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (v3) - Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© ---
class RAGPipeline:
    def __init__(self):
        print("--- ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ RAG ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© (v3) ---")
        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©
        self.answer_llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
        self.classifier_llm = Ollama(model=CLASSIFIER_MODEL, base_url=OLLAMA_HOST)
        
        # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        self.vector_store = FAISS.load_local(UNIFIED_DB_PATH, self.embeddings, allow_dangerous_deserialization=True)
        self.reranker = Ranker()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª)
        all_docs = list(self.vector_store.docstore._dict.values())
        all_tenant_docs = self._group_docs_by_tenant(all_docs)
        self.retriever_manager = RetrieverManager(self.vector_store, all_tenant_docs)
        
        # Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.classifier_prompt = ChatPromptTemplate.from_template(QUESTION_CLASSIFIER_PROMPT)
        self.classifier_chain = self.classifier_prompt | self.classifier_llm | StrOutputParser()
        
        self.final_prompt = ChatPromptTemplate.from_template(DYNAMIC_PROMPT_TEMPLATE)
        self.answer_chain = self.final_prompt | self.answer_llm | StrOutputParser()
        print("--- âœ… Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ ---")

    def _group_docs_by_tenant(self, all_docs: List[Document]) -> Dict[str, List[Document]]:
        """ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø­Ø³Ø¨ Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„."""
        grouped = {}
        for doc in all_docs:
            tenant_id = doc.metadata.get("tenant_id")
            if tenant_id:
                if tenant_id not in grouped:
                    grouped[tenant_id] = []
                grouped[tenant_id].append(doc)
        return grouped

    def _get_verbosity(self, question: str) -> str:
        """ÙŠØ­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„."""
        # ... (Ù†ÙØ³ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ù† v2) ...
        question_lower = question.lower()
        if any(word in question_lower for word in ["Ø¨Ø§Ø®ØªØµØ§Ø±", "Ù…ÙˆØ¬Ø²", "Ù‡Ù„ ÙŠÙ…ÙƒÙ†"]):
            return "Ù…Ø®ØªØµØ±"
        return "Ù…ÙØµÙ„"

    async def get_answer(self, question: str, tenant_id: str) -> str:
        print(f"\n[>>] ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}': '{question}'")

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³Ø¨Ù‚ (Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©) ---
        print("[1/3] ğŸ›¡ï¸ ØªØµÙ†ÙŠÙ Ù†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…...")
        classification_result = await self.classifier_chain.ainvoke({"question": question})
        classification = classification_result.strip().lower()
        print(f"   -> Ø§Ù„ØªØµÙ†ÙŠÙ: {classification}")

        if classification == "general_chitchat":
            return "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ Ø£Ù†Ø¸Ù…Ø© Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ"
        if classification == "nonsensical":
            return "Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ØŸ"

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©) ---
        print("[2/3] ğŸ” ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„ (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©)...")
        try:
            retrievers = self.retriever_manager.get_retrievers(tenant_id)
            hybrid_retriever = retrievers['hybrid']
            parent_retriever = retrievers['parent']
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            hybrid_docs, parent_docs = await asyncio.gather(
                hybrid_retriever.ainvoke(question),
                asyncio.to_thread(parent_retriever.invoke, question)
            )
        except Exception as e:
            return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}"

        # ... (Ø¨Ù‚ÙŠØ© Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ) ...
        combined_initial_docs = hybrid_docs + parent_docs
        unique_docs_map = {doc.page_content: doc for doc in reversed(combined_initial_docs)}
        unique_docs = list(unique_docs_map.values())[::-1]
        if not unique_docs: return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø©."
        passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
        reranked_results = self.reranker.rerank(RerankRequest(query=question, passages=passages))
        top_results = reranked_results[:4]
        original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
        final_context_docs = [original_docs_map[res["id"]] for res in top_results]
        final_context = "\n\n---\n\n".join([doc.page_content for doc in final_context_docs])
        
        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ---
        verbosity = self._get_verbosity(question)
        print(f"[3/3] ğŸ§  ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„: {verbosity})...")
        
        final_answer = await self.answer_chain.ainvoke({
            "context": final_context,
            "question": question,
            "verbosity": verbosity
        })
        
        return final_answer

# --- 6. Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© ---
async def main():
    try:
        pipeline = RAGPipeline()
    except Exception as e:
        print(f"ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨: {e}")
        return

    # --- Ø£Ø³Ø¦Ù„Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ ---
    test_cases = [
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø§Ø¯ÙŠ
        {"tenant_id": "school_beta", "question": "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…Ø§ Ù‡ÙŠ Ø·Ø¨Ù‚Ø© Ø§Ù„Ù€ pooling ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŸ"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© (Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…)
        {"tenant_id": "un", "question": "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© (Ø³Ø¤Ø§Ù„ ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠ)
        {"tenant_id": "sys", "question": "Ø¨Ù„Ø¨Ù„Ø¨Ù„Ø¨Ù„Ø¨Ø¨"},
        # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø¤Ø§Ù„ Ù…Ø±ÙƒØ¨ (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„Ù… ØªØªØ£Ø«Ø±)
        {"tenant_id": "university_alpha", "question": "Ø¨Ø§Ø®ØªØµØ§Ø±ØŒ ÙƒÙŠÙ ÙŠØ³Ø§Ù‡Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¹Ø§Ø¦Ø¯ Ù…Ø§Ù„ÙŠ Ù„Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ†ØŸ"}
    ]

    for case in test_cases:
        start_time = time.time()
        answer = await pipeline.get_answer(question=case["question"], tenant_id=case["tenant_id"])
        duration = time.time() - start_time
        
        print("\n" + "="*30 + " ğŸ’¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸ’¬ " + "="*30)
        print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {case['question']}")
        print(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n{answer}")
        print(f"â±ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {duration:.2f} Ø«Ø§Ù†ÙŠØ©")
        print("="*86)

if __name__ == "__main__":
    asyncio.run(main())
