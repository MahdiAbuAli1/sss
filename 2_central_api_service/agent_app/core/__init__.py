# Ø§Ù„Ù…Ø³Ø§Ø±: 2_central_api_service/agent_app/core/__init__.py
# --- Ø§Ù„Ø¥ØµØ¯Ø§Ø± v6: Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù‡Ø±Ù…ÙŠ ---

import os
import logging
import asyncio
import json
import random
import re
import time
import uuid
from typing import AsyncGenerator, Dict, List, Optional

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.storage import InMemoryStore
from langchain.retrievers.parent_document_retriever import ParentDocumentRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from flashrank import Ranker, RerankRequest

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
__all__ = ["initialize_agent", "get_answer_stream", "agent_ready"]

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME")
CLASSIFIER_MODEL = os.getenv("CLASSIFIER_MODEL_NAME")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
LOGS_DIR = os.path.join(PROJECT_ROOT, "5_analysis_logs")
CANNED_RESPONSES_DIR = os.path.join(PROJECT_ROOT, "2_central_api_service/agent_app/static_responses")

TOP_K = 7
os.makedirs(LOGS_DIR, exist_ok=True)

# --- 2. Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯: Ù…Ø³Ø¬Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª (Ù…ÙØ­Ø³ÙÙ‘Ù†) ---
class RequestLogger:
    LOG_FILE_PATH = os.path.join(LOGS_DIR, "central_analysis.log")
    _lock = asyncio.Lock()

    def __init__(self, session_id: str):
        self.request_id = str(uuid.uuid4())[:8]
        self.session_id = session_id
        self.log_entries = []
        self.start_time = time.time()

    def log(self, message: str):
        self.log_entries.append(str(message))

    def log_docs(self, docs: List[Document], title: str):
        self.log(f"\n--- {title} (Ø¹Ø¯Ø¯: {len(docs)}) ---")
        if not docs:
            self.log("   -> Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
            return
        for i, doc in enumerate(docs):
            source = doc.metadata.get('source', 'N/A').split(os.sep)[-1]
            content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:90]
            self.log(f"   {i+1}. [Ø§Ù„Ù…ØµØ¯Ø±: {source}] -> \"{content_preview}...\"")

    def log_reranked_docs(self, reranked_results, original_docs_map, title: str):
        self.log(f"\n--- {title} (Ø¹Ø¯Ø¯: {len(reranked_results)}) ---")
        if not reranked_results:
            self.log("   -> Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
            return
        for i, res in enumerate(reranked_results):
            doc_id = res.get("id")
            if doc_id is not None and doc_id in original_docs_map:
                doc = original_docs_map[doc_id]
                source = doc.metadata.get('source', 'N/A').split(os.sep)[-1]
                content_preview = ' '.join(doc.page_content.replace('\n', ' ').split())[:90]
                self.log(f"   {i+1}. [Ø§Ù„Ø¯Ø±Ø¬Ø©: {res['score']:.4f}] [Ø§Ù„Ù…ØµØ¯Ø±: {source}] -> \"{content_preview}...\"")

    async def save(self):
        total_time = time.time() - self.start_time
        self.log(f"\n--- Ø§Ù„Ø£Ø¯Ø§Ø¡ ---")
        self.log(f"â±ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø²Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {total_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        full_report = "\n".join(self.log_entries)
        
        async with self._lock:
            try:
                with open(self.LOG_FILE_PATH, "a", encoding="utf-8") as f:
                    f.write(full_report + "\n\n")
                logging.info(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø³Ø¬Ù„ Ø§Ù„Ø·Ù„Ø¨ '{self.request_id}' Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ.")
            except IOError as e:
                logging.error(f"âŒ ÙØ´Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ: {e}")

# --- 3. Ø§Ù„Ø·Ø¨Ù‚Ø© 0: Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙÙˆØ±ÙŠØ© (Ø¨Ù…Ù†Ø·Ù‚ ØµØ§Ø±Ù…) ---
class InstantMemory:
    def __init__(self):
        self.responses = {}
        self.load_responses()

    def load_responses(self):
        logging.info("ğŸ§  [Ø§Ù„Ø·Ø¨Ù‚Ø© 0] ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙÙˆØ±ÙŠØ© (Ø¨Ù…Ù†Ø·Ù‚ ØµØ§Ø±Ù…)...")
        if not os.path.isdir(CANNED_RESPONSES_DIR):
            logging.warning(f"Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø© '{CANNED_RESPONSES_DIR}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
            return
        
        count = 0
        for filename in os.listdir(CANNED_RESPONSES_DIR):
            if filename.endswith(".json"):
                try:
                    with open(os.path.join(CANNED_RESPONSES_DIR, filename), "r", encoding="utf-8") as f:
                        data = json.load(f)
                        for item in data:
                            question = item.get("question", "").strip().lower()
                            answers = item.get("answers")
                            if question and answers:
                                self.responses[question] = answers
                                count += 1
                except Exception as e:
                    logging.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù '{filename}': {e}")
        logging.info(f"âœ… [Ø§Ù„Ø·Ø¨Ù‚Ø© 0] ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} Ù‚Ø§Ø¹Ø¯Ø© Ø±Ø¯ ÙÙˆØ±ÙŠ.")

    def get_response(self, question: str) -> Optional[str]:
        # ØªØ·Ø§Ø¨Ù‚ Ø­Ø±ÙÙŠ ÙˆØµØ§Ø±Ù… 100%
        exact_match = self.responses.get(question.strip().lower())
        if exact_match:
            return random.choice(exact_match)
        return None

# --- 4. Ø§Ù„Ø·Ø¨Ù‚Ø© 1: Ø§Ù„Ø­Ø§Ø±Ø³ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ø¨Ù…Ù†Ø·Ù‚ Ù…ÙØ­Ø³ÙÙ‘Ù†) ---
class FastGatekeeper:
    def is_nonsensical(self, question: str) -> bool:
        q = question.strip()
        # Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§
        if len(q) < 3: return True
        # Ø­Ø±ÙˆÙ Ù…ÙƒØ±Ø±Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ù„Øº ÙÙŠÙ‡
        if re.search(r'(.)\1{3,}', q): return True
        # Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ Ø­Ø±ÙˆÙ (ÙÙ‚Ø· Ø£Ø±Ù‚Ø§Ù… ÙˆØ±Ù…ÙˆØ²)
        if not re.search(r'[a-zA-Z\u0600-\u06FF]', q): return True
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø¥Ù„Ù‰ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·ÙˆÙ„ Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ù‹Ø§ (Ø¶ÙˆØ¶Ø§Ø¡)
        letters = re.findall(r'[a-zA-Z\u0600-\u06FF]', q)
        if len(letters) / len(q) < 0.5: return True
        return False

# --- 5. Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ (Ù…Ø­Ø³Ù‘Ù†Ø©) ---
QUESTION_CLASSIFIER_PROMPT = ChatPromptTemplate.from_template(
"""Your task is to classify the user's question into one of two categories: "specific_query" or "general_chitchat".
- "specific_query": The user is asking a specific question that can likely be answered from a knowledge base (e.g., "how do I reset my password?", "what is max pooling?").
- "general_chitchat": The user is asking a general knowledge question or making a greeting that is not a simple hello/thanks (e.g., "how are you?", "who is the president?", "what is the weather?").

User Question: "{question}"
Category:
"""
)

DYNAMIC_PROMPT_TEMPLATE = ChatPromptTemplate.from_template(
"""Ø£Ù†Øª "Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒÙŠ". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ùƒ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.

**Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:**
1.  **Ø§Ù„ØªØ­ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§:** Ø§Ø¨Ø¯Ø£ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø¹Ø¨Ø§Ø±Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©.
2.  **Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù…Ø·Ù„Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:** Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù‚Ù„ **ÙÙ‚Ø·**: "Ù„Ù‚Ø¯ Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
3.  **Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ({verbosity}):**
    - **"Ù…Ø®ØªØµØ±"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬Ø²Ø© ÙÙŠ Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†.
    - **"Ù…ÙØµÙ„"**: Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù….
4.  **Ø§Ù„Ø§Ø®ØªØµØ§Ø±:** Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø¨Ø¯Ù‹Ø§ ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚" Ø£Ùˆ "ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª".
5.  **Ø§Ù„Ø®Ø§ØªÙ…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø®ØªØªÙ… Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø³Ø¤Ø§Ù„ ØªÙØ§Ø¹Ù„ÙŠØŒ Ù…Ø«Ù„: "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø± ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ù‡ØŸ".

---
**Ø§Ù„Ø³ÙŠØ§Ù‚:**
{context}
---
**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {question}
---
**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** {verbosity}
---
**Ø¥Ø¬Ø§Ø¨ØªÙƒ:**
"""
)

# --- 6. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ---
llm_answer: Ollama = None
llm_classifier: Ollama = None
vector_store: FAISS = None
reranker: Ranker = None
all_docs: List[Document] = None
instant_memory: InstantMemory = None
fast_gatekeeper: FastGatekeeper = None
initialization_lock = asyncio.Lock()

# --- 7. Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
async def initialize_agent():
    global llm_answer, llm_classifier, vector_store, reranker, all_docs, instant_memory, fast_gatekeeper
    async with initialization_lock:
        if agent_ready(): return
        logging.info("--- ğŸš€ Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù‡Ø±Ù…ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© (v6) ---")
        try:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            if not all([EMBEDDING_MODEL, CHAT_MODEL, CLASSIFIER_MODEL, OLLAMA_HOST]):
                raise ValueError("Ø£Ø­Ø¯ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ù„Ù .env")

            llm_answer = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            llm_classifier = Ollama(model=CLASSIFIER_MODEL, base_url=OLLAMA_HOST)
            embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            vector_store = await asyncio.to_thread(FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            reranker = Ranker()
            all_docs = list(vector_store.docstore._dict.values())
            
            instant_memory = InstantMemory()
            fast_gatekeeper = FastGatekeeper()

            logging.info("--- âœ… Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ ---")
        except Exception as e:
            logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            raise

def agent_ready() -> bool:
    return vector_store is not None and instant_memory is not None

def _get_verbosity(question: str) -> str:
    question_lower = question.lower()
    if any(word in question_lower for word in ["Ø¨Ø§Ø®ØªØµØ§Ø±", "Ù…ÙˆØ¬Ø²"]):
        return "Ù…Ø®ØªØµØ±"
    return "Ù…ÙØµÙ„"

# --- 8. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù‡Ø±Ù…ÙŠ) ---
async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    question = request_info.get("question", "")
    tenant_id = request_info.get("tenant_id", "default_session")
    session_id = request_info.get("session_id", "default_session")
    logger = RequestLogger(session_id)
    
    logger.log("="*80)
    logger.log(f"Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯ | ID: {logger.request_id}")
    logger.log(f"Ø§Ù„Ø¹Ù…ÙŠÙ„: {tenant_id}")
    logger.log(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
    logger.log("="*80)

    if not agent_ready():
        yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø²."}
        await logger.save()
        return

    try:
        # --- Ø§Ù„Ø·Ø¨Ù‚Ø© 0: Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙÙˆØ±ÙŠØ© (ØªØ·Ø§Ø¨Ù‚ Ø­Ø±ÙÙŠ) ---
        logger.log("\n[Ø§Ù„Ø·Ø¨Ù‚Ø© 0: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙÙˆØ±ÙŠØ©]")
        canned_response = instant_memory.get_response(question)
        if canned_response:
            logger.log(f"-> âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø¯ ÙÙˆØ±ÙŠ: '{canned_response}'")
            yield {"type": "full_answer", "content": canned_response}
            return

        # --- Ø§Ù„Ø·Ø¨Ù‚Ø© 1: Ø§Ù„Ø­Ø§Ø±Ø³ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ø£Ø³Ø¦Ù„Ø© ØªØ§ÙÙ‡Ø©) ---
        logger.log("\n[Ø§Ù„Ø·Ø¨Ù‚Ø© 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹]")
        if fast_gatekeeper.is_nonsensical(question):
            answer = "Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ØŸ"
            logger.log(f"->  GATEKEEPER: ØªÙ… ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ ØªØ§ÙÙ‡.")
            logger.log(f"\n--- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ù…Ù† Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©) ---\n{answer}")
            yield {"type": "full_answer", "content": answer}
            return

        # --- Ø§Ù„Ø·Ø¨Ù‚Ø© 2: Ø§Ù„Ù…ØµÙ†Ù Ø§Ù„Ø°ÙƒÙŠ (LLM) ---
        logger.log("\n[Ø§Ù„Ø·Ø¨Ù‚Ø© 2: ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ]")
        classifier_chain = QUESTION_CLASSIFIER_PROMPT | llm_classifier | StrOutputParser()
        classification_result = await classifier_chain.ainvoke({"question": question})
        classification = re.sub(r'[^a-z_]', '', classification_result.strip().lower())
        logger.log(f"-> Ø§Ù„ØªØµÙ†ÙŠÙ: {classification}")

        if "general_chitchat" in classification:
            answer = "Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙˆÙ„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©. Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…ØŸ"
            logger.log(f"\n--- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ù…Ù† Ø§Ù„Ù…ØµÙ†Ù) ---\n{answer}")
            yield {"type": "full_answer", "content": answer}
            return

        # --- Ø§Ù„Ø·Ø¨Ù‚Ø© 3: Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø®Ø§Ø±Ù‚ (RAG) ---
        logger.log("\n[Ø§Ù„Ø·Ø¨Ù‚Ø© 3: Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯ (Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø®Ø§Ø±Ù‚)]")
        tenant_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
        if not tenant_docs:
            answer = f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'."
            logger.log(f"\n--- Ø®Ø·Ø£ ---\n{answer}")
            yield {"type": "full_answer", "content": answer}
            return

        bm25_retriever = BM25Retriever.from_documents(tenant_docs, k=TOP_K)
        store = InMemoryStore()
        parent_retriever = ParentDocumentRetriever(vectorstore=vector_store, docstore=store, child_splitter=RecursiveCharacterTextSplitter(chunk_size=400))
        parent_retriever.add_documents(tenant_docs, ids=None)
        faiss_retriever = vector_store.as_retriever(search_kwargs={'k': TOP_K, 'filter': {'tenant_id': tenant_id}})
        hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])

        hybrid_docs, parent_docs = await asyncio.gather(
            hybrid_retriever.ainvoke(question),
            asyncio.to_thread(parent_retriever.invoke, question)
        )
        logger.log_docs(hybrid_docs, "Ø§Ù„Ù…Ø±Ø´Ø­ÙˆÙ† Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†")
        logger.log_docs(parent_docs, "Ø§Ù„Ù…Ø±Ø´Ø­ÙˆÙ† Ù…Ù† Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©")

        combined_docs = hybrid_docs + parent_docs
        unique_docs_map = {doc.page_content: doc for doc in reversed(combined_docs)}
        unique_docs = list(unique_docs_map.values())[::-1]
        logger.log(f"-> Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø§Ù„ÙØ±ÙŠØ¯ÙŠÙ†: {len(unique_docs)}")

        if not unique_docs:
            answer = "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."
            yield {"type": "full_answer", "content": answer}
            return

        passages = [{"id": i, "text": doc.page_content} for i, doc in enumerate(unique_docs)]
        reranked_results = reranker.rerank(RerankRequest(query=question, passages=passages))
        
        top_results = reranked_results[:4]
        original_docs_map = {i: doc for i, doc in enumerate(unique_docs)}
        final_context_docs = [original_docs_map[res["id"]] for res in top_results if res.get("id") in original_docs_map]
        logger.log_reranked_docs(top_results, original_docs_map, "Ø£ÙØ¶Ù„ 4 Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨")
        
        final_context = "\n\n---\n\n".join([doc.page_content for doc in final_context_docs])
        logger.log("\n--- Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø±Ø³Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ---\n" + final_context)

        verbosity = _get_verbosity(question)
        logger.log(f"\n[ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©] Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {verbosity}")
        
        answer_chain = DYNAMIC_PROMPT_TEMPLATE | llm_answer | StrOutputParser()
        
        full_answer = ""
        async for chunk in answer_chain.astream({"context": final_context, "question": question, "verbosity": verbosity}):
            if chunk:
                full_answer += chunk
                yield {"type": "chunk", "content": chunk}
        
        logger.log("\n--- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---\n" + full_answer)

    except Exception as e:
        error_message = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ."
        logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
        logger.log(f"\n--- Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ---\n{e}")
        yield {"type": "error", "content": error_message}
    finally:
        await logger.save()
