# project_core/processing/2_run_enrichment.py

import os
import logging
import json
from datetime import datetime
from tqdm import tqdm
import time

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ---
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ---
from project_core.core.config import (
    get_text_enrichment_llm,
    get_multimodal_llm,
    ENABLE_PROCESSING_ENRICHMENT,
    BASE_DIR
)

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ---
LOGS_DIR = os.path.join(BASE_DIR, "logs")
if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)
log_filename = datetime.now().strftime(f"enrichment_run_%Y-%m-%d_%H-%M-%S.log")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(os.path.join(LOGS_DIR, log_filename), encoding='utf-8'), logging.StreamHandler()])
logger = logging.getLogger("enrichment_pipeline")

# --- ØªØ¹Ø±ÙŠÙ Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬ ---
INTERMEDIATE_DIR = os.path.join(BASE_DIR, "intermediate_outputs")
ENRICHED_DIR = os.path.join(BASE_DIR, "enriched_outputs")
if not os.path.exists(ENRICHED_DIR): os.makedirs(ENRICHED_DIR)

# --- ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ ÙˆØ³Ù„Ø§Ø³Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
try:
    enrichment_llm = get_text_enrichment_llm().with_config({"request_timeout": 600}) # 10 Ø¯Ù‚Ø§Ø¦Ù‚
    multimodal_llm = get_multimodal_llm().with_config({"request_timeout": 600}) # 10 Ø¯Ù‚Ø§Ø¦Ù‚

    enrichment_prompt = ChatPromptTemplate.from_template(
        """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙŠÙƒÙˆÙ† Ø³Ù‡Ù„ Ø§Ù„ÙÙ‡Ù… ÙˆÙ…Ø«Ø§Ù„ÙŠØ§Ù‹ Ù„Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø«. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙˆØ§Ø¶Ø­Ø© ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø·ÙŠØ©. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØµÙ Ø®Ø·ÙˆØ§ØªØŒ Ø±Ù‚Ù…Ù‡Ø§. Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ¹Ø±ÙŠÙØ§ØªØŒ Ø£Ø¨Ø±Ø²Ù‡Ø§.

Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:
---
{text}
---

Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù…ÙØ«Ø±Ø§Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:"""
    )
    enrichment_chain = enrichment_prompt | enrichment_llm | StrOutputParser()

    image_summarize_prompt = ChatPromptTemplate.from_messages(
        [("user", [
            {"type": "text", "text": """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ø¬Ù‡Ø§Øª ÙˆØªØµØ§Ù…ÙŠÙ…. ØµÙ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© ÙƒØ£Ù†Ùƒ ØªØ´Ø±Ø­Ù‡Ø§ Ù„Ø´Ø®Øµ ÙƒÙÙŠÙ. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ Ø¬Ø¯Ù‹Ø§.
1.  **Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ÙˆØµÙ Ø§Ù„Ø¹Ø§Ù…:** Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© (ÙˆØ§Ø¬Ù‡Ø© ØªØ·Ø¨ÙŠÙ‚ØŒ Ù…Ø®Ø·Ø·ØŒ Ø´Ø¹Ø§Ø±)ØŸ
2.  **Ø­Ù„Ù„ Ø§Ù„Ù‡ÙŠÙƒÙ„:** ØµÙ Ø§Ù„ØªØ®Ø·ÙŠØ· (Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ³Ø·ØŒ Ø£Ø³ÙÙ„).
3.  **ØµÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø°ÙƒØ± ÙƒÙ„ Ø²Ø±ØŒ Ø£ÙŠÙ‚ÙˆÙ†Ø©ØŒ Ù‚Ø§Ø¦Ù…Ø©ØŒ Ø£Ùˆ Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„. ØµÙ Ø´ÙƒÙ„Ù‡ØŒ Ù„ÙˆÙ†Ù‡ØŒ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ø¹Ù„ÙŠÙ‡ØŒ ÙˆÙ…ÙˆÙ‚Ø¹Ù‡ Ø§Ù„Ø¯Ù‚ÙŠÙ‚.
4.  **Ø§Ù‚Ø±Ø£ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ:** Ø§ÙƒØªØ¨ ÙƒÙ„ Ù†Øµ ØªØ±Ø§Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ…Ø§ Ù‡Ùˆ.
5.  **ØµÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©:** Ø§Ø°ÙƒØ± Ø£ÙŠ ØµÙˆØ±ØŒ Ø´Ø¹Ø§Ø±Ø§ØªØŒ Ø£Ùˆ Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ø£Ø®Ø±Ù‰."""},
            {"type": "image_url", "image_url": "data:image/jpeg;base64,{image_base64}"},
        ])]
    )
    image_summarize_chain = image_summarize_prompt | multimodal_llm | StrOutputParser()

except Exception as e:
    logger.error(f"ÙØ´Ù„ ÙƒØ§Ø±Ø«ÙŠ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©: {e}")
    enrichment_chain, image_summarize_chain = None, None


def run_enrichment():
    logger.info("="*50 + "\nğŸš€ Ø¨Ø¯Ø¡ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ (Ù†Ø³Ø®Ø© Ù‚ÙˆÙŠØ© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø­ÙØ¸)...\n" + "="*50)

    if not ENABLE_PROCESSING_ENRICHMENT:
        logger.warning("ØªÙ… ØªØ¹Ø·ÙŠÙ„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©.")
        return

    if not all([enrichment_chain, image_summarize_chain]):
        logger.error("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
        return

    files_to_process = [f for f in os.listdir(INTERMEDIATE_DIR) if f.endswith(".json")]
    
    # --- Ø­Ù„Ù‚Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„Ù ---
    for filename in tqdm(files_to_process, desc="Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"):
        input_path = os.path.join(INTERMEDIATE_DIR, filename)
        # --- Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª (Ù†Ù‚Ø·Ø© Ø§Ù„Ø­ÙØ¸) ---
        final_output_path = os.path.join(ENRICHED_DIR, filename)
        temp_output_path = os.path.join(ENRICHED_DIR, f"temp_{filename}")

        # --- ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ ---
        if os.path.exists(final_output_path):
            logger.info(f"âœ… Ø§Ù„Ù…Ù„Ù {filename} ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø¨Ø§Ù„ÙØ¹Ù„. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠÙ‡.")
            continue

        logger.info(f"\n--- Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {filename} ---")

        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ø¥Ù† ÙˆØ¬Ø¯ ---
        enriched_chunks = []
        if os.path.exists(temp_output_path):
            logger.info("ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ù…Ø¤Ù‚Øª. Ø³ÙŠØªÙ… Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¹Ù…Ù„...")
            with open(temp_output_path, 'r', encoding='utf-8') as f:
                enriched_chunks = json.load(f)
        
        num_already_processed = len(enriched_chunks)
        logger.info(f"ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {num_already_processed} Ù‚Ø·Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ø³Ø§Ø¨Ù‚Ù‹Ø§.")

        # --- ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© ---
        all_items_to_process = []
        source_file = data.get("source_file")
        tenant_id = data.get("tenant_id")
        
        for item in data.get("extracted_texts", []):
            all_items_to_process.append({"type": "text", "content": item})
        for item in data.get("extracted_tables_html", []):
            all_items_to_process.append({"type": "table", "content": item})
        for item in data.get("extracted_images_base64", []):
            all_items_to_process.append({"type": "image", "content": item})
            
        # --- ØªØ®Ø·ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙŠ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ ---
        items_to_process_now = all_items_to_process[num_already_processed:]

        if not items_to_process_now:
            logger.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù.")
        else:
            # --- Ø­Ù„Ù‚Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© ÙˆØ­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù… ---
            with tqdm(total=len(items_to_process_now), desc=f"Ø¥Ø«Ø±Ø§Ø¡ {filename[:15]}") as pbar:
                for i, item_data in enumerate(items_to_process_now):
                    item_type = item_data["type"]
                    original_content = item_data["content"]
                    enriched_content = ""
                    
                    try:
                        if item_type in ["text", "table"]:
                            enriched_content = enrichment_chain.invoke({"text": original_content})
                        elif item_type == "image":
                            enriched_content = image_summarize_chain.invoke({"image_base64": original_content})
                        
                        chunk = {
                            "type": item_type,
                            "original_content": original_content,
                            "enriched_content": enriched_content,
                            "metadata": {"source_file": source_file, "tenant_id": tenant_id}
                        }
                        enriched_chunks.append(chunk)

                        # --- Ù†Ù‚Ø·Ø© Ø§Ù„Ø­ÙØ¸: Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¹Ù†ØµØ± ---
                        with open(temp_output_path, 'w', encoding='utf-8') as f:
                            json.dump(enriched_chunks, f, ensure_ascii=False, indent=4)

                    except Exception as e:
                        logger.error(f"âŒ ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‚Ø·Ø¹Ø© Ø±Ù‚Ù… {num_already_processed + i + 1}: {e}")
                        # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„ØŒ Ù†Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„ØªØ¬Ù†Ø¨ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        chunk = {
                            "type": item_type,
                            "original_content": original_content,
                            "enriched_content": f"ÙØ´Ù„ Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡: {original_content}",
                            "metadata": {"source_file": source_file, "tenant_id": tenant_id}
                        }
                        enriched_chunks.append(chunk)
                        with open(temp_output_path, 'w', encoding='utf-8') as f:
                            json.dump(enriched_chunks, f, ensure_ascii=False, indent=4)
                    
                    pbar.update(1)

        # --- Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ---
        # 1. Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ù„Ù‰ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        os.rename(temp_output_path, final_output_path)
        logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ù„Ù {filename}. ØªÙ… Ø­ÙØ¸ {len(enriched_chunks)} Ù‚Ø·Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù†Ø¸Ù…Ø©.")

if __name__ == "__main__":
    run_enrichment()
