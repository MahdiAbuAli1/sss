# # project_core/processing/3_run_ingestion.py

# import os
# import logging
# import json
# from datetime import datetime
# from tqdm import tqdm

# # --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ---
# from langchain_community.vectorstores.chroma import Chroma
# from langchain.storage import LocalFileStore
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.schema.document import Document

# # --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© ---
# from project_core.core.config import (
#     get_embeddings_model,
#     # ØªÙ… Ø­Ø°Ù ENRICHED_DIR Ù…Ù† Ù‡Ù†Ø§
#     VECTORSTORE_PATH,
#     DOCSTORE_PATH,
#     COLLECTION_NAME,
#     BASE_DIR  # Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
# )
# from project_core.processing.utils import load_processed_files_log, save_processed_files_log

# # --- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ---
# LOGS_DIR = os.path.join(BASE_DIR, "logs")
# if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)
# log_filename = datetime.now().strftime(f"ingestion_run_%Y-%m-%d_%H-%M-%S.log")
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(os.path.join(LOGS_DIR, log_filename), encoding='utf-8'), logging.StreamHandler()])
# logger = logging.getLogger("ingestion_pipeline")

# # --- ØªØ¹Ø±ÙŠÙ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§) ---
# ENRICHED_DIR = os.path.join(BASE_DIR, "enriched_outputs")


# def run_ingestion():
#     """
#     Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
#     """
#     logger.info("="*50 + "\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ...\n" + "="*50)

#     try:
#         # --- ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† ---
#         embeddings_model = get_embeddings_model()

#         # --- ØªÙ‡ÙŠØ¦Ø© Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ) ---
#         if not os.path.exists(DOCSTORE_PATH): os.makedirs(DOCSTORE_PATH)
#         fs = LocalFileStore(DOCSTORE_PATH)
        
#         # --- ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© (Vector Store) ---
#         vectorstore = Chroma(
#             collection_name=COLLECTION_NAME,
#             embedding_function=embeddings_model,
#             persist_directory=VECTORSTORE_PATH,
#         )
#         logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

#         # --- ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙƒÙŠ ---
#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=1000,
#             chunk_overlap=200,
#             length_function=len,
#             is_separator_regex=False,
#         )
#         logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙƒÙŠ.")

#         # --- ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ---
#         processed_log = load_processed_files_log()
        
#         # --- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®Ø²ÙŠÙ† ---
#         all_files = [f for f in os.listdir(ENRICHED_DIR) if f.endswith(".json")]
#         files_to_process = [f for f in all_files if f not in processed_log]

#         if not files_to_process:
#             logger.warning("ğŸ‰ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ†. ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ù‘Ø«!")
#             return

#         logger.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(files_to_process)} Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®Ø²ÙŠÙ†.")

#         all_docs_for_embedding = []
#         all_original_contents = []
#         all_doc_ids = []

#         # --- Ù‚Ø±Ø§Ø¡Ø© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ---
#         for filename in tqdm(files_to_process, desc="Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø«Ø±ÙŠØ©"):
#             file_path = os.path.join(ENRICHED_DIR, filename)
#             with open(file_path, 'r', encoding='utf-8') as f:
#                 chunks = json.load(f)
            
#             for chunk in chunks:
#                 # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ø£ØµÙ„ÙŠØ©
#                 doc_id = f"{chunk['metadata']['tenant_id']}-{chunk['metadata']['source_file']}-{len(all_doc_ids)}"
                
#                 # --- Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ ---
#                 original_content = chunk["original_content"]
#                 # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§ÙŠØªØ§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ®Ø²ÙŠÙ†
#                 if isinstance(original_content, str):
#                     all_original_contents.append(original_content.encode('utf-8'))
#                 else: # ÙŠÙØªØ±Ø¶ Ø£Ù†Ù‡ Ø¨Ø§ÙŠØªØ§Øª Ø¨Ø§Ù„ÙØ¹Ù„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù†ØµÙ‹Ø§
#                     all_original_contents.append(original_content)
                
#                 all_doc_ids.append(doc_id)

#                 # --- Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙØ«Ø±Ù‰ ---
#                 enriched_content = chunk["enriched_content"]
                
#                 temp_doc = Document(
#                     page_content=enriched_content,
#                     metadata={
#                         "doc_id": doc_id,
#                         "source_file": chunk["metadata"]["source_file"],
#                         "tenant_id": chunk["metadata"]["tenant_id"],
#                         "type": chunk["type"]
#                     }
#                 )
                
#                 split_docs = text_splitter.split_documents([temp_doc])
#                 all_docs_for_embedding.extend(split_docs)

#         if not all_docs_for_embedding:
#             logger.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ØªØ®Ø²ÙŠÙ†.")
#             return

#         logger.info(f"ğŸ’¾ Ø¨Ø¯Ø¡ ØªØ®Ø²ÙŠÙ† {len(all_original_contents)} Ù‚Ø·Ø¹Ø© Ù…Ø­ØªÙˆÙ‰ Ø£ØµÙ„ÙŠ Ùˆ {len(all_docs_for_embedding)} Ù‚Ø·Ø¹Ø© Ù…ØªØ¬Ù‡Ø©...")

#         # --- ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ ---
#         try:
#             fs.mset(list(zip(all_doc_ids, all_original_contents)))
#             logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
#         except Exception as store_err:
#             logger.error(f"ÙØ´Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ: {store_err}")
#             # ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªÙ‚Ø±Ø± Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù‡Ù†Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ø°Ø§ Ø®Ø·Ø£Ù‹ ÙØ§Ø¯Ø­Ù‹Ø§
#             return

#         # --- ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª (Ù…Ø¹ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù…) ---
#         # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ù…Ù‚Ø·Ø¹Ø©
#         split_ids = [f"{doc.metadata['doc_id']}-{i}" for i, doc in enumerate(all_docs_for_embedding)]
        
#         vectorstore.add_documents(
#             documents=tqdm(all_docs_for_embedding, desc="ØªØ¶Ù…ÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª"),
#             ids=split_ids
#         )
#         logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

#         # --- ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
#         processed_log.extend(files_to_process)
#         save_processed_files_log(processed_log)
        
#         logger.info("\nğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­!")

#     except Exception as e:
#         logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")

# if __name__ == "__main__":
#     run_ingestion()
# project_core/processing/3_run_ingestion.py

# import os
# import logging
# import json
# from datetime import datetime
# from tqdm import tqdm
# import time # Ø³Ù†Ø­ØªØ§Ø¬Ù‡ Ù„Ù„ØªØ£Ø®ÙŠØ±

# # --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ---
# from langchain_community.vectorstores.chroma import Chroma
# from langchain.storage import LocalFileStore
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.schema.document import Document

# # --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© ---
# from project_core.core.config import (
#     get_embeddings_model,
#     VECTORSTORE_PATH,
#     DOCSTORE_PATH,
#     COLLECTION_NAME,
#     BASE_DIR
# )
# from project_core.processing.utils import load_processed_files_log, save_processed_files_log

# # --- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ---
# LOGS_DIR = os.path.join(BASE_DIR, "logs")
# if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)
# log_filename = datetime.now().strftime(f"ingestion_run_%Y-%m-%d_%H-%M-%S.log")
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(os.path.join(LOGS_DIR, log_filename), encoding='utf-8'), logging.StreamHandler()])
# logger = logging.getLogger("ingestion_pipeline")

# # --- ØªØ¹Ø±ÙŠÙ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ---
# ENRICHED_DIR = os.path.join(BASE_DIR, "enriched_outputs")

# def run_ingestion():
#     """
#     Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
#     """
#     logger.info("="*50 + "\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (Ù†Ø³Ø®Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù…ØªØ­ÙƒÙ… Ø¨Ù‡Ø§)...\n" + "="*50)

#     try:
#         embeddings_model = get_embeddings_model()
#         if not os.path.exists(DOCSTORE_PATH): os.makedirs(DOCSTORE_PATH)
#         fs = LocalFileStore(DOCSTORE_PATH)
        
#         vectorstore = Chroma(
#             collection_name=COLLECTION_NAME,
#             embedding_function=embeddings_model,
#             persist_directory=VECTORSTORE_PATH,
#         )
#         logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=1000,
#             chunk_overlap=200,
#             length_function=len,
#             is_separator_regex=False,
#         )
#         logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙƒÙŠ.")

#         processed_log = load_processed_files_log()
#         all_files = [f for f in os.listdir(ENRICHED_DIR) if f.endswith(".json")]
#         files_to_process = [f for f in all_files if f not in processed_log]

#         if not files_to_process:
#             logger.warning("ğŸ‰ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ†. ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ù‘Ø«!")
#             return

#         logger.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(files_to_process)} Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®Ø²ÙŠÙ†.")

#         all_docs_for_embedding = []
#         all_original_contents = []
#         all_doc_ids = []

#         for filename in tqdm(files_to_process, desc="Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø«Ø±ÙŠØ©"):
#             file_path = os.path.join(ENRICHED_DIR, filename)
#             with open(file_path, 'r', encoding='utf-8') as f:
#                 chunks = json.load(f)
            
#             for chunk in chunks:
#                 doc_id = f"{chunk['metadata']['tenant_id']}-{chunk['metadata']['source_file']}-{len(all_doc_ids)}"
                
#                 original_content = chunk["original_content"]
#                 encoded_content = original_content.encode('utf-8') if isinstance(original_content, str) else original_content
#                 all_original_contents.append(encoded_content)
#                 all_doc_ids.append(doc_id)

#                 enriched_content = chunk["enriched_content"]
#                 temp_doc = Document(
#                     page_content=enriched_content,
#                     metadata={
#                         "doc_id": doc_id,
#                         "source_file": chunk["metadata"]["source_file"],
#                         "tenant_id": chunk["metadata"]["tenant_id"],
#                         "type": chunk["type"]
#                     }
#                 )
                
#                 split_docs = text_splitter.split_documents([temp_doc])
#                 all_docs_for_embedding.extend(split_docs)

#         if not all_docs_for_embedding:
#             logger.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ØªØ®Ø²ÙŠÙ†.")
#             return

#         logger.info(f"ğŸ’¾ Ø¨Ø¯Ø¡ ØªØ®Ø²ÙŠÙ† {len(all_original_contents)} Ù‚Ø·Ø¹Ø© Ù…Ø­ØªÙˆÙ‰ Ø£ØµÙ„ÙŠ Ùˆ {len(all_docs_for_embedding)} Ù‚Ø·Ø¹Ø© Ù…ØªØ¬Ù‡Ø©...")

#         # --- ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ ---
#         fs.mset(list(zip(all_doc_ids, all_original_contents)))
#         logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.")

#         # --- **Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¬Ø¯ÙŠØ¯: Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© ÙˆØ§Ù„Ù…ØªØ­ÙƒÙ… Ø¨Ù‡Ø§** ---
#         batch_size = 32  # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù…ØŒ 32 Ù‡Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø¬ÙŠØ¯Ø©
#         total_batches = (len(all_docs_for_embedding) + batch_size - 1) // batch_size
        
#         logger.info(f"Ø³ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… {len(all_docs_for_embedding)} Ù‚Ø·Ø¹Ø© Ø¥Ù„Ù‰ {total_batches} Ø¯ÙØ¹Ø© (Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {batch_size}).")

#         with tqdm(total=len(all_docs_for_embedding), desc="ØªØ¶Ù…ÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª") as pbar:
#             for i in range(0, len(all_docs_for_embedding), batch_size):
#                 batch_docs = all_docs_for_embedding[i:i + batch_size]
                
#                 # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙØ¹Ø© ÙÙ‚Ø·
#                 batch_ids = [f"{doc.metadata['doc_id']}-{i+j}" for j, doc in enumerate(batch_docs)]
                
#                 try:
#                     vectorstore.add_documents(
#                         documents=batch_docs,
#                         ids=batch_ids
#                     )
#                     pbar.update(len(batch_docs))
#                     time.sleep(0.1) # <-- Ø¥Ø¶Ø§ÙØ© ÙØªØ±Ø© Ø±Ø§Ø­Ø© Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ù‹Ø§
#                 except Exception as batch_err:
#                     logger.error(f"ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© ØªØ¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ø¹Ù†ØµØ± {i}. Ø§Ù„Ø®Ø·Ø£: {batch_err}")
#                     logger.warning("Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙØ¹Ø© ÙˆØ§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
#                     pbar.update(len(batch_docs)) # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙˆÙ‚Ù
#                     continue # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©

#         logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

#         # --- ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
#         processed_log.extend(files_to_process)
#         save_processed_files_log(processed_log)
        
#         logger.info("\nğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­!")

#     except Exception as e:
#         logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")

# if __name__ == "__main__":
#     run_ingestion()
    
# project_core/processing/3_run_ingestion.py

import os
import logging
import json
from datetime import datetime
from tqdm import tqdm
import time

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ---
from langchain_community.vectorstores.chroma import Chroma
from langchain.storage import LocalFileStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.document import Document

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© ---
from project_core.core.config import (
    get_embeddings_model,
    VECTORSTORE_PATH,
    DOCSTORE_PATH,
    COLLECTION_NAME,
    BASE_DIR,
    PROCESSED_LOG_FILE,
    ENRICHED_DIR
)
from project_core.processing.utils import load_processed_files_log, save_processed_files_log

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ---
LOGS_DIR = os.path.join(BASE_DIR, "logs")
if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)
log_filename = datetime.now().strftime(f"ingestion_run_%Y-%m-%d_%H-%M-%S.log")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(os.path.join(LOGS_DIR, log_filename), encoding='utf-8'), logging.StreamHandler()])
logger = logging.getLogger("ingestion_pipeline")

def run_ingestion():
    logger.info("="*50 + "\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø±Ø´Ø­Ø§Øª)...\n" + "="*50)

    try:
        embeddings_model = get_embeddings_model()
        if not os.path.exists(DOCSTORE_PATH): os.makedirs(DOCSTORE_PATH)
        fs = LocalFileStore(DOCSTORE_PATH)
        
        vectorstore = Chroma(
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings_model,
            persist_directory=VECTORSTORE_PATH,
        )
        logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            is_separator_regex=False,
        )
        logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø°ÙƒÙŠ.")

        # --- **ØªØ¹Ø¯ÙŠÙ„ Ù…Ù‡Ù…: Ø­Ø°Ù Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ø¨Ø¯Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯** ---
        if os.path.exists(PROCESSED_LOG_FILE):
            os.remove(PROCESSED_LOG_FILE)
            logger.warning("ØªÙ… Ø­Ø°Ù Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ… Ù„Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯.")

        processed_log = load_processed_files_log()
        all_files = [f for f in os.listdir(ENRICHED_DIR) if f.endswith(".json")]
        files_to_process = [f for f in all_files if f not in processed_log]

        if not files_to_process:
            logger.warning("ğŸ‰ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ†. ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ù‘Ø«!")
            return

        logger.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(files_to_process)} Ù…Ù„ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ®Ø²ÙŠÙ†.")

        all_docs_for_embedding = []
        all_original_contents = []
        all_doc_ids = []

        for filename in tqdm(files_to_process, desc="Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø«Ø±ÙŠØ©"):
            file_path = os.path.join(ENRICHED_DIR, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            for i, chunk in enumerate(chunks):
                # Ø¥Ù†Ø´Ø§Ø¡ doc_id ÙØ±ÙŠØ¯ ÙˆÙ…Ø³ØªÙ‚Ø±
                doc_id = f"{chunk['metadata']['tenant_id']}-{os.path.splitext(chunk['metadata']['source_file'])[0]}-{i}"
                
                original_content = chunk["original_content"]
                encoded_content = original_content.encode('utf-8') if isinstance(original_content, str) else original_content
                all_original_contents.append(encoded_content)
                all_doc_ids.append(doc_id)

                enriched_content = chunk["enriched_content"]
                
                # --- **Ø§Ù„Ø­Ù„ Ø§Ù„Ø­Ø§Ø³Ù… Ù‡Ù†Ø§** ---
                # Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø©ØŒ Ù…Ø¹ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
                # Ø³ÙŠÙ‚ÙˆÙ… Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù†Ø³Ø® Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ø¥Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù†Ø§ØªØ¬Ø©
                metadata = {
                    "doc_id": doc_id,
                    "source_file": chunk["metadata"]["source_file"],
                    "tenant_id": chunk["metadata"]["tenant_id"],
                    "type": chunk["type"]
                }
                
                temp_doc = Document(
                    page_content=enriched_content,
                    metadata=metadata
                )
                
                # Ø§Ù„Ø¢Ù† Ù†Ù‚ÙˆÙ… Ø¨ØªÙ‚Ø·ÙŠØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„ÙˆØ§Ø­Ø¯
                split_docs = text_splitter.split_documents([temp_doc])
                
                # ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ù…Ù† split_docs Ø³ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
                all_docs_for_embedding.extend(split_docs)

        if not all_docs_for_embedding:
            logger.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ØªØ®Ø²ÙŠÙ†.")
            return

        logger.info(f"ğŸ’¾ Ø¨Ø¯Ø¡ ØªØ®Ø²ÙŠÙ† {len(all_original_contents)} Ù‚Ø·Ø¹Ø© Ù…Ø­ØªÙˆÙ‰ Ø£ØµÙ„ÙŠ Ùˆ {len(all_docs_for_embedding)} Ù‚Ø·Ø¹Ø© Ù…ØªØ¬Ù‡Ø©...")

        fs.mset(list(zip(all_doc_ids, all_original_contents)))
        logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ.")

        batch_size = 32
        total_batches = (len(all_docs_for_embedding) + batch_size - 1) // batch_size
        logger.info(f"Ø³ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… {len(all_docs_for_embedding)} Ù‚Ø·Ø¹Ø© Ø¥Ù„Ù‰ {total_batches} Ø¯ÙØ¹Ø© (Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {batch_size}).")

        with tqdm(total=len(all_docs_for_embedding), desc="ØªØ¶Ù…ÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª") as pbar:
            for i in range(0, len(all_docs_for_embedding), batch_size):
                batch_docs = all_docs_for_embedding[i:i + batch_size]
                
                # Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ù…ØªØ¬Ù‡Ø©
                batch_ids = [f"{doc.metadata['doc_id']}-{j}" for j, doc in enumerate(batch_docs)]
                
                try:
                    vectorstore.add_documents(documents=batch_docs, ids=batch_ids)
                    pbar.update(len(batch_docs))
                    time.sleep(0.1)
                except Exception as batch_err:
                    logger.error(f"ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© ØªØ¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ø¹Ù†ØµØ± {i}. Ø§Ù„Ø®Ø·Ø£: {batch_err}")
                    pbar.update(len(batch_docs))
                    continue

        logger.info("   > âœ… Ø§ÙƒØªÙ…Ù„ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

        processed_log.extend(files_to_process)
        save_processed_files_log(processed_log)
        
        logger.info("\nğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­!")

    except Exception as e:
        logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}", exc_info=True)

if __name__ == "__main__":
    run_ingestion()
