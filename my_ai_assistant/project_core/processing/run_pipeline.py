# project_core/processing/run_pipeline.py

import os
import logging
import json
from datetime import datetime

# ØªØ¬Ø§Ù‡Ù„ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ù‡Ù…Ø§Ù„ Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø£Ù†Ø¸Ù
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain_community.vectorstores import Chroma
from langchain.storage import LocalFileStore

from project_core.core.config import (
    DATA_SOURCES_DIR, VECTORSTORE_PATH, DOCSTORE_PATH, LOGS_DIR, BASE_DIR, get_embeddings_model,
)
from project_core.processing.pipeline import process_document_elements

# ==============================================================================
# 0. Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging)
# ==============================================================================
def setup_logging():
    """ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙÙŠ Ù…Ù„Ù ÙˆØ¹Ø±Ø¶Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©. """
    if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)
    log_filename = datetime.now().strftime(f"pipeline_run_%Y-%m-%d_%H-%M-%S.log")
    log_filepath = os.path.join(LOGS_DIR, log_filename)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger("run_pipeline")
    logger.info(f"Ø³ÙŠØªÙ… Ø­ÙØ¸ Ø³Ø¬Ù„Ø§Øª Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ù„Ù: {log_filepath}")
    return logger

logger = setup_logging()

# ==============================================================================
# 1. Ø¥Ø¯Ø§Ø±Ø© Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
# ==============================================================================
PROCESSED_FILES_LOG = os.path.join(BASE_DIR, "processed_files.json")

def load_processed_files_log():
    """ ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù…Ù† Ù…Ù„Ù JSON. """
    if os.path.exists(PROCESSED_FILES_LOG):
        try:
            with open(PROCESSED_FILES_LOG, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning("Ù…Ù„Ù Ø³Ø¬Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØªØ§Ù„Ù. Ø³ÙŠØªÙ… Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯.")
            return {}
    return {}

def save_processed_files_log(log_data):
    """ Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙÙŠ Ù…Ù„Ù JSON. """
    with open(PROCESSED_FILES_LOG, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, ensure_ascii=False, indent=4)

# ==============================================================================
# 2. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ
# ==============================================================================
def main():
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ§Ù„ØªØ²Ø§ÙŠØ¯ÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª.
    """
    processed_log = load_processed_files_log()
    files_to_process = []

    logger.info("="*50 + "\nğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠØ© (Ø§Ù„ØªØ²Ø§ÙŠØ¯ÙŠØ©)...\n" + "="*50)

    if not os.path.exists(DATA_SOURCES_DIR):
        logger.error(f"Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø± '{DATA_SOURCES_DIR}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
        return

    # --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ù…Ø­Ø¯Ø«Ø©) ---
    logger.info("--- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª ---")
    for tenant_id in os.listdir(DATA_SOURCES_DIR):
        tenant_path = os.path.join(DATA_SOURCES_DIR, tenant_id)
        if os.path.isdir(tenant_path):
            for file_name in os.listdir(tenant_path):
                file_path = os.path.join(tenant_path, file_name)
                if not os.path.isfile(file_path): continue
                
                try:
                    file_mod_time = os.path.getmtime(file_path)
                    if file_path not in processed_log or processed_log[file_path] < file_mod_time:
                        logger.info(f"âœ”ï¸ [Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©] Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ø£Ùˆ Ù…Ø­Ø¯Ø«: {file_name}")
                        files_to_process.append((file_path, tenant_id, file_mod_time))
                    else:
                        logger.info(f"âšªï¸ [ØªØ¬Ø§Ù‡Ù„] Ù…Ù„Ù Ù„Ù… ÙŠØªØºÙŠØ±: {file_name}")
                except FileNotFoundError:
                    logger.warning(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø³Ø¬Ù„ Ù„Ù… ÙŠØ¹Ø¯ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§: {file_path}. Ø³ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡.")
                    continue

    if not files_to_process:
        logger.info("\nâœ… ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø­Ø¯Ù‘Ø«Ø©. Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø´ÙŠØ¡ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. Ø§Ù†ØªÙ‡Ù‰.")
        return

    # --- Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø· ---
    logger.info("\n--- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ---")
    all_docs, all_ids, all_contents = [], [], []
    
    for file_path, tenant_id, file_mod_time in files_to_process:
        docs, ids, contents = process_document_elements(file_path, tenant_id)
        if docs:
            all_docs.extend(docs)
            all_ids.extend(ids)
            all_contents.extend(contents)
            # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø¬Ø§Ø­
            processed_log[file_path] = file_mod_time
    
    # --- Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø· ---
    if all_docs:
        logger.info("\n--- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---")
        try:
            vectorstore = Chroma(collection_name="rag-chroma", embedding_function=get_embeddings_model(), persist_directory=VECTORSTORE_PATH)
            doc_store = LocalFileStore(DOCSTORE_PATH)
            
            vectorstore.add_documents(all_docs)
            doc_store.mset(list(zip(all_ids, all_contents)))
            
            save_processed_files_log(processed_log) # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø¯Ø«
            logger.info("\nğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­!")
            logger.info(f"  - ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙÙŠ: '{VECTORSTORE_PATH}'")
            logger.info(f"  - ØªÙ… ØªØ­Ø¯ÙŠØ« Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙŠ: '{DOCSTORE_PATH}'")
        except Exception as e:
            logger.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    else:
        logger.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")

# --- Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ---
if __name__ == "__main__":
    main()
